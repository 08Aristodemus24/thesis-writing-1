{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multi-modal sensor dataset for continuous stress detection of nurses in a hospital\n",
    "Hosseini, Seyedmajid1; Katragadda, Satya1; Bhupatiraju, Ravi Teja1; Ashkar, Ziad1; Borst, Christoph1; Cochran, Kenneth1; Gottumukkala, Raju1\n",
    "\n",
    "https://www.nature.com/articles/s41597-022-01361-y/tables/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scipy.signal as scisig\n",
    "import scipy.stats\n",
    "\n",
    "from utilities.stress_feature_extractors import decompose_signal\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = os.listdir('./data/Hosseini_Stress_Dataset/')[:-1]\n",
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract data from zip file\n",
    "# for dir in dirs:\n",
    "#     for index, file in enumerate(os.listdir(f'./data/Hosseini_Stress_Dataset/{dir}/')):\n",
    "#         with zipfile.ZipFile(f'./data/Hosseini_Stress_Dataset/{dir}/{file}', 'r') as zip_ref:\n",
    "#             os.makedirs(f'./data/Hosseini_Stress_Dataset/{dir}/{index}')\n",
    "#             zip_ref.extract('EDA.csv', path=f'./data/Hosseini_Stress_Dataset/{dir}/{index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete zip files\n",
    "# for dir in dirs:\n",
    "#     zip_files = os.listdir(f'./data/Hosseini_Stress_Dataset/{dir}/')\n",
    "#     filt_zip_files = list(filter(lambda zip_file: \".zip\" in zip_file, zip_files))\n",
    "#     print(filt_zip_files)\n",
    "#     for index, file in enumerate(filt_zip_files):\n",
    "#         os.remove(f'./data/Hosseini_Stress_Dataset/{dir}/{file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read each file in each subjects subfolder i.e. 5C has subfolders 0 to 25 containing `EDA.csv`, convert its header to a readable timedelta format as well as total seconds or timestmap as this indicates it in the excel spreadsheet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_results = pd.read_excel('./data/Hosseini_Stress_Dataset/SurveyResults.xlsx', index_col=0)\n",
    "survey_results.reset_index(inplace=True)\n",
    "survey_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number 1586886626.000000 is a Unix timestamp, which represents the number of seconds that have elapsed since January 1, 1970, 00:00:00 UTC.\n",
    "\n",
    "To convert this timestamp to a human-readable format like HH:MM:SS, you can use the following methods:\n",
    "\n",
    "1. Using the datetime module:\n",
    "\n",
    "Python\n",
    "```\n",
    "import datetime\n",
    "\n",
    "timestamp = 1586886626.000000\n",
    "dt = datetime.datetime.fromtimestamp(timestamp)\n",
    "formatted_time = dt.strftime('%H:%M:%S')\n",
    "print(formatted_time)  # Output: 14:37:06\n",
    "```\n",
    "Use code with caution.\n",
    "\n",
    "2. Using the time module:\n",
    "\n",
    "Python\n",
    "```\n",
    "import time\n",
    "\n",
    "timestamp = 1586886626.000000\n",
    "formatted_time = time.strftime('%H:%M:%S', time.localtime(timestamp))\n",
    "print(formatted_time)  # Output: 14:37:06\n",
    "```\n",
    "Use code with caution.\n",
    "\n",
    "Both methods will output the time in the format HH:MM:SS, where HH is the hour, MM is the minute, and SS is the second.\n",
    "\n",
    "If you need to convert the time to a different time zone, you can use the timezone argument of the datetime.datetime.fromtimestamp() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows in survey_results with na in stress level\n",
    "survey_results_cleaned = survey_results[survey_results['Stress level'] != 'na'].reset_index(drop=True)\n",
    "survey_results_cleaned = survey_results_cleaned[['ID', 'Start time', 'End time', 'duration', 'date', 'Stress level']]\n",
    "survey_results_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the date column contains information e.g.\n",
    "we can check\n",
    "YY:MM:DD HH:MM:SS\n",
    "2020-04-15 08:00:00 is the start time and \n",
    "2020-04-15 09:00:00 is the end time of one patients recording session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = {\n",
    "    # '5C': [\n",
    "    #     (timestamp, df), \n",
    "    #     (timestamp, df),\n",
    "    #     ...\n",
    "    #     (timestamp, df),\n",
    "    # ],\n",
    "    # '6B': [\n",
    "    #     (timestamp, df), \n",
    "    #     (timestamp, df),\n",
    "    #     ...\n",
    "    #     (timestamp, df),\n",
    "    # ],\n",
    "    # ...\n",
    "    # 'EG': [\n",
    "    #     (timestamp, df), \n",
    "    #     (timestamp, df),\n",
    "    #     ...\n",
    "    #     (timestamp, df),\n",
    "    # ]\n",
    "}\n",
    "\n",
    "# read csvs first\n",
    "for dir in dirs:\n",
    "\n",
    "    subject_dfs = []\n",
    "    for subdir in os.listdir(f'./data/Hosseini_Stress_Dataset/{dir}/'):\n",
    "        df = pd.read_csv(f'./data/Hosseini_Stress_Dataset/{dir}/{subdir}/EDA.csv', )\n",
    "        start_epoch_header = df.columns[0]\n",
    "\n",
    "        # calculate also phasic and tonic component of raw signal\n",
    "        tonic, phasic = decompose_signal(df[start_epoch_header], samp_freq=4, method=\"median\")\n",
    "        df['phasic'] = phasic\n",
    "        df['tonic'] = tonic\n",
    "\n",
    "        # retrieve timestamp string from column\n",
    "        timestamp = float(start_epoch_header)\n",
    "\n",
    "        # modify header of raw signal\n",
    "        df.rename(columns={start_epoch_header: 'raw_signal'}, inplace=True)\n",
    "\n",
    "        # so according to hosseini et al. (2022) the header is actually the \n",
    "        # time that signal was generated using the internal clock of the wristband. \n",
    "        # The DateTime is stored in the first row of every data column.\n",
    "        # this is the starttime\n",
    "        subject_dfs.append((timestamp, df))\n",
    "\n",
    "    subjects[dir] = subject_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects['15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "def time_in_seconds(date_time):\n",
    "    epoch = dt.utcfromtimestamp(0)\n",
    "    delta = date_time - epoch\n",
    "    return delta.total_seconds()\n",
    "\n",
    "def helper(row):\n",
    "    \n",
    "    start_time = row['Start time'].strftime('%H:%M')\n",
    "    end_time = row['End time'].strftime('%H:%M')\n",
    "    date = row['date'].strftime('%Y-%m-%d')\n",
    "    duration = row['duration']\n",
    "    subject = str(row['ID'])\n",
    "    print(f'current subject: {subject}')\n",
    "\n",
    "    # print(type(start_time))\n",
    "\n",
    "    start_timestamp = dt.strptime(f'{date} {start_time}', '%Y-%m-%d %H:%M')\n",
    "    end_timestamp = dt.strptime(f'{date} {end_time}', '%Y-%m-%d %H:%M')\n",
    "    start_timestamp_s = time_in_seconds(start_timestamp)\n",
    "    end_timestamp_s = time_in_seconds(end_timestamp)\n",
    "    # 1594140175.0\n",
    "    # 1586937600.0\n",
    "    # print(f'start timestamp in seconds {start_timestamp_s}')\n",
    "    # print(f'end timestamp in seconds {end_timestamp_s}')\n",
    "\n",
    "    for index, (timestamp, df) in enumerate(subjects[subject]):\n",
    "        flag = start_timestamp_s <= timestamp <= end_timestamp_s\n",
    "        if flag == True:\n",
    "            print(f'start time: {start_timestamp}')\n",
    "            print(f'start time seconds: {start_timestamp_s} {timestamp} for subject: {subject}')\n",
    "            df['label'] = row['Stress level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_results_cleaned.apply(helper, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_subjects = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject, records in subjects.items():\n",
    "\n",
    "    new_subject_dfs = []\n",
    "    for record in records:\n",
    "        if 'label' in record[1].columns:\n",
    "            print(f'subject: {subject}')\n",
    "            print(f'record: {record}')\n",
    "\n",
    "            new_subject_dfs.append(record)\n",
    "        \n",
    "    new_subjects[subject] = new_subject_dfs            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but how can I know if this timestamp matches the times in my excel sheet? Because I do have information of the start time and end time of my actually eda signals. E.g. YY:MM:DD HH:MM:SS\n",
    "\n",
    "2020-04-15 08:00:00 is the start time and \n",
    "\n",
    "2020-04-15 09:00:00 is the end time of one patients recording session\n",
    "\n",
    "calculated timedelta or its duration was 01:00:00. How can I leverage this information to map this timedelta to this start time and end time and duration\n",
    "\n",
    "To determine if a timestamp matches the time range specified in your Excel sheet, you can follow these steps:\n",
    "\n",
    "Convert Excel time to Unix timestamp:\n",
    "\n",
    "Convert the start and end times from your Excel sheet to Unix timestamps using a library like openpyxl or pandas.\n",
    "You can use the datetime.datetime.strptime() function to parse the time strings and convert them to datetime objects, then use the timestamp() method to get the Unix timestamp.\n",
    "Compare timestamps:\n",
    "\n",
    "Check if the given timestamp (1586886626.000000) falls within the range of the start and end timestamps from your Excel sheet.\n",
    "Here's an example using pandas:\n",
    "\n",
    "Python\n",
    "import pandas as pd\n",
    "```\n",
    "# Assuming you have an Excel file with columns 'start_time' and 'end_time'\n",
    "df = pd.read_excel('your_excel_file.xlsx')\n",
    "\n",
    "# Convert start and end times to Unix timestamps\n",
    "df['start_time_unix'] = pd.to_datetime(df['start_time']).astype(int) // 10**9\n",
    "df['end_time_unix'] = pd.to_datetime(df['end_time']).astype(int) // 10**9\n",
    "\n",
    "# Check if the given timestamp falls within the range\n",
    "timestamp = 1586886626.000000\n",
    "is_within_range = (df['start_time_unix'] <= timestamp) & (df['end_time_unix'] >= timestamp)\n",
    "\n",
    "if is_within_range.any():\n",
    "    print(\"The timestamp falls within the recorded time range.\")\n",
    "else:\n",
    "    print(\"The timestamp is outside the recorded time range.\")\n",
    "```\n",
    "Use code with caution.\n",
    "\n",
    "This code will read the Excel file, convert the start and end times to Unix timestamps, and check if the given timestamp falls within any of the recorded time ranges. You can then use the information from the DataFrame to map the timestamp to the corresponding row and extract other relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new_subject, records in new_subjects.items():\n",
    "    print(len(records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_vis = new_subjects['15'][0][1]\n",
    "data_for_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_time = 0\n",
    "end_time = round((data_for_vis.shape[0] - 1) / 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_time_frame(raw_eda_df, samp_freq=128, begin_time_s=1750, end_time_s=1765, cols_to_use=['rawdata', 'cleandata', 'signal_automatic'], save_img=True, img_title='untitled'):\n",
    "    \"\"\"\n",
    "    ntoe cols to use must be equal to 5 or more\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(17, 5))\n",
    "\n",
    "    axis = fig.add_subplot()\n",
    "\n",
    "\n",
    "    # why is it multiplied by 128?\n",
    "    # results it 224000 and 225920 and is used as indeces to access a slice of the dataframes rows\n",
    "    begin_sample, end_sample = begin_time_s * samp_freq, end_time_s * samp_freq\n",
    "    print(begin_sample, end_sample)\n",
    "\n",
    "    # \n",
    "    time_to_plot = np.arange(begin_sample, end_sample)\n",
    "    \n",
    "\n",
    "    # colors and linestyles to use\n",
    "    colors = ['#2cb88e', '#c21d5c', '#d16d15', '#fc8003', '#3dfc03']\n",
    "    lines = ['solid', 'dotted', 'dashed', 'dashdot', (5, (10, 3))]\n",
    "\n",
    "    for i, col in enumerate(cols_to_use):\n",
    "        \n",
    "        col_to_plot = raw_eda_df[col].iloc[begin_sample:end_sample]\n",
    "        print(time_to_plot)\n",
    "        print(col_to_plot)\n",
    "        axis.plot(time_to_plot, col_to_plot, label=col, alpha=0.75, linestyle=lines[i], c=colors[i])\n",
    "            \n",
    "    axis.legend(fontsize=14)\n",
    "    axis.grid()\n",
    "    \n",
    "    axis.set_title(f\"{img_title}\")\n",
    "    axis.set_ylabel(r'$\\mu S$', fontsize=16)\n",
    "    axis.set_xlabel(\"Time (s)\", fontsize=16)\n",
    "\n",
    "    if save_img:\n",
    "        plt.savefig(f'./figures & images/{img_title}.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_time_frame(data_for_vis, samp_freq=4, begin_time_s=begin_time, end_time_s=end_time, cols_to_use=['raw_signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_time_frame(data_for_vis, samp_freq=4, begin_time_s=begin_time, end_time_s=end_time, cols_to_use=['phasic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have \n",
    "hertz = 4\n",
    "window_size = 5\n",
    "samples_per_sec = hertz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subjects = {\n",
    "    '15': new_subjects['15'],\n",
    "    '5C': new_subjects['5C']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_subjects['15'][0][1]['phasic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.stress_feature_extractors import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_features(new_subjects, hertz, window_size)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notee taht labels 0, 1, and 2 represent abseline, medium, and high stress levels respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results['15'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export features_and_labels dataframe of each subject to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject, dfs in results.items():\n",
    "    for i, df in enumerate(dfs):\n",
    "        df.to_csv(f'./data/Stress Detection Features/train/{subject}_{i}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.loaders import save_lookup_array, load_lookup_array, load_model, save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_lookup_array('./data/Stress Detection Features/subjects.txt', dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_names = load_lookup_array('./data/Stress Detection Features/subjects.txt')\n",
    "subject_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframes into a list\n",
    "# remove rows with nans\n",
    "temp = []\n",
    "for file in os.listdir('./data/Stress Detection Features/train'):\n",
    "    # subject, df_index = file.split('_')\n",
    "    df = pd.read_csv(f'./data/Stress Detection Features/train/{file}', index_col=0)\n",
    "    temp.append(df)\n",
    "subjects_features_and_labels = pd.concat(temp, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_features_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_features_and_labels.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = ['raw_4hz_1d_median',\n",
    "'raw_4hz_2d_median',\n",
    "'phasic_4hz_1d_median',\n",
    "'phasic_4hz_2d_median',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_features_and_labels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_features_and_labels = subjects_features_and_labels.drop(columns=cols_to_remove)\n",
    "subjects_features_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_features_and_labels = subjects_features_and_labels.dropna()\n",
    "subjects_features_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_features_and_labels['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recall that 0 is baseline, 1 is medium stress, and 2 is high stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBClassifier \n",
    "import itertools \n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = subjects_features_and_labels.drop(columns=['label']).to_numpy()\n",
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = subjects_features_and_labels['label'].to_numpy().ravel()\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper_params = {'n_estimators': [200, 400, 600], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 5, 10]}\n",
    "hyper_params = {'n_estimators': [400], 'learning_rate': [0.01], 'max_depth': [3]}\n",
    "hyper_param_score_list = []\n",
    "\n",
    "# unpack the dictionaries items and separate into list of keys and values\n",
    "# ('n_estimators', 'max_depth', 'gamma'), ([10, 50, 100], [3], [1, 10, 100, 1000])\n",
    "keys, values = zip(*hyper_params.items())\n",
    "\n",
    "# since values is an iterable and product receives variable arguments,\n",
    "# variable arg in product are essentially split thus above values\n",
    "# will be split into [10, 50, 100], [3], and [1, 10, 100, 1000]\n",
    "# this can be then easily used to produce all possible permutations of\n",
    "# these values e.g. (10, 3, 1), (10, 3, 10), (10, 3, 100) and so on...\n",
    "for prod in itertools.product(*values):\n",
    "    # we use the possible permutations and create a dictionary\n",
    "    # of the same keys as hyper params\n",
    "    hyper_param_config = dict(zip(keys, prod))\n",
    "    xgb = XGBClassifier(**hyper_param_config)\n",
    "    hyper_param_values = list(hyper_param_config.values())\n",
    "    scores = cross_validate(xgb, X_train, Y_train, cv=5, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc_ovr_weighted'], return_train_score=True)\n",
    "    mean_cross_acc = round(np.mean(scores['test_accuracy']), 4)\n",
    "    mean_cross_prec = round(np.mean(scores['test_precision_weighted']), 4)\n",
    "    mean_cross_rec = round(np.mean(scores['test_recall_weighted']), 4)\n",
    "    mean_cross_f1 = round(np.mean(scores['test_f1_weighted']), 4)\n",
    "    mean_cross_roc_auc = round(np.mean(scores['test_roc_auc_ovr_weighted']), 4)\n",
    "    hyper_param_score_list.append(hyper_param_values + [mean_cross_acc, mean_cross_prec, mean_cross_rec, mean_cross_f1, mean_cross_roc_auc])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_param_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the hyper-parameters (with highest average accuracy)\n",
    "results_table = PrettyTable([\"n_estimators\", \"learning_rate\", \"max_depth\", \"mean_cross_accuracy\", \"mean_cross_prec\", \"mean_cross_rec\", \"mean_cross_f1\", \"mean_cross_roc_auc\"])\n",
    "for row in hyper_param_score_list:\n",
    "    results_table.add_row(row)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "200      |      0.1      |     10    |        0.847        |     0.8452    |       0.9531\n",
    "\n",
    "600      |      0.1      |     10    |        0.8492       |     0.8474    |       0.9543 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyper_params = {'n_estimators': 600, 'learning_rate': 0.1, 'max_depth': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max scaler\n",
    "xgb_scaler = MinMaxScaler()\n",
    "X_train_scaled = xgb_scaler.fit_transform(X_train)\n",
    "X_test_scaled = xgb_scaler.transform(X_test)\n",
    "\n",
    "save_model(xgb_scaler, './saved/misc/xgb_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the performance of model with the Best parameters on testing set\n",
    "xgb_best = XGBClassifier(**best_hyper_params, verbose=1)\n",
    "xgb_best.fit(X_train_scaled, Y_train)\n",
    "print(\"Best Model Testing Score: \", xgb_best.score(X_test_scaled, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(xgb_best, './saved/models/stress_detector.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train = xgb_best.predict(X_train_scaled)\n",
    "Y_pred_test = xgb_best.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cm = confusion_matrix(Y_train, Y_pred_train)\n",
    "test_cm = confusion_matrix(Y_test, Y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.visualizers import multi_class_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class_heatmap(train_cm, img_title='train confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class_heatmap(test_cm, cmap=\"magma\", img_title='test confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subject_corrected = pd.read_csv('./results/pqbqpr_expert2_corrected.csv', index_col=0)\n",
    "test_subject_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tonic, phasic = decompose_signal(test_subject_corrected['new_signal'])\n",
    "test_subject_corrected['phasic'] = phasic\n",
    "test_subject_corrected['tonic'] = tonic\n",
    "test_subject_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_time_s = 0\n",
    "end_time_s = round(test_subject_corrected.shape[0] / 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_time_frame(test_subject_corrected, samp_freq=128, begin_time_s=begin_time_s, end_time_s=end_time_s, cols_to_use=['phasic', 'tonic', 'raw_signal'], img_title='raw phasic and tonic components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.feature_extractors import interpolate_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subject_corrected_4hz = interpolate_signals(test_subject_corrected, sample_rate=128, target_hz=4)\n",
    "test_subject_corrected_4hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subjects_corrected = {\n",
    "    'test': [('na', test_subject_corrected_4hz)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subjects_corrected_features = get_features(test_subjects_corrected, hertz=4, window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subject_corrected_features = test_subjects_corrected_features['test'][0]\n",
    "test_subject_corrected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subject_corrected_features.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subject_corrected_features_cleaned = test_subject_corrected_features.drop(columns=cols_to_remove)\n",
    "test_subject_corrected_features_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subject_corrected_features_cleaned = test_subject_corrected_features_cleaned.dropna()\n",
    "test_subject_corrected_features_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_test = test_subject_corrected_features_cleaned.drop(columns=['label']).to_numpy()\n",
    "final_Y_test = test_subject_corrected_features_cleaned['label'].to_numpy().ravel()\n",
    "final_X_test.shape, final_Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(final_Y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_xgb_scaler = load_model('./saved/misc/xgb_scaler.pkl')\n",
    "saved_stress_detector = load_model('./saved/models/stress_detector.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_test_scaled = saved_xgb_scaler.transform(final_X_test)\n",
    "final_X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_Y_test_pred = saved_stress_detector.predict(final_X_test_scaled)\n",
    "final_Y_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(final_Y_test_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coonclusion: we used unannotated eda signals from the EDABE dataset that has already been corrected to make sure noise is removed and used it as test signals in of itself to extract features from it to use as test data for a trained stress detection model trained on annotated signals  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "however if for training only this can stillbe used and removing rows because of mismatched sampling rate can still be viable\n",
    "\n",
    "# WESAD\n",
    "A Multimodal Dataset for Wearable Stress and Affect Detection\n",
    "Using only data from the Empatica E4 (EDA, BVP, ACM, TEMP), and RESP from Respiban (currently)\n",
    "Matthew Johnson, 2019\n",
    "\n",
    "## Dataset Information [1]:\n",
    "Data Set Information:\n",
    "\n",
    "\"WESAD is a publicly available dataset for wearable stress and affect detection. This multimodal dataset features physiological and motion data, recorded from both a wrist- and a chest-worn device, of 15 subjects during a lab study. The following sensor modalities are included: blood volume pulse, electrocardiogram, electrodermal activity, electromyogram, respiration, body temperature, and three-axis acceleration. Moreover, the dataset bridges the gap between previous lab studies on stress and emotions, by containing three different affective states (neutral, stress, amusement). In addition, self-reports of the subjects, which were obtained using several established questionnaires, are contained in the dataset. Details can be found in the dataset's readme-file, as well as in [1].\n",
    "\n",
    "## Attribute Information:\n",
    "\n",
    "Raw sensor data was recorded with two devices: a chest-worn device (RespiBAN) and a wrist-worn device (Empatica E4). The RespiBAN device provides the following sensor data: electrocardiogram (ECG), electrodermal activity (EDA), electromyogram (EMG), respiration, body temperature, and three-axis acceleration. All signals are sampled at 700 Hz. The Empatica E4 device provides the following sensor data: blood volume pulse (BVP, 64 Hz), electrodermal activity (EDA, 4 Hz), body temperature (4 Hz), and three-axis acceleration (32 Hz).\n",
    "\n",
    "The dataset's readme-file contains all further details with respect to the dataset structure, data format (RespiBAN device, Empatica E4 device, synchronised data), study protocol, and the self-report questionnaires.\"\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/WESAD+%28Wearable+Stress+and+Affect+Detection%29\n",
    "Classes\n",
    "Baseline condition: 20 minute period of standing/sitting reading magazines.\n",
    "Amusement condition: During the amusement condition, the subjects watched a set of eleven funny video clips.\n",
    "Stress condition: Trier Social Stress Test (TSST), consisting of public speaking and mental arithmetic.\n",
    "\n",
    "# References\n",
    "[1] Schmidt, Philip & Reiss, Attila & Duerichen, Robert & Marberger, Claus & Van Laerhoven, Kristof. (2018). Introducing WESAD, a Multimodal Dataset for Wearable Stress and Affect Detection. 400-408. 10.1145/3242969.3242985. https://dl.acm.org/citation.cfm?doid=3242969.3242985\n",
    "\n",
    "[2] A Greco, G Valenza, A Lanata, EP Scilingo, and L Citi \"cvxEDA: a Convex Optimization Approach to Electrodermal Activity Processing\" IEEE Transactions on Biomedical Engineering, 2015 DOI: 10.1109/TBME.2015.2474131 https://github.com/lciti/cvxEDA\n",
    "\n",
    "[3] J. Choi, B. Ahmed, and R. Gutierrez-Osuna. 2012. Development and evaluation of an ambulatory stress monitor based on wearable sensors. IEEE Transactions on Information Technology in Biomedicine 16, 2 (2012).\n",
    "http://research.cs.tamu.edu/prism/publications/choi2011ambulatoryStressMonitor.pdf\n",
    "\n",
    "[6] J. Healey and R. Picard. 2005. Detecting stress during real-world driving tasks using physiological sensors. IEEE Transactions on Intelligent Transportation Systems 6, 2 (2005), 156–166.\n",
    "\n",
    "Useful Resources:\n",
    "https://github.com/jaganjag/stress_affect_detection\n",
    "https://github.com/arsen-movsesyan/springboard_WESAD\n",
    "https://www.birmingham.ac.uk/Documents/college-les/psych/saal/guide-electrodermal-activity.pdf\n",
    "http://research.cs.tamu.edu/prism/publications/choi2011ambulatoryStressMonitor.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # E4 (wrist) Sampling Frequencies\n",
    "\n",
    "# \"\"\"contain the sampling frequencies/rate/hertz of each signals\"\"\"\n",
    "# fs_dict = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4, 'label': 700, 'Resp': 700}\n",
    "\n",
    "# WINDOW_IN_SECONDS = 30\n",
    "# label_dict = {'baseline': 1, 'stress': 2, 'amusement': 0}\n",
    "# int_to_label = {1: 'baseline', 2: 'stress', 0: 'amusement'}\n",
    "# feat_names = None\n",
    "# savePath = './data'\n",
    "# subject_feature_path = '/Stress Detection Data'\n",
    "\n",
    "# if not os.path.exists(savePath):\n",
    "#     os.makedirs(savePath)\n",
    "# if not os.path.exists(savePath + subject_feature_path):\n",
    "#     os.makedirs(savePath + subject_feature_path)\n",
    "\n",
    "# # cvxEDA\n",
    "# def eda_stats(y):\n",
    "#     Fs = fs_dict['EDA']\n",
    "#     yn = (y - y.mean()) / y.std()\n",
    "#     [r, p, t, l, d, e, obj] = cvxEDA.cvxEDA(yn, 1. / Fs)\n",
    "#     return [r, p, t, l, d, e, obj]\n",
    "\n",
    "\n",
    "# class SubjectData:\n",
    "\n",
    "#     def __init__(self, main_path, subject_number):\n",
    "#         self.name = f'S{subject_number}'\n",
    "#         self.subject_keys = ['signal', 'label', 'subject']\n",
    "#         self.signal_keys = ['chest', 'wrist']\n",
    "#         self.chest_keys = ['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
    "#         self.wrist_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "\n",
    "#         # ./data/WESAD/{2-17}/{2-17}.pkl is the main path\n",
    "#         with open(os.path.join(main_path, self.name) + '/' + self.name + '.pkl', 'rb') as file:\n",
    "#             self.data = pickle.load(file, encoding='latin1')\n",
    "#         self.labels = self.data['label']\n",
    "\n",
    "#     def get_wrist_data(self):\n",
    "#         data = self.data['signal']['wrist']\n",
    "#         data.update({'Resp': self.data['signal']['chest']['Resp']})\n",
    "        \n",
    "#         return data\n",
    "\n",
    "#     def get_chest_data(self):\n",
    "#         return self.data['signal']['chest']\n",
    "\n",
    "#     # \"\"\"function never used\"\"\"\n",
    "#     # def extract_features(self):  # only wrist\n",
    "#     #     results = \\\n",
    "#     #         {\n",
    "#     #             key: get_statistics(self.get_wrist_data()[key].flatten(), self.labels, key)\n",
    "#     #             for key in self.wrist_keys\n",
    "#     #         }\n",
    "#     #     return results\n",
    "\n",
    "\n",
    "# # https://github.com/MITMediaLabAffectiveComputing/eda-explorer/blob/master/load_files.py\n",
    "# def butter_lowpass(cutoff, fs, order=5):\n",
    "#     # Filtering Helper functions\n",
    "#     nyq = 0.5 * fs\n",
    "#     normal_cutoff = cutoff / nyq\n",
    "#     b, a = scisig.butter(order, normal_cutoff, btype='low', analog=False)\n",
    "#     return b, a\n",
    "\n",
    "\n",
    "# def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "#     # Filtering Helper functions\n",
    "#     b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "#     y = scisig.lfilter(b, a, data)\n",
    "#     return y\n",
    "\n",
    "# def get_slope(series):\n",
    "#     linreg = scipy.stats.linregress(np.arange(len(series)), series )\n",
    "#     slope = linreg[0]\n",
    "#     return slope\n",
    "\n",
    "# def get_window_stats(data, label=-1):\n",
    "#     mean_features = np.mean(data)\n",
    "#     std_features = np.std(data)\n",
    "#     min_features = np.amin(data)\n",
    "#     max_features = np.amax(data)\n",
    "\n",
    "#     features = {'mean': mean_features, 'std': std_features, 'min': min_features, 'max': max_features,\n",
    "#                 'label': label}\n",
    "#     return features\n",
    "\n",
    "\n",
    "# def get_net_accel(data):\n",
    "#     return (data['ACC_x'] ** 2 + data['ACC_y'] ** 2 + data['ACC_z'] ** 2).apply(lambda x: np.sqrt(x))\n",
    "\n",
    "\n",
    "# def get_peak_freq(x):\n",
    "#     f, Pxx = scisig.periodogram(x, fs=8)\n",
    "#     psd_dict = {amp: freq for amp, freq in zip(Pxx, f)}\n",
    "#     peak_freq = psd_dict[max(psd_dict.keys())]\n",
    "#     return peak_freq\n",
    "\n",
    "\n",
    "# # https://github.com/MITMediaLabAffectiveComputing/eda-explorer/blob/master/AccelerometerFeatureExtractionScript.py\n",
    "# def filterSignalFIR(eda, cutoff=0.4, numtaps=64):\n",
    "#     f = cutoff / (fs_dict['ACC'] / 2.0)\n",
    "#     FIR_coeff = scisig.firwin(numtaps, f)\n",
    "\n",
    "#     return scisig.lfilter(FIR_coeff, 1, eda)\n",
    "\n",
    "\n",
    "# def compute_features(e4_data_dict, labels, norm_type=None):\n",
    "#     # Dataframes for each sensor type\n",
    "#     \"\"\"EDA from the e4 data dict is what I need\"\"\"\n",
    "#     \"\"\"EDA IS A (m, 1) numpy array\"\"\"\n",
    "#     print(f'eda shape: {e4_data_dict['EDA'].shape}')\n",
    "#     eda_df = pd.DataFrame(e4_data_dict['EDA'], columns=['EDA'])\n",
    "#     bvp_df = pd.DataFrame(e4_data_dict['BVP'], columns=['BVP'])\n",
    "\n",
    "#     print(f'ACC shape: {e4_data_dict['ACC'].shape}')\n",
    "#     acc_df = pd.DataFrame(e4_data_dict['ACC'], columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
    "#     temp_df = pd.DataFrame(e4_data_dict['TEMP'], columns=['TEMP'])\n",
    "\n",
    "#     \"\"\"I also need the label\"\"\"\n",
    "#     print(f'labels shape: {labels.shape}, unique: {np.unique(labels)}')\n",
    "#     label_df = pd.DataFrame(labels, columns=['label'])\n",
    "#     resp_df = pd.DataFrame(e4_data_dict['Resp'], columns=['Resp'])\n",
    "\n",
    "#     \"\"\"\n",
    "#     eda shape: (24316, 1) this means each 4 rows is 1 second since its sampling rate was 4hz\n",
    "#     ACC shape: (194528, 3) this means \n",
    "#     labels shape: (4255300,)\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Filter EDA\n",
    "#     eda_df['EDA'] = butter_lowpass_filter(eda_df['EDA'], 1.0, fs_dict['EDA'], 6)\n",
    "\n",
    "#     # Filter ACM\n",
    "#     for _ in acc_df.columns:\n",
    "#         acc_df[_] = filterSignalFIR(acc_df.values)\n",
    "\n",
    "#     # Adding indices for combination due to differing sampling frequencies\n",
    "#     \"\"\"but my question is what would be the final sampling \n",
    "#     frequency of the dataframe when all these signals are combined\"\"\"\n",
    "    \n",
    "#     \"\"\"(1 / 4) * 0 = 0\n",
    "#     (1 / 4) * 1 = .25\n",
    "#     (1 / 4) * 2 = 0.5\n",
    "#     (1 / 4) * 3 = 0.75\n",
    "#     as we can see we go from 0 to 0.75 which is four rows before it gets to 1 second\n",
    "#     this is because our eda signal has been sampled at 4hz\n",
    "#     ...\n",
    "#     (1 / 4) * 24315 = 6078.75\"\"\"\n",
    "#     eda_df.index = [(1 / fs_dict['EDA']) * i for i in range(len(eda_df))]\n",
    "#     bvp_df.index = [(1 / fs_dict['BVP']) * i for i in range(len(bvp_df))]\n",
    "#     acc_df.index = [(1 / fs_dict['ACC']) * i for i in range(len(acc_df))]\n",
    "#     temp_df.index = [(1 / fs_dict['TEMP']) * i for i in range(len(temp_df))]\n",
    "#     label_df.index = [(1 / fs_dict['label']) * i for i in range(len(label_df))]\n",
    "#     resp_df.index = [(1 / fs_dict['Resp']) * i for i in range(len(resp_df))]\n",
    "\n",
    "#     # Change indices to datetime\n",
    "#     eda_df.index = pd.to_datetime(eda_df.index, unit='s')\n",
    "#     bvp_df.index = pd.to_datetime(bvp_df.index, unit='s')\n",
    "#     temp_df.index = pd.to_datetime(temp_df.index, unit='s')\n",
    "#     acc_df.index = pd.to_datetime(acc_df.index, unit='s')\n",
    "#     label_df.index = pd.to_datetime(label_df.index, unit='s')\n",
    "#     resp_df.index = pd.to_datetime(resp_df.index, unit='s')\n",
    "\n",
    "#     # New EDA features\n",
    "#     \"\"\"r is the phasic component of the eda signal, t is the tonic component of the eda signal\"\"\"\n",
    "#     r, p, t, l, d, e, obj = eda_stats(eda_df['EDA'])\n",
    "#     eda_df['EDA_phasic'] = r\n",
    "#     eda_df['EDA_smna'] = p\n",
    "#     eda_df['EDA_tonic'] = t\n",
    "        \n",
    "#     # Combined dataframe - not used yet\n",
    "#     df = eda_df.join(bvp_df, how='outer')\n",
    "#     df = df.join(temp_df, how='outer')\n",
    "#     df = df.join(acc_df, how='outer')\n",
    "#     df = df.join(resp_df, how='outer')\n",
    "#     df = df.join(label_df, how='outer')\n",
    "#     print(f'combined df: {df}, shape: {df.shape}')\n",
    "#     \"\"\"\n",
    "#     what this does is combines all uneven dfs into a dataframe of\n",
    "#     700hz the largest smapling freq used particularly for the labels\"\"\"\n",
    "#     print(f'df label column contains null? {df['label'].isnull().sum()}')\n",
    "\n",
    "#     \"\"\"364740 rows contain na rows, backfill / bfill is used\n",
    "#     which uses next valid observation to fill gap\"\"\"\n",
    "#     df['label'] = df['label'].fillna(method='bfill')\n",
    "\n",
    "#     print(f'df label column contains null? {df['label'].isnull().sum()}')\n",
    "    \n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "#     print(f'combined final df: {df}, shape: {df.shape}')\n",
    "\n",
    "#     \"\"\"but we can create a column taht indicates the seconds in each row\"\"\"\n",
    "#     print(f'uniques and counts of label: {df['label'].value_counts()}')\n",
    "\n",
    "#     if norm_type is 'std':\n",
    "#         # std norm\n",
    "#         df = (df - df.mean()) / df.std()\n",
    "#     elif norm_type is 'minmax':\n",
    "#         # minmax norm\n",
    "#         df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "#     # Groupby\n",
    "#     \"\"\"\n",
    "#     No. of Labels ==> 8 ; \n",
    "#     0 = not defined / transient\n",
    "#     1 = baseline\n",
    "#     2 = stress\n",
    "#     3 = amusement\n",
    "#     4 = meditation\n",
    "#     5/6/7 = should be ignored in this dataset\n",
    "#     instead of this in order to retain 700hz sampling freq we can turn instead the 4/5/6/7/ and 0 labels will be\n",
    "\n",
    "#     ff. are value counts of each label in 700hz sampling frequency\n",
    "#     0.0    2326361 for transient\n",
    "#     1.0     869440 for baseline\n",
    "#     4.0     583679 for meditation\n",
    "#     2.0     467400 for stress\n",
    "#     3.0     275120 for amusement\n",
    "#     6.0      49400 should be ignored\n",
    "#     7.0      48640 should be ignored\n",
    "\n",
    "#     what we can do instead is have the dataset have some of its labels\n",
    "#     combined that results in less labels that best indicate the\n",
    "#     affective states of an individual\n",
    "\n",
    "#     i.e. transience, and labels 6 and 7, can be combined to form a transient state of being\n",
    "#     baseline can be a state of being\n",
    "#     meditation can be a state of being\n",
    "#     stress can be a state of being\n",
    "#     amusement can be a state of being\n",
    "\n",
    "#     that way we can retain still the 700hz sampling freq without removing rows or samples in the data\n",
    "#     \"\"\"\n",
    "\n",
    "#     grouped = df.groupby('label')\n",
    "#     baseline = grouped.get_group(1)\n",
    "#     stress = grouped.get_group(2)\n",
    "#     amusement = grouped.get_group(3)\n",
    "\n",
    "#     return grouped, baseline, stress, amusement\n",
    "\n",
    "\n",
    "# def get_samples(data, n_windows, label):\n",
    "#     global feat_names\n",
    "\n",
    "#     # 30 seconds\n",
    "#     global WINDOW_IN_SECONDS\n",
    "\n",
    "#     samples = []\n",
    "#     # Using label freq (700 Hz) as our reference frequency due to it being the largest\n",
    "#     # and thus encompassing the lesser ones in its resolution.\n",
    "#     window_len = fs_dict['label'] * WINDOW_IN_SECONDS\n",
    "\n",
    "#     for i in range(n_windows):\n",
    "#         # Get window of data\n",
    "#         w = data[window_len * i: window_len * (i + 1)]\n",
    "\n",
    "#         # Add/Calc rms acc\n",
    "#         # w['net_acc'] = get_net_accel(w)\n",
    "#         w = pd.concat([w, get_net_accel(w)])\n",
    "#         #w.columns = ['net_acc', 'ACC_x', 'ACC_y', 'ACC_z', 'BVP',\n",
    "#           #           'EDA', 'EDA_phasic', 'EDA_smna', 'EDA_tonic', 'TEMP',\n",
    "#             #         'label']\n",
    "#         cols = list(w.columns)\n",
    "#         cols[0] = 'net_acc'\n",
    "#         w.columns = cols\n",
    "        \n",
    "#         # Calculate stats for window\n",
    "#         wstats = get_window_stats(data=w, label=label)\n",
    "\n",
    "#         # Seperating sample and label\n",
    "#         x = pd.DataFrame(wstats).drop('label', axis=0)\n",
    "#         y = x['label'][0]\n",
    "#         x.drop('label', axis=1, inplace=True)\n",
    "\n",
    "#         if feat_names is None:\n",
    "#             feat_names = []\n",
    "#             for row in x.index:\n",
    "#                 for col in x.columns:\n",
    "#                     \"\"\"error TypeError: sequence item 0: expected str instance, int found occurs here\"\"\"\n",
    "#                     feat_names.append('_'.join([str(row), str(col)]))\n",
    "\n",
    "#         # sample df\n",
    "#         wdf = pd.DataFrame(x.values.flatten()).T\n",
    "#         wdf.columns = feat_names\n",
    "#         wdf = pd.concat([wdf, pd.DataFrame({'label': y}, index=[0])], axis=1)\n",
    "        \n",
    "#         # More feats\n",
    "#         wdf['BVP_peak_freq'] = get_peak_freq(w['BVP'].dropna())\n",
    "#         wdf['TEMP_slope'] = get_slope(w['TEMP'].dropna())\n",
    "#         samples.append(wdf)\n",
    "\n",
    "#     return pd.concat(samples)\n",
    "\n",
    "\n",
    "# def make_patient_data(subject_id):\n",
    "#     global savePath\n",
    "#     global WINDOW_IN_SECONDS\n",
    "\n",
    "#     # Make subject data object for Sx\n",
    "#     subject = SubjectData(main_path='./data/WESAD', subject_number=subject_id)\n",
    "\n",
    "#     # Empatica E4 data - now with resp\n",
    "#     \"\"\"returns dictionary\"\"\"\n",
    "#     e4_data_dict = subject.get_wrist_data()\n",
    "#     print(f'{e4_data_dict}')\n",
    "\n",
    "#     # norm type\n",
    "#     norm_type = None\n",
    "\n",
    "#     # The 3 classes we are classifying\n",
    "#     print(f'subject labels: {subject.labels}, type: {type(subject.labels)}, shape: {subject.labels.shape}')\n",
    "#     grouped, baseline, stress, amusement = compute_features(e4_data_dict, subject.labels, norm_type)\n",
    "#     # print(f'grouped: {grouped}')\n",
    "#     # print(f'grouped shape: {grouped.shape}')\n",
    "#     # print(f'baseline: {baseline}')\n",
    "#     print(f'baseline shape: {len(baseline)}')\n",
    "#     # print(f'stress: {stress}')\n",
    "#     print(f'stress shape: {len(stress)}')\n",
    "#     # print(f'amusement: {amusement}')\n",
    "#     print(f'amusement shape: {len(amusement)}')\n",
    "\n",
    "#     # print(f'Available windows for {subject.name}:')\n",
    "#     \"\"\"label is 700hz, window in seconds is 30s\n",
    "#     41\n",
    "#     22\n",
    "#     13\n",
    "#     \"\"\"\n",
    "#     n_baseline_wdws = int(len(baseline) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "#     n_stress_wdws = int(len(stress) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "#     n_amusement_wdws = int(len(amusement) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "#     print(f'Baseline: {n_baseline_wdws}\\nStress: {n_stress_wdws}\\nAmusement: {n_amusement_wdws}\\n')\n",
    "\n",
    "#     #\n",
    "#     baseline_samples = get_samples(baseline, n_baseline_wdws, label=1)\n",
    "#     # Downsampling\n",
    "#     # baseline_samples = baseline_samples[::2]\n",
    "#     stress_samples = get_samples(stress, n_stress_wdws, label=2)\n",
    "#     amusement_samples = get_samples(amusement, n_amusement_wdws, label=0)\n",
    "\n",
    "#     all_samples = pd.concat([baseline_samples, stress_samples, amusement_samples])\n",
    "#     all_samples = pd.concat([all_samples.drop('label', axis=1), pd.get_dummies(all_samples['label'])], axis=1)\n",
    "#     # Selected Features\n",
    "#     # all_samples = all_samples[['EDA_mean', 'EDA_std', 'EDA_min', 'EDA_max',\n",
    "#     #                          'BVP_mean', 'BVP_std', 'BVP_min', 'BVP_max',\n",
    "#     #                        'TEMP_mean', 'TEMP_std', 'TEMP_min', 'TEMP_max',\n",
    "#     #                        'net_acc_mean', 'net_acc_std', 'net_acc_min', 'net_acc_max',\n",
    "#     #                        0, 1, 2]]\n",
    "#     # Save file as csv (for now)\n",
    "#     all_samples.to_csv(f'{savePath}{subject_feature_path}/S{subject_id}_feats_4.csv')\n",
    "\n",
    "#     # Does this save any space?\n",
    "#     subject = None\n",
    "\n",
    "\n",
    "# def combine_files(subjects):\n",
    "#     df_list = []\n",
    "#     for s in subjects:\n",
    "#         df = pd.read_csv(f'{savePath}{subject_feature_path}/S{s}_feats_4.csv', index_col=0)\n",
    "#         df['subject'] = s\n",
    "#         df_list.append(df)\n",
    "\n",
    "#     df = pd.concat(df_list, axis=0)\n",
    "#     print(df)\n",
    "\n",
    "#     \"\"\"error ValueError: substring not found occurs in this line\"\"\"\n",
    "#     # 0 is the annotation for amusement\n",
    "#     # 1 is baseline \n",
    "#     # 2 is the annotation for stress\n",
    "#     df['label'] = df.apply(lambda row: 0 if row['0'] == True else 1 if row['1'] == True else 2, axis=1)\n",
    "#     # labels = (df['0'].astype(str) + df['1'].astype(str) + df['2'].astype(str))\n",
    "#     # print(labels)\n",
    "#     # # print(labels.index)\n",
    "#     # df['label'] = (df['0'].astype(str) + df['1'].astype(str) + df['2'].astype(str)).apply(lambda row: row.index(0))\n",
    "#     df.drop(['0', '1', '2'], axis=1, inplace=True)\n",
    "\n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "#     print(df)\n",
    "\n",
    "#     df.to_csv(f'{savePath}{subject_feature_path}/may14_feats4.csv')\n",
    "\n",
    "#     counts = df['label'].value_counts()\n",
    "#     print('Number of samples per class:')\n",
    "#     for label, number in zip(counts.index, counts.values):\n",
    "#         print(f'{int_to_label[label]}: {number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "\n",
    "# for patient in subject_ids:\n",
    "#     print(f'Processing data for S{patient}...')\n",
    "#     make_patient_data(patient)\n",
    "\n",
    "# combine_files(subject_ids)\n",
    "# print('Processing complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# fs_dict = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4, 'label': 700, 'Resp': 700}\n",
    "# WINDOW_IN_SECONDS = 30\n",
    "# label_dict = {'baseline': 1, 'stress': 2, 'amusement': 0}\n",
    "# int_to_label = {1: 'baseline', 2: 'stress', 0: 'amusement'}\n",
    "# feat_names = None\n",
    "# savePath = './data'\n",
    "# subject_feature_path = '/Stress Detection Data'\n",
    "\n",
    "# if not os.path.exists(savePath):\n",
    "#     os.makedirs(savePath)\n",
    "# if not os.path.exists(savePath + subject_feature_path):\n",
    "#     os.makedirs(savePath + subject_feature_path)\n",
    "\n",
    "# class SubjectData:\n",
    "\n",
    "#     def __init__(self, main_path, subject_number):\n",
    "#         self.name = f'S{subject_number}'\n",
    "#         self.subject_keys = ['signal', 'label', 'subject']\n",
    "#         self.signal_keys = ['chest', 'wrist']\n",
    "#         self.chest_keys = ['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
    "#         self.wrist_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "\n",
    "#         # ./data/WESAD/{2-17}/{2-17}.pkl is the main path\n",
    "#         with open(os.path.join(main_path, self.name) + '/' + self.name + '.pkl', 'rb') as file:\n",
    "#             self.data = pickle.load(file, encoding='latin1')\n",
    "#         self.labels = self.data['label']\n",
    "\n",
    "#     def get_wrist_data(self):\n",
    "#         data = self.data['signal']['wrist']\n",
    "#         data.update({'Resp': self.data['signal']['chest']['Resp']})\n",
    "        \n",
    "#         return data\n",
    "\n",
    "#     def get_chest_data(self):\n",
    "#         return self.data['signal']['chest']\n",
    "\n",
    "# def new_label_mapper(label):\n",
    "#     # for transient\n",
    "#     if label == 0 or label == 6 or label == 7:\n",
    "#         return 0\n",
    "    \n",
    "#     # for baseline\n",
    "#     elif label == 1:\n",
    "#         return 1\n",
    "    \n",
    "#     # for stress\n",
    "#     elif label == 2:\n",
    "#         return 2\n",
    "    \n",
    "#     # for amusement\n",
    "#     elif label == 3:\n",
    "#         return 3\n",
    "    \n",
    "#     # for meditation\n",
    "#     elif label == 4:\n",
    "#         return 4\n",
    "\n",
    "# # all functions are reversed engineered version of the above functions such that the\n",
    "# # dataframe created only contains the eda signals and affective labels for stress detection\n",
    "# def eda_stats(y):\n",
    "#     Fs = fs_dict['EDA']\n",
    "#     yn = (y - y.mean()) / y.std()\n",
    "#     [r, p, t, l, d, e, obj] = cvxEDA.cvxEDA(yn, 1. / Fs)\n",
    "#     return [r, p, t, l, d, e, obj]\n",
    "\n",
    "# def compute_features(e4_data_dict, labels, norm_type=None):\n",
    "#     # Dataframes for each sensor type\n",
    "#     # eda shape: (24316, 1) this means each 4 rows is 1 second since its sampling rate was 4hz\n",
    "#     # ACC shape: (194528, 3) this means \n",
    "#     # labels shape: (4255300,)\n",
    "#     eda_df = pd.DataFrame(e4_data_dict['EDA'], columns=['EDA'])\n",
    "#     label_df = pd.DataFrame(labels, columns=['label'])\n",
    "#     print(f'label df shape: {label_df}')\n",
    "\n",
    "#     # Adding indices for combination due to differing sampling frequencies\n",
    "#     # (1 / 4) * 0 = 0\n",
    "#     # (1 / 4) * 1 = .25\n",
    "#     # (1 / 4) * 2 = 0.5\n",
    "#     # (1 / 4) * 3 = 0.75\n",
    "#     # as we can see we go from 0 to 0.75 which is four rows before it gets to 1 second\n",
    "#     # this is because our eda signal has been sampled at 4hz\n",
    "#     # ...\n",
    "#     # (1 / 4) * 24315 = 6078.75\n",
    "#     # eda_df.index = [(1 / fs_dict['EDA']) * i for i in range(len(eda_df))]\n",
    "#     # label_df.index = [(1 / fs_dict['label']) * i for i in range(len(label_df))]\n",
    "\n",
    "#     # # Change indices to datetime\n",
    "#     # eda_df.index = pd.to_datetime(eda_df.index, unit='s')\n",
    "#     # label_df.index = pd.to_datetime(label_df.index, unit='s')\n",
    "        \n",
    "#     # Combined dataframe - not used yet\n",
    "#     # what this does is combines all uneven dfs into a dataframe of\n",
    "#     # 700hz the largest smapling freq used particularly for the labels\n",
    "#     df = eda_df.join(label_df, how='outer')\n",
    "#     # print(f'combined df: {df}, shape: {df.shape}')\n",
    "    \n",
    "    \n",
    "#     # print(f'combined final df: {df}, shape: {df.shape}')\n",
    "\n",
    "#     # get number of rows of 700hz dataframe and use it to create time indeces\n",
    "#     # 1000 / 700 milliseconds or 1.428571428571429 is 0.001428571428571429\n",
    "#     # in seconds but in nanoseconds is 1428571.4285714289 or 0.0014285714285714289e-9\n",
    "#     n_rows = df.shape[0]\n",
    "#     timestamps_700s = pd.date_range(start=pd.to_datetime(0, unit='s'), periods=n_rows, freq=f'{1428571}ns')\n",
    "#     df['time'] = timestamps_700s\n",
    "#     df['time'] = df['time'].apply(lambda timestamp: timestamp.timestamp())\n",
    "\n",
    "#     # 364740 rows contain null rows, backfill / bfill is used\n",
    "#     # which uses next valid observation to fill gap. resulting df\n",
    "#     # will not contain nulls anymore\n",
    "#     df['label'] = df['label'].fillna(method='bfill')\n",
    "#     df['label'] = df['label'].apply(new_label_mapper)\n",
    "\n",
    "#     # reset index\n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     if norm_type is 'std':\n",
    "#         # std norm\n",
    "#         df = (df - df.mean()) / df.std()\n",
    "#     elif norm_type is 'minmax':\n",
    "#         # minmax norm\n",
    "#         df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "#     # No. of Labels ==> 8 ; \n",
    "#     # 0 = not defined / transient\n",
    "#     # 1 = baseline\n",
    "#     # 2 = stress\n",
    "#     # 3 = amusement\n",
    "#     # 4 = meditation\n",
    "#     # 5/6/7 = should be ignored in this dataset\n",
    "#     # instead of this in order to retain 700hz sampling freq we can turn instead the 4/5/6/7/ and 0 labels will be\n",
    "\n",
    "#     # ff. are value counts of each label in 700hz sampling frequency\n",
    "#     # 0.0    2326361 for transient\n",
    "#     # 1.0     869440 for baseline\n",
    "#     # 4.0     583679 for meditation\n",
    "#     # 2.0     467400 for stress\n",
    "#     # 3.0     275120 for amusement\n",
    "#     # 6.0      49400 should be ignored\n",
    "#     # 7.0      48640 should be ignored\n",
    "\n",
    "#     # what we can do instead is have the dataset have some of its labels\n",
    "#     # combined that results in less labels that best indicate the\n",
    "#     # affective states of an individual\n",
    "\n",
    "#     # i.e. transience, and labels 6 and 7, can be combined to form a transient state of being (0)\n",
    "#     # baseline can be a state of being  (1)\n",
    "#     # meditation can be a state of being (4)\n",
    "#     # stress can be a state of being (2)\n",
    "#     # amusement can be a state of being (3)\n",
    "\n",
    "#     # that way we can retain still the 700hz sampling freq without removing rows or samples in the data\n",
    "#     return df\n",
    "\n",
    "# def make_patient_data(subject_id):\n",
    "#     global savePath\n",
    "#     global WINDOW_IN_SECONDS\n",
    "\n",
    "#     # Make subject data object for Sx\n",
    "#     subject = SubjectData(main_path='./data/WESAD', subject_number=subject_id)\n",
    "\n",
    "#     # Empatica E4 data - now with resp\n",
    "#     e4_data_dict = subject.get_wrist_data()\n",
    "#     # print(f'{e4_data_dict}')\n",
    "\n",
    "#     # norm type\n",
    "#     norm_type = None\n",
    "\n",
    "#     # The 3 classes we are classifying\n",
    "#     # print(f'subject labels: {subject.labels}, type: {type(subject.labels)}, shape: {subject.labels.shape}')\n",
    "#     df = compute_features(e4_data_dict, subject.labels, norm_type)\n",
    "\n",
    "#     # Save file as csv (for now)\n",
    "#     df.to_csv(f'{savePath}{subject_feature_path}/S{subject_id}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "\n",
    "# for patient in subject_ids:\n",
    "#     print(f'Processing data for S{patient}...')\n",
    "#     make_patient_data(patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utilities.feature_extractors import interpolate_signals\n",
    "# from utilities.visualizers import view_time_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2_700hz = pd.read_csv('./data/Stress Detection Data/S2.csv', index_col=0)\n",
    "# s2_700hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end_time_s = round(s2_700hz['time'].iloc[-1])\n",
    "# end_time_s * 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view_time_frame(s2_700hz, samp_freq=700, begin_time_s=0, end_time_s=end_time_s, cols_to_use=['EDA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2_700hz['EDA'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2_700hz['EDA'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2_4hz = interpolate_signals(s2_700hz, sample_rate=700, start_time=pd.to_datetime(0, unit='s'), target_hz=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2_4hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2_4hz['EDA'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2_4hz['EDA'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2_4hz.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2_4hz['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end_time_s = round(s2_4hz['time'].iloc[-1])\n",
    "# end_time_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end_time_s * 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-writing-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
