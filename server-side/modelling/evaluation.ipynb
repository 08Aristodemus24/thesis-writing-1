{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import requests\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# import and load model architectures as well as decoder\n",
    "from models.cueva import LSTM_FE\n",
    "from models.llanes_jurado import LSTM_CNN\n",
    "from utilities.preprocessors import correct_signals\n",
    "from utilities.loaders import load_meta_data, load_model, load_lookup_array, save_meta_data, concur_load_data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load engineered test features from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'cueva_second_phase-svm':{\n",
    "\n",
    "    },\n",
    "    'cueva-lstm-fe': {\n",
    "        # 'model':\n",
    "        # 'hyper_params':\n",
    "    },\n",
    "    'jurado-lstm-cnn': {\n",
    "        # 'model':\n",
    "        # 'hyper_params':\n",
    "    },\n",
    "    'taylor-svm': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "    },\n",
    "    'taylor-lr': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "    },\n",
    "    'taylor-rf': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "    },\n",
    "    'hossain-gbt': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "        # 'scaler':\n",
    "    },\n",
    "    'hossain-svm': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "        # 'scaler':\n",
    "    },\n",
    "    'hossain-lr': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "        # 'scaler':\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_miscs():\n",
    "    \"\"\"\n",
    "    loads miscellaneous variables to be used by the model\n",
    "    \"\"\"\n",
    "\n",
    "    global models\n",
    "\n",
    "    print('loading miscellaneous...')\n",
    "    # this is for loading miscellaneous variables for \n",
    "    # deep learning models such as hyper parameters\n",
    "    lstm_fe_hp = load_meta_data('./saved/misc/cueva_lstm-fe_meta_data.json')\n",
    "    lstm_cnn_hp = load_meta_data('./saved/misc/jurado_lstm-cnn_meta_data.json')\n",
    "\n",
    "    models['cueva-lstm-fe']['hyper_params'] = lstm_fe_hp\n",
    "    models['jurado-lstm-cnn']['hyper_params'] = lstm_cnn_hp\n",
    "\n",
    "    # this is for loading miscellaneous variables for\n",
    "    # machine learning models such as the reduced feature set\n",
    "    taylor_lr_red_feats = load_lookup_array(f'./data/Artifact Detection Data/reduced_taylor_lr_feature_set.txt')\n",
    "    # taylor_svm_red_feats = load_lookup_array(f'./data/Artifact Detection Data/reduced_taylor_svm_feature_set.txt')\n",
    "    # taylor_rf_red_feats = load_lookup_array(f'./data/Artifact Detection Data/reduced_taylor_rf_feature_set.txt')\n",
    "    hossain_lr_red_feats = load_lookup_array(f'./data/Artifact Detection Data/reduced_hossain_lr_feature_set.txt')\n",
    "    # hossain_svm_red_feats = load_lookup_array(f'./data/Artifact Detection Data/reduced_hossain_svm_feature_set.txt')\n",
    "    # hossain_gbt_red_feats = load_lookup_array(f'./data/Artifact Detection Data/reduced_hossain_gbt_feature_set.txt')\n",
    "    # cueva_second_phase_svm_red_feats = load_lookup_array(f'./data/Artifact Detection Data/reduced_cueva_second_phase_svm_feature_set1.txt')\n",
    "\n",
    "    # pre-load reduced features here so that features don't have to \n",
    "    # be loaded every single time user makes a request\n",
    "    models['taylor-lr']['selected_feats'] = taylor_lr_red_feats\n",
    "    # models['taylor-svm']['selected_feats'] = taylor_svm_red_feats\n",
    "    # models['taylor-rf']['selected_feats'] = taylor_rf_red_feats\n",
    "    models['hossain-lr']['selected_feats'] = hossain_lr_red_feats\n",
    "    # models['hossain-svm']['selected_feats'] = hossain_svm_red_feats\n",
    "    # models['hossain-gbt']['selected_feats'] = hossain_gbt_red_feats\n",
    "    # models['cueva_second_phase-svm']['selected_feats'] = cueva_second_phase_svm_red_feats\n",
    "\n",
    "    print('miscellaneous loaded.')\n",
    "\n",
    "\n",
    "def load_preprocessors():\n",
    "    \"\"\"\n",
    "    prepares and loads the saved encoders, normalizers of\n",
    "    the dataset to later transform raw user input from\n",
    "    client-side\n",
    "    \"\"\"\n",
    "    global models\n",
    "\n",
    "    print('loading preprocessors...')\n",
    "\n",
    "    # pre-load here scaler of hossain used during training\n",
    "    hossain_lr_scaler = load_model('./saved/misc/hossain_lr_scaler.pkl')\n",
    "    # hossain_svm_scaler = load_model('./saved/misc/hossain_svm_scaler.pkl')\n",
    "    # hossain_gbt_scaler = load_model('./saved/misc/hossain_gbt_scaler.pkl')\n",
    "\n",
    "    models['hossain-lr']['scaler'] = hossain_lr_scaler\n",
    "    # models['hossain-svm']['scaler'] = hossain_svm_scaler\n",
    "    # models['hossain-gbt']['scaler'] = hossain_gbt_scaler\n",
    "\n",
    "    print('preprocessors loaded.')\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"\n",
    "    prepares and loads sample input and custom model in\n",
    "    order to use trained weights/parameters/coefficients\n",
    "    \"\"\"\n",
    "    global models\n",
    "    \n",
    "    print('loading models...')\n",
    "    # pre load saved weights for deep learning models\n",
    "    jurado_lstm_cnn = LSTM_CNN(**models['jurado-lstm-cnn']['hyper_params'])\n",
    "    jurado_lstm_cnn.load_weights('./saved/weights/EDABE_LSTM_1DCNN_Model.h5')\n",
    "\n",
    "    # load side task model and convert it to a feature extractor model \n",
    "    lstm_fe = LSTM_FE(**models['cueva-lstm-fe']['hyper_params'])\n",
    "    lstm_fe.load_weights('./saved/weights/cueva_lstm-fe_21_0.7489.weights.h5')\n",
    "    # lstm_layer_2 = lstm_fe.get_layer('lstm-layer-2')\n",
    "    # lstm_fe_main = tf.keras.Model(inputs=lstm_fe.inputs, outputs=lstm_layer_2.output)\n",
    "\n",
    "    # # pre load saved machine learning models\n",
    "    taylor_lr = load_model('./saved/models/taylor_lr_clf.pkl')\n",
    "    # taylor_svm = load_model('./saved/models/taylor_svm_clf.pkl')\n",
    "    # taylor_rf = load_model('./saved/models/taylor_rf_clf.pkl')\n",
    "    hossain_lr = load_model('./saved/models/hossain_lr_clf.pkl')\n",
    "    # hossain_svm = load_model('./saved/models/hossain_svm_clf.pkl')\n",
    "    # hossain_gbt = load_model('./saved/models/hossain_gbt_clf.pkl')\n",
    "    # cueva_second_phase_svm = load_model('./saved/models/cueva_second_phase_svm_clf1.pkl')\n",
    "\n",
    "    # populate dictionary with loaded models\n",
    "    models['jurado-lstm-cnn']['model'] = jurado_lstm_cnn\n",
    "    models['cueva-lstm-fe']['model'] = lstm_fe\n",
    "\n",
    "    models['taylor-lr']['model'] = taylor_lr\n",
    "    # models['taylor-svm']['model'] = taylor_svm\n",
    "    # models['taylor-rf']['model'] = taylor_rf\n",
    "    models['hossain-lr']['model'] = hossain_lr\n",
    "    # models['hossain-svm']['model'] = hossain_svm\n",
    "    # models['hossain-gbt']['model'] = hossain_gbt\n",
    "    # models['cueva_second_phase-svm']['model'] = cueva_second_phase_svm\n",
    "\n",
    "    print('models loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_miscs()\n",
    "load_preprocessors()\n",
    "load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = ['cueva_second_phase-svm', 'taylor-lr', 'taylor-rf', 'taylor-svm', 'hossain-lr', 'hossain-gbt', 'hossain-svm']\n",
    "model_names = ['taylor-lr', 'hossain-lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = [\"test\", \"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load engineered test and trained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    selector_config, estimator_name = model_name.split('-', 1)\n",
    "    \n",
    "    for data_split in data_splits:\n",
    "        loader_args = {\n",
    "            'feat_config': selector_config, \n",
    "            'data_split': data_split,\n",
    "            'exc_lof': False\n",
    "        } if selector_config == 'cueva_second_phase' else {\n",
    "            'feat_config': selector_config, \n",
    "            'data_split': data_split,\n",
    "            'exc_lof': False\n",
    "        }\n",
    "\n",
    "        subjects_features, subjects_labels, subjects_names, subject_to_id = concur_load_data(**loader_args)\n",
    "        print(f'selector config: {selector_config}')\n",
    "        print(f'estimator name: {estimator_name}')\n",
    "        # print(f'subjects features shape: {subjects_features.shape}')\n",
    "        # print(f'subjects labels shape: {subjects_labels.shape}\\n')\n",
    "        \n",
    "        # loop through each generated features dataframes from test subjects signals and feed repeatedly to a trained ml models\n",
    "        for index, subject_name in enumerate(subjects_names):\n",
    "            # print(f'subject features columns: {subjects_features[subjects_features['subject_id'] == index].columns}')\n",
    "            print(f'subject: {subject_name}')\n",
    "            # once features are extracted features selected during\n",
    "            # tuning will be used in testing as done also during training\n",
    "\n",
    "            selected_feats = models[model_name]['selected_feats']\n",
    "\n",
    "            if loader_args.get('exc_lof') == None or loader_args.get('exc_lof') == False:\n",
    "                subject_features = subjects_features.loc[subjects_features['subject_id'] == index, selected_feats]\n",
    "                subject_labels = subjects_labels[subjects_labels['subject_id'] == index].drop(columns=['subject_id'])\n",
    "                print(f'subject features shape: {subject_features.shape}')\n",
    "                print(f'subject labels shape: {subject_labels.shape}\\n')\n",
    "\n",
    "            # this will only fire if exc_lof is not None or is false\n",
    "            else:\n",
    "                # if user excludes lower order features, higher order features will only be loaded\n",
    "                subject_features = subjects_features[subjects_features['subject_id'] == index].drop(columns=['subject_id'])\n",
    "                subject_labels = subjects_labels[subjects_labels['subject_id'] == index].drop(columns=['subject_id'])\n",
    "                \n",
    "                print(f'subject features shape: {subject_features.shape}')\n",
    "                print(f'subject labels shape: {subject_labels.shape}\\n')\n",
    "\n",
    "            # convert features and labels into numpy matrices\n",
    "            X = subject_features.to_numpy()\n",
    "            Y = subject_labels.to_numpy().ravel()\n",
    "\n",
    "            # if hossain is the researcher chosen the scaler used during training\n",
    "            # will be used to scale the test subject features\n",
    "            if selector_config == \"hossain\":    \n",
    "                scaler = models[model_name]['scaler']\n",
    "                X = scaler.transform(X)\n",
    "\n",
    "            model = models[model_name]['model']\n",
    "            Y_pred = model.predict(X)\n",
    "            Y_pred_prob = model.predict_proba(X)\n",
    "            print(f\"predicted Y: {Y_pred}\")\n",
    "            print(f\"unique values and counts: {np.unique(Y_pred, return_counts=True)}\")\n",
    "            print(f\"true Y: {Y}\")\n",
    "            print(f\"unique values and counts: {np.unique(Y, return_counts=True)}\")\n",
    "\n",
    "            # compute performance metric values for test subject\n",
    "            acc = accuracy_score(y_true=Y, y_pred=Y_pred)\n",
    "            prec = precision_score(y_true=Y, y_pred=Y_pred, average=\"weighted\")\n",
    "            rec = recall_score(y_true=Y, y_pred=Y_pred, average=\"weighted\")\n",
    "            f1 = f1_score(y_true=Y, y_pred=Y_pred, average=\"weighted\")\n",
    "            roc_auc = roc_auc_score(y_true=Y, y_score=Y_pred_prob[:, 1], average=\"weighted\", multi_class=\"ovo\")\n",
    "            conf_matrix = confusion_matrix(Y, Y_pred).tolist()\n",
    "\n",
    "            print(f\"{data_split} acc: {acc} \\\n",
    "            \\n{data_split} prec: {prec} \\\n",
    "            \\n{data_split} rec: {rec} \\\n",
    "            \\n{data_split} f1: {f1} \\\n",
    "            \\n{data_split} roc_auc: {roc_auc} \\\n",
    "            \\n{data_split} conf_matrix: {conf_matrix}\")\n",
    "\n",
    "            results = models[f'{model_name}'].get(f'{data_split}_results', [])\n",
    "            results.append(\n",
    "                (subject_name, {\n",
    "                    f'{data_split}_acc': acc,\n",
    "                    f'{data_split}_prec': prec, \n",
    "                    f'{data_split}_rec': rec,\n",
    "                    f'{data_split}_f1': f1,\n",
    "                    f'{data_split}_roc_auc': roc_auc,\n",
    "                    f'{data_split}_conf_matrix': conf_matrix\n",
    "                })\n",
    "            )\n",
    "            models[f'{model_name}'][f'{data_split}_results'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m242/392\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 41ms/step"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m models[model_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# depending on dl model Y_pred will either be unactivated logits \u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# or sigmoid probabilities\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# when our predictions is 0.2, 0.15, and below which is <= 0.2 then y_pred will be 0\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# when our predictions is 1, 0.5, 0.4, 0.3, 0.21, and above which is > 0.2 then y_pred will be 1\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# why we do this is because of the imbalance of our dataset, and we\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# want to place a threshold of 20% since there our dataset only consists\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# of 20% of positive classes. Note this conversion is to be used in precision and recall metrics\u001b[39;00m\n\u001b[0;32m     25\u001b[0m Y_pred_whole \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(Y_pred \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, tf\u001b[38;5;241m.\u001b[39mint64)\n",
      "File \u001b[1;32mc:\\Users\\LARRY\\anaconda3\\envs\\thesis-writing-1\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\LARRY\\anaconda3\\envs\\thesis-writing-1\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:508\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    506\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m    507\u001b[0m data \u001b[38;5;241m=\u001b[39m get_data(iterator)\n\u001b[1;32m--> 508\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m outputs \u001b[38;5;241m=\u001b[39m append_to_outputs(batch_outputs, outputs)\n\u001b[0;32m    510\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_end(step, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_outputs})\n",
      "File \u001b[1;32mc:\\Users\\LARRY\\anaconda3\\envs\\thesis-writing-1\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\LARRY\\anaconda3\\envs\\thesis-writing-1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\LARRY\\anaconda3\\envs\\thesis-writing-1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LARRY\\anaconda3\\envs\\thesis-writing-1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LARRY\\anaconda3\\envs\\thesis-writing-1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\LARRY\\anaconda3\\envs\\thesis-writing-1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\LARRY\\anaconda3\\envs\\thesis-writing-1\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\LARRY\\anaconda3\\envs\\thesis-writing-1\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\LARRY\\anaconda3\\envs\\thesis-writing-1\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for model_name in [\"cueva-lstm-fe\", \"jurado-lstm-cnn\"]:\n",
    "    selector_config, estimator_name = model_name.split('-', 1)\n",
    "\n",
    "    for data_split in data_splits:\n",
    "        subjects_signals, subjects_labels, subjects_names, subject_to_id = concur_load_data(feat_config=selector_config, data_split=data_split)\n",
    "        print(f'selector config: {selector_config}')\n",
    "        print(f'estimator name: {estimator_name}')\n",
    "\n",
    "        for index, subject_name in enumerate(subjects_names):\n",
    "            X = subjects_signals[index]\n",
    "            Y = subjects_labels[index]\n",
    "\n",
    "            # assign model of jurado to variable\n",
    "            model = models[model_name]['model']\n",
    "\n",
    "            # depending on dl model Y_pred will either be unactivated logits \n",
    "            # or sigmoid probabilities\n",
    "            Y_pred = model.predict(X)\n",
    "            \n",
    "            # when our predictions is 0.2, 0.15, and below which is <= 0.2 then y_pred will be 0\n",
    "            # when our predictions is 1, 0.5, 0.4, 0.3, 0.21, and above which is > 0.2 then y_pred will be 1\n",
    "            # why we do this is because of the imbalance of our dataset, and we\n",
    "            # want to place a threshold of 20% since there our dataset only consists\n",
    "            # of 20% of positive classes. Note this conversion is to be used in precision and recall metrics\n",
    "            Y_pred_whole = tf.cast(Y_pred >= 0.2, tf.int64)\n",
    "\n",
    "            print(f\"predicted Y: {Y_pred_whole}\")\n",
    "            print(f\"unique values and counts: {np.unique(Y_pred_whole, return_counts=True)}\")\n",
    "            print(f\"true Y: {Y}\")\n",
    "            print(f\"unique values and counts: {np.unique(Y, return_counts=True)}\")\n",
    "\n",
    "            # compute performance metric values for test subject\n",
    "            acc = accuracy_score(y_true=Y, y_pred=Y_pred_whole)\n",
    "            prec = precision_score(y_true=Y, y_pred=Y_pred_whole, average=\"weighted\")\n",
    "            rec = recall_score(y_true=Y, y_pred=Y_pred_whole, average=\"weighted\")\n",
    "            f1 = f1_score(y_true=Y, y_pred=Y_pred_whole, average=\"weighted\")\n",
    "            roc_auc = roc_auc_score(y_true=Y, y_score=Y_pred, average=\"weighted\", multi_class=\"ovo\")\n",
    "            conf_matrix = confusion_matrix(Y, Y_pred_whole).tolist()\n",
    "\n",
    "            print(f\"{data_split} acc: {acc} \\\n",
    "                \\n{data_split} prec: {prec} \\\n",
    "                \\n{data_split} rec: {rec} \\\n",
    "                \\n{data_split} f1: {f1} \\\n",
    "                \\n{data_split} roc_auc: {roc_auc} \\\n",
    "                \\n{data_split} conf_matrix: {conf_matrix}\")\n",
    "            \n",
    "            results = models[model_name].get(f'{data_split}_results', [])\n",
    "            results.append(\n",
    "                (subject_name, {\n",
    "                    f'{data_split}_acc': acc,\n",
    "                    f'{data_split}_prec': prec, \n",
    "                    f'{data_split}_rec': rec,\n",
    "                    f'{data_split}_f1': f1,\n",
    "                    f'{data_split}_roc_auc': roc_auc,\n",
    "                    f'{data_split}_conf_matrix': conf_matrix\n",
    "                })\n",
    "            )\n",
    "            models[model_name][f'{data_split}_results'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = models.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in copy.items():\n",
    "    if copy[key].get('model') is not None:\n",
    "        del copy[key]['model']\n",
    "\n",
    "    if copy[key].get('scaler') is not None:\n",
    "        del copy[key]['scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_meta_data('./results/all_models_results.json', copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-writing-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
