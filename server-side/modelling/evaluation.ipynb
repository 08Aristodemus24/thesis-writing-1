{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import requests\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# import and load model architectures as well as decoder\n",
    "from models.cueva import LSTM_FE\n",
    "from models.llanes_jurado import LSTM_CNN\n",
    "from utilities.preprocessors import correct_signals\n",
    "from utilities.loaders import load_meta_data, load_model, load_lookup_array, save_meta_data, concur_load_data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load engineered test features from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'cueva_second_phase_1-5-weighted-svm':{\n",
    "\n",
    "    },\n",
    "    'cueva_second_phase_1-9-weighted-svm':{\n",
    "\n",
    "    },\n",
    "    'cueva_second_phase_1-2-weighted-svm':{\n",
    "\n",
    "    },\n",
    "    'cueva-lstm-fe': {\n",
    "        # 'model':\n",
    "        # 'hyper_params':\n",
    "    },\n",
    "    'jurado-lstm-cnn': {\n",
    "        # 'model':\n",
    "        # 'hyper_params':\n",
    "    },\n",
    "    'taylor-svm': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "    },\n",
    "    'taylor-lr': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "    },\n",
    "    'taylor-rf': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "    },\n",
    "    'hossain-gbt': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "        # 'scaler':\n",
    "    },\n",
    "    'hossain-svm': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "        # 'scaler':\n",
    "    },\n",
    "    'hossain-lr': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "        # 'scaler':\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_miscs():\n",
    "    \"\"\"\n",
    "    loads miscellaneous variables to be used by the model\n",
    "    \"\"\"\n",
    "\n",
    "    global models\n",
    "\n",
    "    print('loading miscellaneous...')\n",
    "    # this is for loading miscellaneous variables for \n",
    "    # deep learning models such as hyper parameters\n",
    "    lstm_fe_hp = load_meta_data('./saved/misc/cueva_lstm-fe_meta_data.json')\n",
    "    lstm_cnn_hp = load_meta_data('./saved/misc/jurado_lstm-cnn_meta_data.json')\n",
    "\n",
    "    models['cueva-lstm-fe']['hyper_params'] = lstm_fe_hp\n",
    "    models['jurado-lstm-cnn']['hyper_params'] = lstm_cnn_hp\n",
    "\n",
    "    # this is for loading miscellaneous variables for\n",
    "    # machine learning models such as the reduced feature set\n",
    "    taylor_lr_red_feats = load_lookup_array(f'./data/Artifact Detection Data/reduced_taylor_lr_feature_set.txt')\n",
    "    taylor_svm_red_feats = load_lookup_array(f'./data/Artifact Detection Data/reduced_taylor_svm_feature_set.txt')\n",
    "    taylor_rf_red_feats = load_lookup_array(f'./data/Artifact Detection Data/reduced_taylor_rf_feature_set.txt')\n",
    "    hossain_lr_red_feats = load_lookup_array(f'./data/Artifact Detection Data/reduced_hossain_lr_feature_set.txt')\n",
    "    hossain_svm_red_feats = load_lookup_array(f'./data/Artifact Detection Data/reduced_hossain_svm_feature_set.txt')\n",
    "    hossain_gbt_red_feats = load_lookup_array(f'./data/Artifact Detection Data/reduced_hossain_gbt_feature_set.txt')\n",
    "    # cueva_second_phase_svm_red_feats = load_lookup_array(f'./data/Artifact Detection Data/reduced_cueva_second_phase_svm_feature_set.txt')\n",
    "\n",
    "    # pre-load reduced features here so that features don't have to \n",
    "    # be loaded every single time user makes a request\n",
    "    models['taylor-lr']['selected_feats'] = taylor_lr_red_feats\n",
    "    models['taylor-svm']['selected_feats'] = taylor_svm_red_feats\n",
    "    models['taylor-rf']['selected_feats'] = taylor_rf_red_feats\n",
    "    models['hossain-lr']['selected_feats'] = hossain_lr_red_feats\n",
    "    models['hossain-svm']['selected_feats'] = hossain_svm_red_feats\n",
    "    models['hossain-gbt']['selected_feats'] = hossain_gbt_red_feats\n",
    "    # models['cueva_second_phase-svm']['selected_feats'] = cueva_second_phase_svm_red_feats\n",
    "\n",
    "    print('miscellaneous loaded.')\n",
    "\n",
    "\n",
    "def load_preprocessors():\n",
    "    \"\"\"\n",
    "    prepares and loads the saved encoders, normalizers of\n",
    "    the dataset to later transform raw user input from\n",
    "    client-side\n",
    "    \"\"\"\n",
    "    global models\n",
    "\n",
    "    print('loading preprocessors...')\n",
    "\n",
    "    # pre-load here scaler of hossain used during training\n",
    "    hossain_lr_scaler = load_model('./saved/misc/hossain_lr_scaler.pkl')\n",
    "    hossain_svm_scaler = load_model('./saved/misc/hossain_svm_scaler.pkl')\n",
    "    hossain_gbt_scaler = load_model('./saved/misc/hossain_gbt_scaler.pkl')\n",
    "\n",
    "    models['hossain-lr']['scaler'] = hossain_lr_scaler\n",
    "    models['hossain-svm']['scaler'] = hossain_svm_scaler\n",
    "    models['hossain-gbt']['scaler'] = hossain_gbt_scaler\n",
    "\n",
    "    print('preprocessors loaded.')\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"\n",
    "    prepares and loads sample input and custom model in\n",
    "    order to use trained weights/parameters/coefficients\n",
    "    \"\"\"\n",
    "    global models\n",
    "    \n",
    "    print('loading models...')\n",
    "    # pre load saved weights for deep learning models\n",
    "    jurado_lstm_cnn = LSTM_CNN(**models['jurado-lstm-cnn']['hyper_params'])\n",
    "    jurado_lstm_cnn.load_weights('./saved/weights/EDABE_LSTM_1DCNN_Model.h5')\n",
    "\n",
    "    # load side task model and convert it to a feature extractor model \n",
    "    lstm_fe = LSTM_FE(**models['cueva-lstm-fe']['hyper_params'])\n",
    "    lstm_fe.load_weights('./saved/weights/cueva_lstm-fe_21_0.7489.weights.h5')\n",
    "    lstm_layer_2 = lstm_fe.get_layer('lstm-layer-2')\n",
    "    lstm_fe_main = tf.keras.Model(inputs=lstm_fe.inputs, outputs=lstm_layer_2.output)\n",
    "\n",
    "    # # pre load saved machine learning models\n",
    "    taylor_lr = load_model('./saved/models/taylor_lr_clf.pkl')\n",
    "    taylor_svm = load_model('./saved/models/taylor_svm_clf.pkl')\n",
    "    taylor_rf = load_model('./saved/models/taylor_rf_clf.pkl')\n",
    "    hossain_lr = load_model('./saved/models/hossain_lr_clf.pkl')\n",
    "    hossain_svm = load_model('./saved/models/hossain_svm_clf.pkl')\n",
    "    hossain_gbt = load_model('./saved/models/hossain_gbt_clf.pkl')\n",
    "    # cueva_second_phase_1_5_weighted_svm = load_model('./saved/models/cueva_second_phase_1_5_weighted_svm_clf.pkl')\n",
    "    # cueva_second_phase_1_9_weighted_svm = load_model('./saved/models/cueva_second_phase_1_9_weighted_svm_clf.pkl')\n",
    "    # cueva_second_phase_1_2_weighted_svm = load_model('./saved/models/cueva_second_phase_1_2_weighted_svm_clf.pkl')\n",
    "\n",
    "    # populate dictionary with loaded models\n",
    "    models['jurado-lstm-cnn']['model'] = jurado_lstm_cnn\n",
    "    models['cueva-lstm-fe']['model'] = lstm_fe\n",
    "\n",
    "    models['taylor-lr']['model'] = taylor_lr\n",
    "    models['taylor-svm']['model'] = taylor_svm\n",
    "    models['taylor-rf']['model'] = taylor_rf\n",
    "    models['hossain-lr']['model'] = hossain_lr\n",
    "    models['hossain-svm']['model'] = hossain_svm\n",
    "    models['hossain-gbt']['model'] = hossain_gbt\n",
    "    # models['cueva_second_phase_1-5-weighted-svm']['model'] = cueva_second_phase_1_5_weighted_svm\n",
    "    # models['cueva_second_phase_1-9-weighted-svm']['model'] = cueva_second_phase_1_9_weighted_svm\n",
    "    # models['cueva_second_phase_1-2-weighted-svm']['model'] = cueva_second_phase_1_2_weighted_svm\n",
    "\n",
    "    print('models loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading miscellaneous...\n",
      "miscellaneous loaded.\n",
      "loading preprocessors...\n",
      "preprocessors loaded.\n",
      "loading models...\n",
      "models loaded.\n"
     ]
    }
   ],
   "source": [
    "load_miscs()\n",
    "load_preprocessors()\n",
    "load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cueva_second_phase_1-5-weighted-svm': {},\n",
       " 'cueva_second_phase_1-9-weighted-svm': {},\n",
       " 'cueva_second_phase_1-2.5-weighted-svm': {},\n",
       " 'cueva-lstm-fe': {'hyper_params': {'window_size': 640,\n",
       "   'n_a': 32,\n",
       "   'drop_prob': 0.3,\n",
       "   'lamb_da': 0.1},\n",
       "  'model': <Sequential name=sequential, built=True>},\n",
       " 'jurado-lstm-cnn': {'hyper_params': {'window_size': 640,\n",
       "   'n_a': 16,\n",
       "   'drop_prob': 0.05},\n",
       "  'model': <Functional name=functional, built=True>},\n",
       " 'taylor-svm': {'selected_feats': ['raw_128hz_min',\n",
       "   'raw_128hz_median',\n",
       "   'raw_128hz_std',\n",
       "   'raw_128hz_range',\n",
       "   'raw_128hz_1d_max',\n",
       "   'raw_128hz_1d_min',\n",
       "   'raw_128hz_1d_amp',\n",
       "   'raw_128hz_1d_std',\n",
       "   'filt_128hz_min',\n",
       "   'filt_128hz_std',\n",
       "   'filt_128hz_range',\n",
       "   'filt_128hz_1d_min',\n",
       "   'filt_128hz_1d_std',\n",
       "   'filt_128hz_1d_range',\n",
       "   'filt_128hz_1d_avg_abs',\n",
       "   'third_16thofa_sec_max',\n",
       "   'first_16thofa_sec_mean',\n",
       "   'third_16thofa_sec_mean',\n",
       "   'second_16thofa_sec_std',\n",
       "   'third_16thofa_sec_std',\n",
       "   'first_32thofa_sec_mean',\n",
       "   'first_32thofa_sec_std',\n",
       "   'second_32thofa_sec_std',\n",
       "   'raw_16hz_amp',\n",
       "   'raw_16hz_std',\n",
       "   'filt_16hz_max',\n",
       "   'filt_16hz_min',\n",
       "   'filt_16hz_std',\n",
       "   'filt_16hz_range',\n",
       "   'filt_16hz_1d_max',\n",
       "   'filt_16hz_1d_min',\n",
       "   'filt_16hz_1d_std',\n",
       "   'filt_16hz_1d_range',\n",
       "   'first_2thofa_sec_max',\n",
       "   'second_2thofa_sec_max',\n",
       "   'second_2thofa_sec_mean',\n",
       "   'second_2thofa_sec_median',\n",
       "   'second_4thofa_sec_max',\n",
       "   'second_4thofa_sec_std',\n",
       "   'second_4thofa_sec_median'],\n",
       "  'model': SVC(C=10, gamma=0.1, probability=True, verbose=1)},\n",
       " 'taylor-lr': {'selected_feats': ['raw_128hz_min',\n",
       "   'raw_128hz_amp',\n",
       "   'raw_128hz_std',\n",
       "   'raw_128hz_1d_max',\n",
       "   'raw_128hz_1d_min',\n",
       "   'raw_128hz_1d_amp',\n",
       "   'raw_128hz_1d_std',\n",
       "   'raw_128hz_1d_avg_abs',\n",
       "   'raw_128hz_2d_avg_abs',\n",
       "   'filt_128hz_min',\n",
       "   'filt_128hz_std',\n",
       "   'filt_128hz_range',\n",
       "   'filt_128hz_1d_min',\n",
       "   'filt_128hz_1d_std',\n",
       "   'filt_128hz_1d_range',\n",
       "   'third_16thofa_sec_max',\n",
       "   'first_16thofa_sec_mean',\n",
       "   'third_16thofa_sec_mean',\n",
       "   'second_16thofa_sec_std',\n",
       "   'third_16thofa_sec_std',\n",
       "   'first_16thofa_sec_median',\n",
       "   'first_32thofa_sec_mean',\n",
       "   'second_32thofa_sec_mean',\n",
       "   'first_32thofa_sec_std',\n",
       "   'second_32thofa_sec_std',\n",
       "   'raw_16hz_min',\n",
       "   'raw_16hz_std',\n",
       "   'raw_16hz_1d_min',\n",
       "   'filt_16hz_min',\n",
       "   'filt_16hz_std',\n",
       "   'filt_16hz_range',\n",
       "   'filt_16hz_1d_std',\n",
       "   'filt_16hz_1d_range',\n",
       "   'first_2thofa_sec_max',\n",
       "   'second_2thofa_sec_max',\n",
       "   'third_2thofa_sec_max',\n",
       "   'second_2thofa_sec_mean',\n",
       "   'second_2thofa_sec_median',\n",
       "   'second_4thofa_sec_max',\n",
       "   'second_4thofa_sec_std'],\n",
       "  'model': LogisticRegression(C=0.1, verbose=1)},\n",
       " 'taylor-rf': {'selected_feats': ['raw_128hz_min',\n",
       "   'raw_128hz_amp',\n",
       "   'raw_128hz_std',\n",
       "   'raw_128hz_1d_min',\n",
       "   'raw_128hz_1d_amp',\n",
       "   'raw_128hz_1d_std',\n",
       "   'raw_128hz_1d_avg_abs',\n",
       "   'raw_128hz_2d_avg_abs',\n",
       "   'filt_128hz_min',\n",
       "   'filt_128hz_median',\n",
       "   'filt_128hz_std',\n",
       "   'filt_128hz_range',\n",
       "   'filt_128hz_1d_min',\n",
       "   'filt_128hz_1d_std',\n",
       "   'filt_128hz_1d_range',\n",
       "   'third_16thofa_sec_max',\n",
       "   'first_16thofa_sec_mean',\n",
       "   'third_16thofa_sec_mean',\n",
       "   'second_16thofa_sec_std',\n",
       "   'third_16thofa_sec_std',\n",
       "   'first_16thofa_sec_median',\n",
       "   'first_32thofa_sec_mean',\n",
       "   'second_32thofa_sec_mean',\n",
       "   'first_32thofa_sec_std',\n",
       "   'second_32thofa_sec_std',\n",
       "   'raw_16hz_min',\n",
       "   'raw_16hz_amp',\n",
       "   'raw_16hz_std',\n",
       "   'filt_16hz_min',\n",
       "   'filt_16hz_std',\n",
       "   'filt_16hz_range',\n",
       "   'filt_16hz_1d_min',\n",
       "   'filt_16hz_1d_std',\n",
       "   'filt_16hz_1d_range',\n",
       "   'second_2thofa_sec_max',\n",
       "   'second_2thofa_sec_mean',\n",
       "   'third_2thofa_sec_mean',\n",
       "   'first_4thofa_sec_max',\n",
       "   'second_4thofa_sec_max',\n",
       "   'second_4thofa_sec_std'],\n",
       "  'model': RandomForestClassifier(n_estimators=600, verbose=1)},\n",
       " 'hossain-gbt': {'selected_feats': ['raw_128hz_max',\n",
       "   'raw_128hz_min',\n",
       "   'raw_128hz_amp',\n",
       "   'raw_128hz_median',\n",
       "   'raw_128hz_std',\n",
       "   'raw_128hz_range',\n",
       "   'raw_128hz_shannon_entropy',\n",
       "   'raw_128hz_1d_max',\n",
       "   'raw_128hz_1d_min',\n",
       "   'raw_128hz_1d_amp',\n",
       "   'raw_128hz_1d_std',\n",
       "   'raw_128hz_1d_range',\n",
       "   'raw_128hz_1d_shannon_entropy',\n",
       "   'filt_128hz_max',\n",
       "   'filt_128hz_min',\n",
       "   'filt_128hz_amp',\n",
       "   'filt_128hz_median',\n",
       "   'filt_128hz_std',\n",
       "   'filt_128hz_range',\n",
       "   'filt_128hz_1d_max',\n",
       "   'filt_128hz_1d_min',\n",
       "   'filt_128hz_1d_amp',\n",
       "   'filt_128hz_1d_std',\n",
       "   'filt_128hz_1d_range',\n",
       "   'ar_coeff_1_128hz',\n",
       "   'ar_coeff_2_128hz',\n",
       "   'ar_err_var_128hz',\n",
       "   'vfcdm_4/8_128hz_mean',\n",
       "   'first_16thofa_sec_mean',\n",
       "   'second_16thofa_sec_mean',\n",
       "   'third_16thofa_sec_mean',\n",
       "   'first_16thofa_sec_std',\n",
       "   'second_16thofa_sec_std',\n",
       "   'third_16thofa_sec_std',\n",
       "   'third_16thofa_sec_median',\n",
       "   'third_16thofa_sec_range',\n",
       "   'first_32thofa_sec_mean',\n",
       "   'second_32thofa_sec_mean',\n",
       "   'first_32thofa_sec_std',\n",
       "   'second_32thofa_sec_std'],\n",
       "  'scaler': StandardScaler(),\n",
       "  'model': GradientBoostingClassifier(max_depth=5, n_estimators=600, verbose=1)},\n",
       " 'hossain-svm': {'selected_feats': ['raw_128hz_max',\n",
       "   'raw_128hz_min',\n",
       "   'raw_128hz_amp',\n",
       "   'raw_128hz_median',\n",
       "   'raw_128hz_std',\n",
       "   'raw_128hz_range',\n",
       "   'raw_128hz_shannon_entropy',\n",
       "   'raw_128hz_1d_max',\n",
       "   'raw_128hz_1d_min',\n",
       "   'raw_128hz_1d_amp',\n",
       "   'raw_128hz_1d_std',\n",
       "   'raw_128hz_1d_range',\n",
       "   'raw_128hz_1d_shannon_entropy',\n",
       "   'filt_128hz_max',\n",
       "   'filt_128hz_min',\n",
       "   'filt_128hz_amp',\n",
       "   'filt_128hz_median',\n",
       "   'filt_128hz_std',\n",
       "   'filt_128hz_range',\n",
       "   'filt_128hz_1d_max',\n",
       "   'filt_128hz_1d_min',\n",
       "   'filt_128hz_1d_amp',\n",
       "   'filt_128hz_1d_std',\n",
       "   'filt_128hz_1d_range',\n",
       "   'ar_coeff_1_128hz',\n",
       "   'ar_coeff_2_128hz',\n",
       "   'ar_err_var_128hz',\n",
       "   'vfcdm_4/8_128hz_mean',\n",
       "   'first_16thofa_sec_mean',\n",
       "   'second_16thofa_sec_mean',\n",
       "   'third_16thofa_sec_mean',\n",
       "   'first_16thofa_sec_std',\n",
       "   'second_16thofa_sec_std',\n",
       "   'third_16thofa_sec_std',\n",
       "   'third_16thofa_sec_median',\n",
       "   'third_16thofa_sec_range',\n",
       "   'first_32thofa_sec_mean',\n",
       "   'second_32thofa_sec_mean',\n",
       "   'first_32thofa_sec_std',\n",
       "   'second_32thofa_sec_std'],\n",
       "  'scaler': StandardScaler(),\n",
       "  'model': SVC(C=10, gamma=0.1, probability=True, verbose=1)},\n",
       " 'hossain-lr': {'selected_feats': ['raw_128hz_max',\n",
       "   'raw_128hz_min',\n",
       "   'raw_128hz_amp',\n",
       "   'raw_128hz_median',\n",
       "   'raw_128hz_std',\n",
       "   'raw_128hz_range',\n",
       "   'raw_128hz_shannon_entropy',\n",
       "   'raw_128hz_1d_max',\n",
       "   'raw_128hz_1d_min',\n",
       "   'raw_128hz_1d_amp',\n",
       "   'raw_128hz_1d_std',\n",
       "   'raw_128hz_1d_range',\n",
       "   'raw_128hz_1d_shannon_entropy',\n",
       "   'filt_128hz_max',\n",
       "   'filt_128hz_min',\n",
       "   'filt_128hz_amp',\n",
       "   'filt_128hz_median',\n",
       "   'filt_128hz_std',\n",
       "   'filt_128hz_range',\n",
       "   'filt_128hz_1d_max',\n",
       "   'filt_128hz_1d_min',\n",
       "   'filt_128hz_1d_amp',\n",
       "   'filt_128hz_1d_std',\n",
       "   'filt_128hz_1d_range',\n",
       "   'ar_coeff_1_128hz',\n",
       "   'ar_coeff_2_128hz',\n",
       "   'ar_err_var_128hz',\n",
       "   'vfcdm_4/8_128hz_std',\n",
       "   'first_16thofa_sec_mean',\n",
       "   'second_16thofa_sec_mean',\n",
       "   'third_16thofa_sec_mean',\n",
       "   'first_16thofa_sec_std',\n",
       "   'second_16thofa_sec_std',\n",
       "   'third_16thofa_sec_std',\n",
       "   'third_16thofa_sec_median',\n",
       "   'third_16thofa_sec_range',\n",
       "   'first_32thofa_sec_mean',\n",
       "   'second_32thofa_sec_mean',\n",
       "   'first_32thofa_sec_std',\n",
       "   'second_32thofa_sec_std'],\n",
       "  'scaler': StandardScaler(),\n",
       "  'model': LogisticRegression(C=10, verbose=1)}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = ['cueva_second_phase_1-5-weighted-svm',\n",
    "#     'cueva_second_phase_1-9-weighted-svm',\n",
    "#     'cueva_second_phase_1-2-weighted-svm',\n",
    "#     'taylor-lr',\n",
    "#     'taylor-rf',\n",
    "#     'taylor-svm',\n",
    "#     'hossain-lr',\n",
    "#     'hossain-gbt',\n",
    "#     'hossain-svm'\n",
    "# ]\n",
    "model_names = ['cueva_second_phase_1-5-weighted-svm', 'taylor-lr', 'taylor-rf', 'taylor-svm', 'hossain-lr', 'hossain-svm', 'hossain-gbt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = [\"test\", \"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load engineered test and trained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    selector_config, estimator_name = model_name.split('-', 1)\n",
    "    \n",
    "    for data_split in data_splits:\n",
    "        loader_args = {\n",
    "            'feat_config': selector_config, \n",
    "            'data_split': data_split,\n",
    "            'exc_lof': False\n",
    "        } if selector_config == 'cueva_second_phase' else {\n",
    "            'feat_config': selector_config, \n",
    "            'data_split': data_split,\n",
    "            'exc_lof': False\n",
    "        }\n",
    "\n",
    "        subjects_features, subjects_labels, subjects_names, subject_to_id = concur_load_data(**loader_args)\n",
    "        print(f'selector config: {selector_config}')\n",
    "        print(f'estimator name: {estimator_name}')\n",
    "        # print(f'subjects features shape: {subjects_features.shape}')\n",
    "        # print(f'subjects labels shape: {subjects_labels.shape}\\n')\n",
    "        \n",
    "        # loop through each generated features dataframes from test subjects signals and feed repeatedly to a trained ml models\n",
    "        for index, subject_name in enumerate(subjects_names):\n",
    "            # print(f'subject features columns: {subjects_features[subjects_features['subject_id'] == index].columns}')\n",
    "            print(f'subject: {subject_name}')\n",
    "            # once features are extracted features selected during\n",
    "            # tuning will be used in testing as done also during training\n",
    "\n",
    "            selected_feats = models[model_name]['selected_feats']\n",
    "\n",
    "            if loader_args.get('exc_lof') == None or loader_args.get('exc_lof') == False:\n",
    "                subject_features = subjects_features.loc[subjects_features['subject_id'] == index, selected_feats]\n",
    "                subject_labels = subjects_labels[subjects_labels['subject_id'] == index].drop(columns=['subject_id'])\n",
    "                print(f'subject features shape: {subject_features.shape}')\n",
    "                print(f'subject labels shape: {subject_labels.shape}\\n')\n",
    "\n",
    "            # this will only fire if exc_lof is not None or is false\n",
    "            else:\n",
    "                # if user excludes lower order features, higher order features will only be loaded\n",
    "                subject_features = subjects_features[subjects_features['subject_id'] == index].drop(columns=['subject_id'])\n",
    "                subject_labels = subjects_labels[subjects_labels['subject_id'] == index].drop(columns=['subject_id'])\n",
    "                \n",
    "                print(f'subject features shape: {subject_features.shape}')\n",
    "                print(f'subject labels shape: {subject_labels.shape}\\n')\n",
    "\n",
    "            # convert features and labels into numpy matrices\n",
    "            X = subject_features.to_numpy()\n",
    "            Y = subject_labels.to_numpy().ravel()\n",
    "\n",
    "            # if hossain is the researcher chosen the scaler used during training\n",
    "            # will be used to scale the test subject features\n",
    "            if selector_config == \"hossain\":    \n",
    "                scaler = models[model_name]['scaler']\n",
    "                X = scaler.transform(X)\n",
    "\n",
    "            model = models[model_name]['model']\n",
    "            Y_pred = model.predict(X)\n",
    "            Y_pred_prob = model.predict_proba(X)\n",
    "            print(f\"predicted Y: {Y_pred}\")\n",
    "            print(f\"unique values and counts: {np.unique(Y_pred, return_counts=True)}\")\n",
    "            print(f\"true Y: {Y}\")\n",
    "            print(f\"unique values and counts: {np.unique(Y, return_counts=True)}\")\n",
    "\n",
    "            # compute performance metric values for test subject\n",
    "            acc = accuracy_score(y_true=Y, y_pred=Y_pred)\n",
    "            prec = precision_score(y_true=Y, y_pred=Y_pred, average=\"weighted\")\n",
    "            rec = recall_score(y_true=Y, y_pred=Y_pred, average=\"weighted\")\n",
    "            f1 = f1_score(y_true=Y, y_pred=Y_pred, average=\"weighted\")\n",
    "            roc_auc = roc_auc_score(y_true=Y, y_score=Y_pred_prob[:, 1], average=\"weighted\", multi_class=\"ovo\")\n",
    "            conf_matrix = confusion_matrix(Y, Y_pred).tolist()\n",
    "\n",
    "            print(f\"{data_split} acc: {acc} \\\n",
    "            \\n{data_split} prec: {prec} \\\n",
    "            \\n{data_split} rec: {rec} \\\n",
    "            \\n{data_split} f1: {f1} \\\n",
    "            \\n{data_split} roc_auc: {roc_auc} \\\n",
    "            \\n{data_split} conf_matrix: {conf_matrix}\")\n",
    "\n",
    "            results = models[f'{model_name}'].get(f'{data_split}_results', [])\n",
    "            results.append(\n",
    "                (subject_name, {\n",
    "                    f'{data_split}_acc': acc,\n",
    "                    f'{data_split}_prec': prec, \n",
    "                    f'{data_split}_rec': rec,\n",
    "                    f'{data_split}_f1': f1,\n",
    "                    f'{data_split}_roc_auc': roc_auc,\n",
    "                    f'{data_split}_conf_matrix': conf_matrix\n",
    "                })\n",
    "            )\n",
    "            models[f'{model_name}'][f'{data_split}_results'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in [\"jurado-lstm-cnn\"]:\n",
    "    selector_config, estimator_name = model_name.split('-', 1)\n",
    "\n",
    "    for data_split in data_splits:\n",
    "        subjects_signals, subjects_labels, subjects_names, subject_to_id = concur_load_data(feat_config=selector_config, data_split=data_split)\n",
    "        print(f'selector config: {selector_config}')\n",
    "        print(f'estimator name: {estimator_name}')\n",
    "\n",
    "        for index, subject_name in enumerate(subjects_names):\n",
    "            X = subjects_signals[index]\n",
    "            Y = subjects_labels[index]\n",
    "\n",
    "            # assign model of jurado to variable\n",
    "            model = models[model_name]['model']\n",
    "\n",
    "            # depending on dl model Y_pred will either be unactivated logits \n",
    "            # or sigmoid probabilities\n",
    "            Y_pred = model.predict(X)\n",
    "            \n",
    "            # when our predictions is 0.2, 0.15, and below which is <= 0.2 then y_pred will be 0\n",
    "            # when our predictions is 1, 0.5, 0.4, 0.3, 0.21, and above which is > 0.2 then y_pred will be 1\n",
    "            # why we do this is because of the imbalance of our dataset, and we\n",
    "            # want to place a threshold of 20% since there our dataset only consists\n",
    "            # of 20% of positive classes. Note this conversion is to be used in precision and recall metrics\n",
    "            Y_pred_whole = tf.cast(Y_pred >= 0.2, tf.int64)\n",
    "\n",
    "            print(f\"predicted Y: {Y_pred_whole}\")\n",
    "            print(f\"unique values and counts: {np.unique(Y_pred_whole, return_counts=True)}\")\n",
    "            print(f\"true Y: {Y}\")\n",
    "            print(f\"unique values and counts: {np.unique(Y, return_counts=True)}\")\n",
    "\n",
    "            # compute performance metric values for test subject\n",
    "            acc = accuracy_score(y_true=Y, y_pred=Y_pred_whole)\n",
    "            prec = precision_score(y_true=Y, y_pred=Y_pred_whole, average=\"weighted\")\n",
    "            rec = recall_score(y_true=Y, y_pred=Y_pred_whole, average=\"weighted\")\n",
    "            f1 = f1_score(y_true=Y, y_pred=Y_pred_whole, average=\"weighted\")\n",
    "            roc_auc = roc_auc_score(y_true=Y, y_score=Y_pred, average=\"weighted\", multi_class=\"ovo\")\n",
    "            conf_matrix = confusion_matrix(Y, Y_pred_whole).tolist()\n",
    "\n",
    "            print(f\"{data_split} acc: {acc} \\\n",
    "                \\n{data_split} prec: {prec} \\\n",
    "                \\n{data_split} rec: {rec} \\\n",
    "                \\n{data_split} f1: {f1} \\\n",
    "                \\n{data_split} roc_auc: {roc_auc} \\\n",
    "                \\n{data_split} conf_matrix: {conf_matrix}\")\n",
    "            \n",
    "            results = models[model_name].get(f'{data_split}_results', [])\n",
    "            results.append(\n",
    "                (subject_name, {\n",
    "                    f'{data_split}_acc': acc,\n",
    "                    f'{data_split}_prec': prec, \n",
    "                    f'{data_split}_rec': rec,\n",
    "                    f'{data_split}_f1': f1,\n",
    "                    f'{data_split}_roc_auc': roc_auc,\n",
    "                    f'{data_split}_conf_matrix': conf_matrix\n",
    "                })\n",
    "            )\n",
    "            models[model_name][f'{data_split}_results'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = models.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in copy.items():\n",
    "    if copy[key].get('model') is not None:\n",
    "        del copy[key]['model']\n",
    "\n",
    "    if copy[key].get('scaler') is not None:\n",
    "        del copy[key]['scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_meta_data('./results/all_models_results.json', copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
