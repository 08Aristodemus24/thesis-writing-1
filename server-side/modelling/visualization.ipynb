{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import requests\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "font = {'fontname': 'Helvetica'}\n",
    "import matplotlib as mpl\n",
    "\n",
    "# import and load model architectures as well as decoder\n",
    "from models.cueva import LSTM_FE\n",
    "from models.llanes_jurado import LSTM_CNN\n",
    "from utilities.preprocessors import correct_signals\n",
    "from utilities.loaders import load_meta_data, load_model, load_lookup_array, charge_raw_data, _combine_data, save_lookup_array\n",
    "\n",
    "from utilities.visualizers import (\n",
    "    view_time_frame,\n",
    "    view_wavelet_coeffs,\n",
    "    analyze,\n",
    "    data_split_metric_values,\n",
    "    view_value_frequency,\n",
    "    multi_class_heatmap,\n",
    "    view_metric_values,\n",
    "    view_classified_labels,\n",
    "    view_label_freq,\n",
    "    disp_cat_feat,\n",
    "    plot_all_features,\n",
    "    describe_col,\n",
    "    ModelResults,\n",
    "    view_all_splits_results)\n",
    "\n",
    "from utilities.feature_extractors import (\n",
    "    concur_extract_features_from_all,\n",
    "    extract_features,\n",
    "    extract_features_hybrid,\n",
    "    extract_features_per_hour)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_models = {\n",
    "    'cueva_second_phase-svm':{\n",
    "        'train_results': [\n",
    "            ('ahixac_expert1', {\n",
    "                'train_acc': 0.9764,\n",
    "                'train_prec': 0.9764,\n",
    "                'train_rec': 0.9764,\n",
    "                'train_f1': 0.9764,\n",
    "                'train_roc_auc': 0.8764,\n",
    "            }),\n",
    "            ('akakip_expert1', {\n",
    "                'train_acc': 0.9764,\n",
    "                'train_prec': 0.9764,\n",
    "                'train_rec': 0.9764,\n",
    "                'train_f1': 0.9764,\n",
    "                'train_roc_auc': 0.7764,\n",
    "            }),\n",
    "        ],\n",
    "        'test_results': [\n",
    "            ('pqbqpr_expert2', {\n",
    "                'test_acc': 0.9764,\n",
    "                'test_prec': 0.9764,\n",
    "                'test_rec': 0.9764,\n",
    "                'test_f1': 0.9764,\n",
    "                'test_roc_auc': 0.8764,\n",
    "            }),\n",
    "            ('oxused_expert2', {\n",
    "                'test_acc': 0.9764,\n",
    "                'test_prec': 0.9764,\n",
    "                'test_rec': 0.9764,\n",
    "                'test_f1': 0.9764,\n",
    "                'test_roc_auc': 0.6764,\n",
    "            }),\n",
    "            ('subject3', {\n",
    "                'test_acc': 0.9764,\n",
    "                'test_prec': 0.9764,\n",
    "                'test_rec': 0.9764,\n",
    "                'test_f1': 0.9764,\n",
    "                'test_roc_auc': 0.6764,\n",
    "            }),\n",
    "            ('subject4', {\n",
    "                'test_acc': 0.9764,\n",
    "                'test_prec': 0.9764,\n",
    "                'test_rec': 0.9764,\n",
    "                'test_f1': 0.9764,\n",
    "                'test_roc_auc': 0.6764,\n",
    "            }),\n",
    "            ('subject5', {\n",
    "                'test_acc': 0.9764,\n",
    "                'test_prec': 0.9764,\n",
    "                'test_rec': 0.9764,\n",
    "                'test_f1': 0.9764,\n",
    "                'test_roc_auc': 0.6764,\n",
    "            }),\n",
    "            ('subject6', {\n",
    "                'test_acc': 0.9764,\n",
    "                'test_prec': 0.9764,\n",
    "                'test_rec': 0.9764,\n",
    "                'test_f1': 0.9764,\n",
    "                'test_roc_auc': 0.6764,\n",
    "            }),\n",
    "            ('subject7', {\n",
    "                'test_acc': 0.9764,\n",
    "                'test_prec': 0.9764,\n",
    "                'test_rec': 0.9764,\n",
    "                'test_f1': 0.9764,\n",
    "                'test_roc_auc': 0.6764,\n",
    "            }),\n",
    "        ]\n",
    "    },\n",
    "    'cueva-lstm-fe': {\n",
    "        # 'model':\n",
    "        # 'hyper_params':\n",
    "    },\n",
    "    'jurado-lstm-cnn': {\n",
    "        # 'model':\n",
    "        # 'hyper_params':\n",
    "    },\n",
    "    'taylor-svm': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "    },\n",
    "    'taylor-lr': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "    },\n",
    "    'taylor-rf': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "    },\n",
    "    'hossain-gbt': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "        # 'scaler':\n",
    "    },\n",
    "    'hossain-svm': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "        # 'scaler':\n",
    "    },\n",
    "    'hossain-lr': {\n",
    "        # 'model':\n",
    "        # 'selected_feats':\n",
    "        # 'scaler':\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load_meta_data('./results/all_models_results.json')\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "#### what I want is across each model see accuracy for all subjects in train and test set i.e. a figure will show taylor's svm accuracy across all train and test subjects\n",
    "- have the x ticks be labeled as the subject names\n",
    "- have the y ticks be labeled as the percentage value of the accuracy or roc  \n",
    "- the title for the plot will be for instance `taylor svm train accuracy results.png`, and the general title would be `{selector_config} {estimator_name} {data_split} {metric} results.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sample results chart.png](../../sample%20results%20chart.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(subject_name, result['test_roc_auc']) for (subject_name, result) in models['cueva_second_phase-svm']['test_results']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks, y_ticks = list(zip(*data))\n",
    "x_ticks, y_ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = mpl.colormaps['mako']\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "axis = fig.add_subplot()\n",
    "\n",
    "bar = axis.bar(x_ticks, y_ticks, color=cmap(np.linspace(0, 1, len(data))), edgecolor='white', linewidth=0.125)\n",
    "axis.bar_label(bar, fmt='{:.4f}')\n",
    "axis.set_xlabel('subjects', )\n",
    "axis.set_ylabel('score', )\n",
    "axis.tick_params(axis='x', labelrotation=45.0)\n",
    "axis.set_title('{selector_config} {estimator_name} {data_split} {metric} results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_subject_results(data, selector_config: str, estimator_name: str, data_split: str=\"train\", metric: str=\"acc\", colormap: str=\"plasma\", save_img: bool=True, style: str='default'):\n",
    "    \"\"\"\n",
    "    suitable for all discrete input\n",
    "\n",
    "    plots either a horizontal bar graph to display frequency of words top 'limit' \n",
    "    words e.g. top 20 or a pie chart to display the percentages of the top 'limit' \n",
    "    words e.g. top 20, specified by the argument kind which can be either\n",
    "    strings barh or pie\n",
    "\n",
    "    main args:\n",
    "        data - list of tuples representing the subject and the subjects respective score\n",
    "        selector_config - \n",
    "        estimator_name - \n",
    "        data_split - \n",
    "        metric - \n",
    "        colormap - \n",
    "        save_img - \n",
    "        style - \n",
    "    \"\"\"\n",
    "\n",
    "    # compose title based on selector_config, estimator_name, data_split, and metric args\n",
    "    title = f'{selector_config} {estimator_name} {data_split} {metric} results'\n",
    "\n",
    "    # extract x and y values from data\n",
    "    x_ticks, y_ticks = list(zip(*data))\n",
    "\n",
    "    styles = {\n",
    "        'dark': 'dark_background',\n",
    "        'solarized': 'Solarized_Light2',\n",
    "        '538': 'fivethirtyeight',\n",
    "        'ggplot': 'ggplot',\n",
    "    }\n",
    "\n",
    "    plt.style.use(styles.get(style, 'default'))\n",
    "\n",
    "    # define figure\n",
    "    cmap = mpl.colormaps[colormap]\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    axis = fig.add_subplot()\n",
    "\n",
    "    # plot bar graph\n",
    "    bar = axis.bar(x_ticks, y_ticks, color=cmap(np.linspace(0, 1, len(data))), edgecolor='white', linewidth=0.125)\n",
    "    axis.bar_label(bar, fmt='{:.4f}')\n",
    "    axis.set_xlabel('subjects', )\n",
    "    axis.set_ylabel(f'{metric} score', )\n",
    "    axis.tick_params(axis='x', labelrotation=45.0)\n",
    "    axis.set_title(title)\n",
    "    if save_img:\n",
    "        plt.savefig(f'./figures & images/{title}.png')\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = ['cueva_second_phase_1-5-weighted-svm',\n",
    "#     'cueva_second_phase_1-9-weighted-svm',\n",
    "#     'cueva_second_phase_1-2p5-weighted-svm',\n",
    "#     'taylor-lr',\n",
    "#     'taylor-rf',\n",
    "#     'taylor-svm',\n",
    "#     'hossain-lr',\n",
    "#     'hossain-gbt',\n",
    "#     'hossain-svm'\n",
    "# ]\n",
    "model_names = ['cueva_second_phase_1-5-weighted-svm', 'taylor-lr', 'taylor-rf', 'taylor-svm', 'hossain-lr', 'hossain-svm', 'hossain-gbt', 'jurado-lstm-cnn']\n",
    "data_splits = [\"train\", \"test\"]\n",
    "metrics = [\"acc\", \"prec\", \"rec\", \"f1\", \"roc_auc\"]\n",
    "colormaps = ['mako', 'GnBu', 'plasma', 'magma', 'twilight', 'YlOrBr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    # extract selector config and estimator name from model name\n",
    "    selector_config, estimator_name = model_name.split('-', 1)\n",
    "\n",
    "    for data_split in data_splits:\n",
    "        for metric in metrics:\n",
    "            data = [(subject_name, result[f'{data_split}_{metric}']) for (subject_name, result) in models[f'{selector_config}-{estimator_name}'][f'{data_split}_results']]\n",
    "            sample_idx = np.random.choice(len(colormaps), size=1)[0]\n",
    "            colormap = colormaps[sample_idx]\n",
    "            view_subject_results(data, \n",
    "                selector_config=selector_config, \n",
    "                estimator_name=estimator_name,\n",
    "                data_split=data_split,\n",
    "                metric=metric,\n",
    "                colormap=colormap,\n",
    "                style='dark'\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from here we create a graph now that takes the mean value of each metric for each model\n",
    "![sample results chart.png](../../sample%20averaged%20results%20chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So in this case the `x_ticks` would be the `model_names` themselves and the `y_ticks` would be the mean accuracy of each model listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = ['cueva_second_phase_1-5-weighted-svm',\n",
    "#     'cueva_second_phase_1-9-weighted-svm',\n",
    "#     'cueva_second_phase_1-2p5-weighted-svm',\n",
    "#     'taylor-lr',\n",
    "#     'taylor-rf',\n",
    "#     'taylor-svm',\n",
    "#     'hossain-lr',\n",
    "#     'hossain-gbt',\n",
    "#     'hossain-svm'\n",
    "# ]\n",
    "model_names = ['cueva_second_phase-svm', 'taylor-lr', 'taylor-rf', 'taylor-svm', 'hossain-lr', 'hossain-gbt', 'jurado-lstm-cnn']\n",
    "data_splits = [\"train\", \"test\"]\n",
    "metrics = [\"acc\", \"prec\", \"rec\", \"f1\", \"roc_auc\"]\n",
    "colormaps = ['mako', 'GnBu', 'plasma', 'magma', 'twilight', 'YlOrBr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_models_mean_results(data, data_split: str=\"train\", metric: str=\"acc\", colormap: str=\"plasma\", save_img: bool=True, style: str='default'):\n",
    "    \"\"\"\n",
    "    suitable for all discrete input\n",
    "\n",
    "    plots either a horizontal bar graph to display frequency of words top 'limit' \n",
    "    words e.g. top 20 or a pie chart to display the percentages of the top 'limit' \n",
    "    words e.g. top 20, specified by the argument kind which can be either\n",
    "    strings barh or pie\n",
    "\n",
    "    main args:\n",
    "        data - list of tuples representing the subject and the subjects respective score\n",
    "        data_split - \n",
    "        metric - \n",
    "        colormap - \n",
    "        save_img - \n",
    "        style - \n",
    "    \"\"\"\n",
    "\n",
    "    # compose title based on selector_config, estimator_name, data_split, and metric args\n",
    "    title = f'{data_split} {metric} results'\n",
    "\n",
    "    # extract x and y values from data\n",
    "    x_ticks, y_ticks = list(zip(*data))\n",
    "\n",
    "    styles = {\n",
    "        'dark': 'dark_background',\n",
    "        'solarized': 'Solarized_Light2',\n",
    "        '538': 'fivethirtyeight',\n",
    "        'ggplot': 'ggplot',\n",
    "    }\n",
    "\n",
    "    plt.style.use(styles.get(style, 'default'))\n",
    "\n",
    "    # define figure\n",
    "    cmap = mpl.colormaps[colormap]\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    axis = fig.add_subplot()\n",
    "\n",
    "    # plot bar graph\n",
    "    bar = axis.bar(x_ticks, y_ticks, color=cmap(np.linspace(0, 1, len(data))), edgecolor='white', linewidth=0.125)\n",
    "    axis.bar_label(bar, fmt='{:.4f}')\n",
    "    axis.set_xlabel('models', )\n",
    "    axis.set_ylabel(f'{metric} score', )\n",
    "    axis.tick_params(axis='x', labelrotation=45.0)\n",
    "    axis.set_title(title)\n",
    "    if save_img:\n",
    "        plt.savefig(f'./figures & images/{title}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_split in data_splits:\n",
    "    for metric in metrics:\n",
    "        print(f'data split: {data_split}')\n",
    "        print(f'metric: {metric}')\n",
    "        ticks = []\n",
    "        for model_name in model_names:\n",
    "            # extract selector config and estimator name from model name\n",
    "            selector_config, estimator_name = model_name.split('-', 1)\n",
    "            \n",
    "            # take the mean of a specific models metrics\n",
    "            mean_result = np.mean([result[f'{data_split}_{metric}'] for (_, result) in models[model_name][f'{data_split}_results']])\n",
    "\n",
    "            ticks.append((model_name, mean_result))\n",
    "        # print(f'{ticks}\\n')\n",
    "        # x_ticks, y_ticks = list(zip(*ticks))\n",
    "        # print(f'{x_ticks} {y_ticks}\\n')\n",
    "\n",
    "        sample_idx = np.random.choice(len(colormaps), size=1)[0]\n",
    "        colormap = colormaps[sample_idx]\n",
    "        view_models_mean_results(ticks, data_split=data_split, metric=metric, colormap=colormap, style='dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = mpl.colormaps['mako']\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "axis = fig.add_subplot()\n",
    "\n",
    "bar = axis.bar(x_ticks, y_ticks, color=cmap(np.linspace(0, 1, len(data))), edgecolor='white', linewidth=0.125)\n",
    "axis.bar_label(bar, fmt='{:.4f}')\n",
    "axis.set_xlabel('subjects', )\n",
    "axis.set_ylabel('score', )\n",
    "axis.tick_params(axis='x', labelrotation=45.0)\n",
    "axis.set_title('{selector_config} {estimator_name} {data_split} {metric} results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-writing-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
