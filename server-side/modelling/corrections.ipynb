{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will just attempt to correct some columns of the generated dataframes from the feature extractor functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import requests\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# import and load model architectures as well as decoder\n",
    "from models.cueva import LSTM_FE\n",
    "from models.llanes_jurado import LSTM_CNN\n",
    "from utilities.preprocessors import correct_signals\n",
    "from utilities.loaders import load_meta_data, save_meta_data, concur_load_data, charge_raw_data, _combine_data, load_model, load_lookup_array\n",
    "\n",
    "from utilities.visualizers import (\n",
    "    view_time_frame,\n",
    "    view_wavelet_coeffs,\n",
    "    analyze,\n",
    "    data_split_metric_values,\n",
    "    view_value_frequency,\n",
    "    multi_class_heatmap,\n",
    "    view_metric_values,\n",
    "    view_classified_labels,\n",
    "    view_label_freq,\n",
    "    disp_cat_feat,\n",
    "    plot_all_features,\n",
    "    describe_col,\n",
    "    ModelResults,\n",
    "    view_all_splits_results)\n",
    "\n",
    "from utilities.feature_extractors import (\n",
    "    concur_extract_features_from_all,\n",
    "    extract_features,\n",
    "    extract_features_hybrid,\n",
    "    extract_features_per_hour)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>raw_signal</th>\n",
       "      <th>clean_signal</th>\n",
       "      <th>label</th>\n",
       "      <th>auto_signal</th>\n",
       "      <th>pred_art</th>\n",
       "      <th>post_proc_pred_art</th>\n",
       "      <th>new_signal</th>\n",
       "      <th>stress_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.164015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.164015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.164015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.164015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.164015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856081</th>\n",
       "      <td>6688.132812</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856082</th>\n",
       "      <td>6688.140625</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856083</th>\n",
       "      <td>6688.148438</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856084</th>\n",
       "      <td>6688.156250</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856085</th>\n",
       "      <td>6688.164062</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>856086 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               time  raw_signal  clean_signal  label  auto_signal  pred_art  \\\n",
       "0          0.000000    0.000000     -0.164015    0.0     0.000000       0.0   \n",
       "1          0.007812    0.000000     -0.164015    0.0     0.000000       0.0   \n",
       "2          0.015625    0.000000     -0.164015    0.0     0.000000       0.0   \n",
       "3          0.023438    0.000000     -0.164015    0.0     0.000000       0.0   \n",
       "4          0.031250    0.000000     -0.164015    0.0     0.000000       0.0   \n",
       "...             ...         ...           ...    ...          ...       ...   \n",
       "856081  6688.132812    0.000222      0.000222    0.0     0.000222       0.0   \n",
       "856082  6688.140625    0.000222      0.000222    0.0     0.000222       0.0   \n",
       "856083  6688.148438    0.000222      0.000222    0.0     0.000222       0.0   \n",
       "856084  6688.156250    0.000222      0.000222    0.0     0.000222       0.0   \n",
       "856085  6688.164062    0.000222      0.000222    0.0     0.000222       0.0   \n",
       "\n",
       "        post_proc_pred_art  new_signal  stress_level  \n",
       "0                      0.0    0.000000           2.0  \n",
       "1                      0.0    0.000000           2.0  \n",
       "2                      0.0    0.000000           2.0  \n",
       "3                      0.0    0.000000           2.0  \n",
       "4                      0.0    0.000000           2.0  \n",
       "...                    ...         ...           ...  \n",
       "856081                 0.0    0.000222           0.0  \n",
       "856082                 0.0    0.000222           0.0  \n",
       "856083                 0.0    0.000222           0.0  \n",
       "856084                 0.0    0.000222           0.0  \n",
       "856085                 0.0    0.000222           0.0  \n",
       "\n",
       "[856086 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_test_df = pd.read_csv('./results/inefoh_expert1.csv', index_col=0)\n",
    "res_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stress_level\n",
       "2.0    540160\n",
       "1.0    280320\n",
       "0.0     35606\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_test_df['stress_level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 is baseline level of stress, 1 is medium level of stress, and 2 is high level of stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = os.listdir('./data/Electrodermal Activity artifact correction BEnchmark (EDABE)/Train/')\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = os.listdir('./data/Electrodermal Activity artifact correction BEnchmark (EDABE)/Test/')\n",
    "test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subjects_names = list(set([re.sub(r\".csv\", \"\", file) for file in train_files]))\n",
    "train_subjects_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subjects_names = list(set([re.sub(r\".csv\", \"\", file) for file in test_files]))\n",
    "test_subjects_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_results = load_meta_data('./results/all_models_results.json')\n",
    "all_models_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {\n",
    "#     'cueva_second_phase-svm':{\n",
    "\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_miscs():\n",
    "#     \"\"\"\n",
    "#     loads miscellaneous variables to be used by the model\n",
    "#     \"\"\"\n",
    "\n",
    "#     global models\n",
    "\n",
    "#     print('loading miscellaneous...')\n",
    "#     cueva_second_phase_svm_red_feats = load_lookup_array(f'./data/Artifact Detection Data/reduced_cueva_second_phase_svm_feature_set.txt')\n",
    "#     models['cueva_second_phase-svm']['selected_feats'] = cueva_second_phase_svm_red_feats\n",
    "\n",
    "#     print('miscellaneous loaded.')\n",
    "\n",
    "# def load_preprocessors():\n",
    "#     \"\"\"\n",
    "#     prepares and loads the saved encoders, normalizers of\n",
    "#     the dataset to later transform raw user input from\n",
    "#     client-side\n",
    "#     \"\"\"\n",
    "#     global models\n",
    "\n",
    "#     print('loading preprocessors...')\n",
    "\n",
    "#     # pre-load here scaler of hossain used during training\n",
    "#     cueva_second_phase_svm_scaler = load_model('./saved/misc/cueva_second_phase_svm_scaler.pkl')\n",
    "\n",
    "#     models['cueva_second_phase-svm']['scaler'] = cueva_second_phase_svm_scaler\n",
    "\n",
    "#     print('preprocessors loaded.')\n",
    "\n",
    "# def load_models():\n",
    "#     \"\"\"\n",
    "#     prepares and loads sample input and custom model in\n",
    "#     order to use trained weights/parameters/coefficients\n",
    "#     \"\"\"\n",
    "#     global models\n",
    "    \n",
    "#     print('loading models...')\n",
    "#     cueva_second_phase_svm = load_model('./saved/models/cueva_second_phase_svm_clf.pkl')\n",
    "#     models['cueva_second_phase-svm']['model'] = cueva_second_phase_svm\n",
    "\n",
    "#     print('models loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_miscs()\n",
    "# load_preprocessors()\n",
    "# load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_splits = [\"test\", \"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_name in [\"cueva_second_phase-svm\"]:\n",
    "#     selector_config, estimator_name = model_name.split('-', 1)\n",
    "    \n",
    "#     for data_split in data_splits:\n",
    "#         loader_args = {\n",
    "#             'feat_config': selector_config, \n",
    "#             'data_split': data_split,\n",
    "#             'exc_lof': False\n",
    "#         } if selector_config == 'cueva_second_phase' else {\n",
    "#             'feat_config': selector_config, \n",
    "#             'data_split': data_split,\n",
    "#             'exc_lof': False\n",
    "#         }\n",
    "\n",
    "#         subjects_features, subjects_labels, subjects_names, subject_to_id = concur_load_data(**loader_args)\n",
    "#         print(f'selector config: {selector_config}')\n",
    "#         print(f'estimator name: {estimator_name}')\n",
    "#         # print(f'subjects features shape: {subjects_features.shape}')\n",
    "#         # print(f'subjects labels shape: {subjects_labels.shape}\\n')\n",
    "        \n",
    "#         # loop through each generated features dataframes from test subjects signals and feed repeatedly to a trained ml models\n",
    "#         for index, subject_name in enumerate(subjects_names):\n",
    "#             # print(f'subject features columns: {subjects_features[subjects_features['subject_id'] == index].columns}')\n",
    "#             print(f'subject: {subject_name}')\n",
    "#             # once features are extracted features selected during\n",
    "#             # tuning will be used in testing as done also during training\n",
    "\n",
    "#             selected_feats = models[model_name]['selected_feats']\n",
    "\n",
    "#             if loader_args.get('exc_lof') == None or loader_args.get('exc_lof') == False:\n",
    "#                 subject_features = subjects_features.loc[subjects_features['subject_id'] == index, selected_feats]\n",
    "#                 subject_labels = subjects_labels[subjects_labels['subject_id'] == index].drop(columns=['subject_id'])\n",
    "#                 print(f'subject features shape: {subject_features.shape}')\n",
    "#                 print(f'subject labels shape: {subject_labels.shape}\\n')\n",
    "\n",
    "#             # this will only fire if exc_lof is not None or is false\n",
    "#             else:\n",
    "#                 # if user excludes lower order features, higher order features will only be loaded\n",
    "#                 subject_features = subjects_features[subjects_features['subject_id'] == index].drop(columns=['subject_id'])\n",
    "#                 subject_labels = subjects_labels[subjects_labels['subject_id'] == index].drop(columns=['subject_id'])\n",
    "                \n",
    "#                 print(f'subject features shape: {subject_features.shape}')\n",
    "#                 print(f'subject labels shape: {subject_labels.shape}\\n')\n",
    "\n",
    "#             # convert features and labels into numpy matrices\n",
    "#             X = subject_features.to_numpy()\n",
    "#             Y = subject_labels.to_numpy().ravel()\n",
    "\n",
    "#             # if hossain is the researcher chosen the scaler used during training\n",
    "#             # will be used to scale the test subject features\n",
    "#             if selector_config == \"hossain\" or selector_config == \"cueva_second_phase-svm\":    \n",
    "#                 scaler = models[model_name]['scaler']\n",
    "#                 X = scaler.transform(X)\n",
    "\n",
    "#             model = models[model_name]['model']\n",
    "#             Y_pred = model.predict(X)\n",
    "#             Y_pred_prob = model.predict_proba(X)\n",
    "#             print(f\"predicted Y: {Y_pred}\")\n",
    "#             print(f\"unique values and counts: {np.unique(Y_pred, return_counts=True)}\")\n",
    "#             print(f\"true Y: {Y}\")\n",
    "#             print(f\"unique values and counts: {np.unique(Y, return_counts=True)}\")\n",
    "\n",
    "#             # compute performance metric values for test subject\n",
    "#             acc = accuracy_score(y_true=Y, y_pred=Y_pred)\n",
    "#             prec = precision_score(y_true=Y, y_pred=Y_pred, average=\"weighted\")\n",
    "#             rec = recall_score(y_true=Y, y_pred=Y_pred, average=\"weighted\")\n",
    "#             f1 = f1_score(y_true=Y, y_pred=Y_pred, average=\"weighted\")\n",
    "#             roc_auc = roc_auc_score(y_true=Y, y_score=Y_pred_prob[:, 1], average=\"weighted\", multi_class=\"ovo\")\n",
    "#             conf_matrix = confusion_matrix(Y, Y_pred).tolist()\n",
    "#             true_neg = conf_matrix[0][0]\n",
    "#             false_pos = conf_matrix[0][1]\n",
    "#             false_neg = conf_matrix[1][0]\n",
    "#             true_pos = conf_matrix[1][1]\n",
    "#             tpr = true_pos / (true_pos + false_neg)\n",
    "#             tnr = true_neg / (true_neg + false_pos)\n",
    "#             fpr = false_pos / (false_pos + true_neg)\n",
    "#             fnr = false_neg / (false_neg + true_pos)\n",
    "\n",
    "#             print(f\"{data_split} acc: {acc} \\\n",
    "#                 \\n{data_split} prec: {prec} \\\n",
    "#                 \\n{data_split} rec: {rec} \\\n",
    "#                 \\n{data_split} f1: {f1} \\\n",
    "#                 \\n{data_split} roc_auc: {roc_auc} \\\n",
    "#                 \\n{data_split} conf_matrix: {conf_matrix} \\\n",
    "#                 \\n{data_split} tpr: {tpr} \\\n",
    "#                 \\n{data_split} tnr: {tnr} \\\n",
    "#                 \\n{data_split} fpr: {fpr} \\\n",
    "#                 \\n{data_split} fnr: {fnr}\")\n",
    "            \n",
    "#             results = models[model_name].get(f'{data_split}_results', [])\n",
    "#             results.append(\n",
    "#                 (subject_name, {\n",
    "#                     f'{data_split}_acc': acc,\n",
    "#                     f'{data_split}_prec': prec, \n",
    "#                     f'{data_split}_rec': rec,\n",
    "#                     f'{data_split}_f1': f1,\n",
    "#                     f'{data_split}_roc_auc': roc_auc,\n",
    "#                     f'{data_split}_conf_matrix': conf_matrix,\n",
    "#                     f'{data_split}_tpr': tpr,\n",
    "#                     f'{data_split}_tnr': tnr,\n",
    "#                     f'{data_split}_fpr': fpr,\n",
    "#                     f'{data_split}_fnr': fnr,\n",
    "#                 })\n",
    "#             )\n",
    "#             models[f'{model_name}'][f'{data_split}_results'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy = models.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in copy.items():\n",
    "#     if copy[key].get('model') is not None:\n",
    "#         del copy[key]['model']\n",
    "\n",
    "#     if copy[key].get('scaler') is not None:\n",
    "#         del copy[key]['scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_models_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_models_results['cueva_second_phase-svm'] = copy['cueva_second_phase-svm']\n",
    "# all_models_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_meta_data('./results/all_models_results.json', all_models_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = ['cueva_second_phase-1-5-weighted-svm',\n",
    "#     'cueva_second_phase-1-9-weighted-svm',\n",
    "#     'cueva_second_phase-1-2-weighted-svm',\n",
    "#     'taylor-lr',\n",
    "#     'taylor-rf',\n",
    "#     'taylor-svm',\n",
    "#     'hossain-lr',\n",
    "#     'hossain-gbt',\n",
    "#     'hossain-svm',\n",
    "#     'jurado-lstm-cnn'\n",
    "# ]\n",
    "# data_splits = [\"test\", \"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_name in model_names:\n",
    "#     selector_config, estimator_name = model_name.split('-', 1)\n",
    "    \n",
    "#     for data_split in data_splits:\n",
    "        \n",
    "#         # # loop through each generated features dataframes from test subjects signals and feed repeatedly to a trained ml models\n",
    "#         # subjects_names = train_subjects_names if data_split == \"train\" else test_subjects_names\n",
    "#         # for index, subject_name in enumerate(subjects_names):\n",
    "#         #     # print(f'subject features columns: {subjects_features[subjects_features['subject_id'] == index].columns}')\n",
    "#         #     print(f'subject: {subject_name}')\n",
    "#         for i, result in enumerate(results[model_name][f'{data_split}_results']):\n",
    "#             print(result[0])\n",
    "#             print(results[model_name][f'{data_split}_results'][i][0])\n",
    "\n",
    "#             conf_matrix = result[1][f'{data_split}_conf_matrix']\n",
    "#             true_neg = conf_matrix[0][0]\n",
    "#             false_pos = conf_matrix[0][1]\n",
    "#             false_neg = conf_matrix[1][0]\n",
    "#             true_pos = conf_matrix[1][1]\n",
    "#             tpr = true_pos / (true_pos + false_neg)\n",
    "#             tnr = true_neg / (true_neg + false_pos)\n",
    "#             fpr = false_pos / (false_pos + true_neg)\n",
    "#             fnr = false_neg / (false_neg + true_pos)\n",
    "\n",
    "#             print(f\"{data_split} tpr: {tpr} \\\n",
    "#                 \\n{data_split} tnr: {tnr} \\\n",
    "#                 \\n{data_split} fpr: {fpr} \\\n",
    "#                 \\n{data_split} fnr: {fnr}\")\n",
    "            \n",
    "#             results[model_name][f'{data_split}_results'][i][1][f'{data_split}_tpr'] = tpr\n",
    "#             results[model_name][f'{data_split}_results'][i][1][f'{data_split}_tnr'] = tnr\n",
    "#             results[model_name][f'{data_split}_results'][i][1][f'{data_split}_fpr'] = fpr\n",
    "#             results[model_name][f'{data_split}_results'][i][1][f'{data_split}_fnr'] = fnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_meta_data('./results/all_models_results.json', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahixac_lof = pd.read_csv(f'./data/Hybrid Artifact Detection Data/train/ahixac_expert1_lof.csv', index_col=0)\n",
    "# ahixac_lof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahixac_lof['raw_128hz_skewness.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahixac_lof['filt_16hz_skewness.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ren_ahixac_lof = ahixac_lof.rename(columns={\n",
    "#         'raw_128hz_skewness.1': 'raw_128hz_kurt',\n",
    "#         'filt_128hz_skewness.1': 'filt_128hz_kurt',\n",
    "#         'raw_16hz_skewness.1': 'raw_16hz_kurt',\n",
    "#         'filt_16hz_skewness.1': 'filt_16hz_kurt',\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ren_ahixac_lof['raw_128hz_kurt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, train_subject_name in enumerate(train_subjects_names):\n",
    "#     print(f'subject: {train_subject_name}')\n",
    "\n",
    "#     # save both lstm features and lstm labels\n",
    "#     train_subject_lof = pd.read_csv(f'./data/Hybrid Artifact Detection Data/train/{train_subject_name}_lof.csv', index_col=0)\n",
    "#     train_subject_lof.rename(columns={\n",
    "#         'raw_128hz_skewness.1': 'raw_128hz_kurt',\n",
    "#         'filt_128hz_skewness.1': 'filt_128hz_kurt',\n",
    "#         'raw_16hz_skewness.1': 'raw_16hz_kurt',\n",
    "#         'filt_16hz_skewness.1': 'filt_16hz_kurt',\n",
    "#     }, inplace=True)\n",
    "#     train_subject_lof.to_csv(f'./data/Hybrid Artifact Detection Data/train/{train_subject_name}_lof.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, test_subject_name in enumerate(test_subjects_names):\n",
    "#     print(f'subject: {test_subject_name}')\n",
    "\n",
    "#     # save both lstm features and lstm labels\n",
    "#     test_subject_lof = pd.read_csv(f'./data/Hybrid Artifact Detection Data/test/{test_subject_name}_lof.csv', index_col=0)\n",
    "#     test_subject_lof.rename(columns={\n",
    "#         'raw_128hz_skewness.1': 'raw_128hz_kurt',\n",
    "#         'filt_128hz_skewness.1': 'filt_128hz_kurt',\n",
    "#         'raw_16hz_skewness.1': 'raw_16hz_kurt',\n",
    "#         'filt_16hz_skewness.1': 'filt_16hz_kurt',\n",
    "#     }, inplace=True)\n",
    "#     test_subject_lof.to_csv(f'./data/Hybrid Artifact Detection Data/test/{test_subject_name}_lof.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-writing-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
