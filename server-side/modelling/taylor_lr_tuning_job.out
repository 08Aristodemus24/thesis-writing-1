Submitted on Thu Oct 17 16:26:06 PST 2024
JOB PARAMETERS
SLURM_JOB_ID          : 30335
SLURM_JOB_NAME        : taylor_lr_tuning_job
SLURM_JOB_NUM_NODES   : 1
SLURM_JOB_NODELIST    : saliksik-cpu-12
SLURM_NTASKS          : 12
SLURM_NTASKS_PER_NODE : 
SLURM_MEM_PER_NODE    : 24576
TMPDIR                : /tmp/michael.cueva/SLURM_JOB_ID/30335
Module unload: anaconda/3-2023.07-2
/var/spool/slurm/job30335/slurm_script: line 5: conda: command not found
/var/spool/slurm/job30335/slurm_script: line 5: conda: command not found
Module load: anaconda/3-2023.07-2
Python 3.12.3
slurmstepd: error: *** JOB 30335 ON saliksik-cpu-12 CANCELLED AT 2024-10-17T16:33:14 ***
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.37868D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     43     53      1     0     0   6.867D-05   2.923D-01
  F =  0.29233361602521440     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40893D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     40     44      1     0     0   6.698D-05   2.637D-01
  F =  0.26368392977558125     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.29959D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     38     43      1     0     0   9.403D-05   2.961D-01
  F =  0.29610332062779571     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39315D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     42     48      1     0     0   7.354D-05   2.912D-01
  F =  0.29115496759455078     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36790D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     34     41      1     0     0   6.371D-05   2.917D-01
  F =  0.29173044989647545     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36476D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     33     41      1     0     0   5.863D-05   2.933D-01
  F =  0.29333087781161743     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36415D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     38     44      1     0     0   9.355D-05   2.926D-01
  F =  0.29263307588785814     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40146D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     39     49      1     0     0   6.602D-05   2.927D-01
  F =  0.29269267048587455     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.31334D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     34     38      1     0     0   6.734D-05   2.901D-01
  F =  0.29013733161805827     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.33892D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     45     52      1     0     0   6.438D-05   2.936D-01
  F =  0.29357849984347340     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.38743D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     40     48      1     0     0   6.601D-05   2.945D-01
  F =  0.29454892801353189     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40652D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     33     40      1     0     0   6.456D-05   2.844D-01
  F =  0.28438990910282569     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.37497D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     46     50      1     0     0   5.393D-05   2.842D-01
  F =  0.28422677771339083     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.33905D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     38     42      1     0     0   7.372D-05   2.928D-01
  F =  0.29280508769632713     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.37324D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     31     37      1     0     0   5.511D-05   2.953D-01
  F =  0.29529983514651942     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36501D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     44     49      1     0     0   8.100D-05   2.902D-01
  F =  0.29022155073875616     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36065D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     39     44      1     0     0   6.310D-05   2.854D-01
  F =  0.28539991433613737     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40052D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     35     42      1     0     0   7.804D-05   2.781D-01
  F =  0.27807689289304716     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35191D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     37     46      1     0     0   7.566D-05   2.960D-01
  F =  0.29599850963247026     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.28518D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     29     35      1     0     0   9.914D-05   2.920D-01
  F =  0.29196829903627158     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35070D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     39     45      1     0     0   4.878D-05   2.902D-01
  F =  0.29015791600565605     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39759D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     46     55      1     0     0   3.823D-05   2.933D-01
  F =  0.29326464025439208     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35869D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     40     47      1     0     0   8.289D-05   2.829D-01
  F =  0.28293909277938745     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.38473D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     44     53      1     0     0   3.851D-05   2.916D-01
  F =  0.29162498041487084     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39755D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     43     54      1     0     0   6.527D-05   2.854D-01
  F =  0.28543509388636179     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39203D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     35     42      1     0     0   6.653D-05   2.931D-01
  F =  0.29314679572748120     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35810D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     45     49      1     0     0   6.338D-05   2.904D-01
  F =  0.29040339963637224     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36162D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     40     48      1     0     0   7.042D-05   2.947D-01
  F =  0.29470121777864788     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40809D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     33     39      1     0     0   3.958D-05   2.705D-01
  F =  0.27050484430526389     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35304D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     42     54      1     0     0   8.269D-06   2.942D-01
  F =  0.29418918061720267     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.38858D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     43     51      1     0     0   9.451D-05   2.916D-01
  F =  0.29160916423450012     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35892D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     37     47      1     0     0   7.693D-05   2.953D-01
  F =  0.29525815432907299     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36229D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     40     48      1     0     0   9.332D-05   2.956D-01
  F =  0.29555898675885900     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.37868D+00

At iterate   50    f=  2.80056D-01    |proj g|=  2.33482D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     55     66      1     0     0   5.339D-05   2.799D-01
  F =  0.27991319554982430     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40893D+00

At iterate   50    f=  2.52299D-01    |proj g|=  2.24479D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     54     62      1     0     0   6.006D-05   2.522D-01
  F =  0.25217958218549513     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.29959D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     48     56      1     0     0   5.766D-05   2.838D-01
  F =  0.28384284161351964     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39315D+00

At iterate   50    f=  2.79161D-01    |proj g|=  5.30115D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     59     69      1     0     0   6.225D-05   2.786D-01
  F =  0.27860329677805434     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36790D+00

At iterate   50    f=  2.79752D-01    |proj g|=  1.30432D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     65     74      1     0     0   6.965D-05   2.792D-01
  F =  0.27916572130621181     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36476D+00

At iterate   50    f=  2.81025D-01    |proj g|=  3.55569D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     55     68      1     0     0   4.096D-05   2.806D-01
  F =  0.28058389277104512     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36415D+00

At iterate   50    f=  2.81314D-01    |proj g|=  9.36901D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     69     79      1     0     0   6.974D-05   2.801D-01
  F =  0.28014638683470899     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40146D+00

At iterate   50    f=  2.80044D-01    |proj g|=  4.14464D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     67     79      1     0     0   7.593D-05   2.800D-01
  F =  0.27996355916762822     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.31334D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     49     57      1     0     0   2.853D-05   2.767D-01
  F =  0.27673511677290669     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.33892D+00

At iterate   50    f=  2.82540D-01    |proj g|=  1.71862D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     71     81      1     0     0   5.395D-05   2.812D-01
  F =  0.28124318884533989     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.38743D+00

At iterate   50    f=  2.82643D-01    |proj g|=  1.13759D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     73     85      1     0     0   1.210D-05   2.818D-01
  F =  0.28184166235193880     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40652D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     43     55      1     0     0   9.418D-05   2.721D-01
  F =  0.27212627784495746     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.37497D+00

At iterate   50    f=  2.71954D-01    |proj g|=  1.62193D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     58     68      1     0     0   6.725D-05   2.719D-01
  F =  0.27192192216930522     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.33905D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     46     53      1     0     0   6.343D-05   2.803D-01
  F =  0.28026670339607551     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.37324D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     37     45      1     0     0   5.030D-05   2.829D-01
  F =  0.28286306636797087     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36501D+00

At iterate   50    f=  2.78123D-01    |proj g|=  8.85948D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     63     73      1     0     0   4.648D-05   2.779D-01
  F =  0.27792711313709678     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36065D+00

At iterate   50    f=  2.73492D-01    |proj g|=  1.32709D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     61     70      1     0     0   3.985D-05   2.731D-01
  F =  0.27313860165618914     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40052D+00

At iterate   50    f=  2.66547D-01    |proj g|=  5.00782D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     56     69      1     0     0   8.561D-05   2.662D-01
  F =  0.26619580591693925     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35191D+00

At iterate   50    f=  2.84231D-01    |proj g|=  1.62014D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     54     64      1     0     0   8.199D-05   2.837D-01
  F =  0.28368895014461398     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.28518D+00

At iterate   50    f=  2.79322D-01    |proj g|=  6.99033D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     61     67      1     0     0   7.275D-05   2.793D-01
  F =  0.27928264180579104     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35070D+00

At iterate   50    f=  2.76364D-01    |proj g|=  3.74854D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     66     75      1     0     0   8.781D-05   2.756D-01
  F =  0.27557200104418222     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39759D+00

At iterate   50    f=  2.80583D-01    |proj g|=  9.10572D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     52     60      1     0     0   6.925D-05   2.806D-01
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
  F =  0.28058207954828795     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35869D+00

At iterate   50    f=  2.71425D-01    |proj g|=  1.22575D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     60     70      1     0     0   5.512D-05   2.709D-01
  F =  0.27088235394160909     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.38473D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     38     48      1     0     0   4.592D-05   2.791D-01
  F =  0.27911256759780834     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39755D+00

At iterate   50    f=  2.74155D-01    |proj g|=  1.10539D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     59     71      1     0     0   5.300D-05   2.731D-01
  F =  0.27313753130584612     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39203D+00

At iterate   50    f=  2.81824D-01    |proj g|=  1.40571D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     73     83      1     0     0   5.706D-05   2.805D-01
  F =  0.28046870653113132     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35810D+00

At iterate   50    f=  2.78736D-01    |proj g|=  1.98155D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     62     68      1     0     0   5.376D-05   2.778D-01
  F =  0.27780469579828271     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36162D+00

At iterate   50    f=  2.83180D-01    |proj g|=  1.17931D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     55     66      1     0     0   5.294D-05   2.823D-01
  F =  0.28226552080798767     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40809D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     45     53      1     0     0   4.640D-05   2.597D-01
  F =  0.25971823354006762     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35304D+00

At iterate   50    f=  2.82040D-01    |proj g|=  9.53150D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     54     64      1     0     0   6.257D-05   2.819D-01
  F =  0.28191932764299926     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.38858D+00

At iterate   50    f=  2.80493D-01    |proj g|=  5.31206D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     67     75      1     0     0   5.361D-05   2.792D-01
  F =  0.27916892567803037     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35892D+00

At iterate   50    f=  2.82754D-01    |proj g|=  1.18336D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     51     61      1     0     0   6.947D-05   2.827D-01
  F =  0.28274204944176906     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36229D+00

At iterate   50    f=  2.83880D-01    |proj g|=  2.40768D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     56     66      1     0     0   4.878D-05   2.832D-01
  F =  0.28315187787986679     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.37868D+00

At iterate   50    f=  2.77045D-01    |proj g|=  3.49631D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     99    114      1     0     0   7.011D-05   2.689D-01
  F =  0.26892187830011594     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40893D+00

At iterate   50    f=  2.47237D-01    |proj g|=  2.06500D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     81     91      1     0     0   9.238D-05   2.424D-01
  F =  0.24238389823426526     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.29959D+00

At iterate   50    f=  2.79356D-01    |proj g|=  1.93004D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     93    106      1     0     0   6.489D-05   2.728D-01
  F =  0.27280533484060082     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39315D+00
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

At iterate   50    f=  2.69765D-01    |proj g|=  2.35108D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     76     94      1     0     0   5.585D-05   2.675D-01
  F =  0.26749594462744325     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36790D+00

At iterate   50    f=  2.76300D-01    |proj g|=  4.05444D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     85     99      1     0     0   6.350D-05   2.679D-01
  F =  0.26793242638061449     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36476D+00

At iterate   50    f=  2.77061D-01    |proj g|=  8.40949D-03

At iterate  100    f=  2.70771D-01    |proj g|=  9.95607D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    117      1     0     0   9.956D-03   2.708D-01
  F =  0.27077110244465019     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36415D+00

At iterate   50    f=  2.77169D-01    |proj g|=  4.94170D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     96    109      1     0     0   8.797D-05   2.692D-01
  F =  0.26920377383323618     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40146D+00

At iterate   50    f=  2.73910D-01    |proj g|=  9.86138D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     74     92      1     0     0   4.022D-05   2.687D-01
  F =  0.26869939742272081     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.31334D+00

At iterate   50    f=  2.67046D-01    |proj g|=  6.02681D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     87    100      1     0     0   6.311D-05   2.655D-01
  F =  0.26552500155971248     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.33892D+00

At iterate   50    f=  2.78362D-01    |proj g|=  3.73654D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     96    109      1     0     0   6.282D-05   2.700D-01
  F =  0.26998649373626515     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.38743D+00

At iterate   50    f=  2.77263D-01    |proj g|=  1.93261D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     81     95      1     0     0   4.222D-05   2.709D-01
  F =  0.27089744763247137     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40652D+00

At iterate   50    f=  2.62927D-01    |proj g|=  4.79410D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     68     87      1     0     0   6.567D-05   2.612D-01
  F =  0.26117958654358286     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.37497D+00

At iterate   50    f=  2.69496D-01    |proj g|=  3.40049D-03

At iterate  100    f=  2.61727D-01    |proj g|=  2.88803D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    111      1     0     0   2.888D-03   2.617D-01
  F =  0.26172733421757588     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.33905D+00

At iterate   50    f=  2.76738D-01    |proj g|=  3.79626D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     93    114      1     0     0   3.816D-05   2.684D-01
  F =  0.26840477358096515     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.37324D+00

At iterate   50    f=  2.72753D-01    |proj g|=  2.59468D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     56     67      1     0     0   7.251D-05   2.719D-01
  F =  0.27189044795630307     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36501D+00

At iterate   50    f=  2.69008D-01    |proj g|=  1.00308D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     64     76      1     0     0   7.375D-05   2.669D-01
  F =  0.26691516927415793     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36065D+00

At iterate   50    f=  2.64317D-01    |proj g|=  2.56288D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     71     84      1     0     0   4.170D-05   2.625D-01
  F =  0.26247479298192017     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40052D+00

At iterate   50    f=  2.62765D-01    |proj g|=  2.18433D-03

           * * *

Tit   = total number of iterations
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     96    117      1     0     0   9.491D-05   2.560D-01
  F =  0.25596883960997663     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35191D+00

At iterate   50    f=  2.76834D-01    |proj g|=  8.49322D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     72     87      1     0     0   3.667D-05   2.727D-01
  F =  0.27270807007007092     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.28518D+00

At iterate   50    f=  2.74326D-01    |proj g|=  3.24386D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     72     80      1     0     0   3.729D-05   2.684D-01
  F =  0.26840666240976796     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35070D+00

At iterate   50    f=  2.69531D-01    |proj g|=  1.12319D-02

At iterate  100    f=  2.61849D-01    |proj g|=  4.39524D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    112      1     0     0   4.395D-04   2.618D-01
  F =  0.26184881045510533     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39759D+00

At iterate   50    f=  2.71411D-01    |proj g|=  4.63001D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     64     79      1     0     0   5.700D-05   2.692D-01
  F =  0.26924729298725297     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35869D+00

At iterate   50    f=  2.62839D-01    |proj g|=  1.43078D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     80     95      1     0     0   8.367D-05   2.607D-01
  F =  0.26067558925869261     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.38473D+00

At iterate   50    f=  2.70935D-01    |proj g|=  1.01776D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     92    107      1     0     0   4.553D-05   2.677D-01
  F =  0.26767426316219617     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39755D+00

At iterate   50    f=  2.69817D-01    |proj g|=  3.34957D-03

At iterate  100    f=  2.62135D-01    |proj g|=  3.47121D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    114      1     0     0   3.471D-04   2.621D-01
  F =  0.26213528851888451     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39203D+00

At iterate   50    f=  2.76652D-01    |proj g|=  2.54837D-02

At iterate  100    f=  2.69230D-01    |proj g|=  1.41872D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    115      1     0     0   1.419D-03   2.692D-01
  F =  0.26923038241261987     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35810D+00

At iterate   50    f=  2.68555D-01    |proj g|=  1.05395D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     70     80      1     0     0   5.795D-05   2.657D-01
  F =  0.26569597692041913     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36162D+00

At iterate   50    f=  2.78193D-01    |proj g|=  7.17271D-03

At iterate  100    f=  2.71419D-01    |proj g|=  4.25384D-05

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    117      1     0     0   4.254D-05   2.714D-01
  F =  0.27141889183387757     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40809D+00

At iterate   50    f=  2.52022D-01    |proj g|=  2.19511D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     83     99      1     0     0   7.395D-05   2.504D-01
  F =  0.25039333074006115     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35304D+00

At iterate   50    f=  2.73399D-01    |proj g|=  1.50967D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     92    111      1     0     0   7.181D-05   2.710D-01
  F =  0.27103628385123757     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.38858D+00

At iterate   50    f=  2.73180D-01    |proj g|=  1.21989D-02

At iterate  100    f=  2.68079D-01    |proj g|=  1.01061D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    114      1     0     0   1.011D-04   2.681D-01
  F =  0.26807852406126487     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35892D+00

At iterate   50    f=  2.75898D-01    |proj g|=  1.29094D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     89    107      1     0     0   7.968D-05   2.717D-01
  F =  0.27173877686316955     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36229D+00

At iterate   50    f=  2.74570D-01    |proj g|=  3.05223D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     66     82      1     0     0   8.516D-05   2.723D-01
  F =  0.27226643909893666     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.37868D+00

At iterate   50    f=  2.76408D-01    |proj g|=  1.46124D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     93    111      1     0     0   6.223D-05   2.643D-01
  F =  0.26428974695586815     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40893D+00

At iterate   50    f=  2.47353D-01    |proj g|=  4.11523D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     91    106      1     0     0   8.795D-05   2.385D-01
  F =  0.23850782047540586     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.29959D+00

At iterate   50    f=  2.75456D-01    |proj g|=  8.43812D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     82     96      1     0     0   7.570D-05   2.681D-01
  F =  0.26807303054375681     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39315D+00

At iterate   50    f=  2.73537D-01    |proj g|=  1.18503D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     81     97      1     0     0   6.165D-05   2.629D-01
  F =  0.26285747746625093     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36790D+00

At iterate   50    f=  2.75032D-01    |proj g|=  1.60150D-02

At iterate  100    f=  2.63084D-01    |proj g|=  6.66163D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    112      1     0     0   6.662D-04   2.631D-01
  F =  0.26308416184578953     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36476D+00

At iterate   50    f=  2.76273D-01    |proj g|=  2.16685D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     93    107      1     0     0   6.174D-05   2.659D-01
  F =  0.26586439354150326     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36415D+00

At iterate   50    f=  2.76105D-01    |proj g|=  2.76053D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     95    109      1     0     0   7.688D-05   2.646D-01
  F =  0.26458624440182293     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40146D+00

At iterate   50    f=  2.68184D-01    |proj g|=  7.86391D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     83    101      1     0     0   5.201D-05   2.640D-01
  F =  0.26399200763555064     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.31334D+00

At iterate   50    f=  2.64757D-01    |proj g|=  2.80304D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     81     97      1     0     0   8.702D-05   2.608D-01
  F =  0.26075222533238618     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.33892D+00

At iterate   50    f=  2.77785D-01    |proj g|=  5.98346D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     97    108      1     0     0   6.607D-05   2.651D-01
  F =  0.26505994660404664     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.38743D+00

At iterate   50    f=  2.78662D-01    |proj g|=  1.14429D-03

At iterate  100    f=  2.67860D-01    |proj g|=  1.19951D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    116      1     0     0   1.200D-02   2.679D-01
  F =  0.26786026495664894     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40652D+00

At iterate   50    f=  2.60851D-01    |proj g|=  7.30639D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     88    105      1     0     0   8.732D-05   2.565D-01
  F =  0.25646748078647841     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.37497D+00

At iterate   50    f=  2.64547D-01    |proj g|=  1.82683D-03

           * * *
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     75     85      1     0     0   6.218D-05   2.569D-01
  F =  0.25693748878561545     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.33905D+00

At iterate   50    f=  2.76376D-01    |proj g|=  1.93754D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     88    104      1     0     0   8.963D-05   2.630D-01
  F =  0.26304802812381406     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.37324D+00

At iterate   50    f=  2.70858D-01    |proj g|=  1.29002D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     71     82      1     0     0   7.874D-05   2.673D-01
  F =  0.26732011624845159     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36501D+00

At iterate   50    f=  2.73332D-01    |proj g|=  4.38219D-03

At iterate  100    f=  2.62251D-01    |proj g|=  3.76474D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    113      1     0     0   3.765D-04   2.623D-01
  F =  0.26225057644757643     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36065D+00

At iterate   50    f=  2.61246D-01    |proj g|=  1.98075D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     65     77      1     0     0   6.262D-05   2.578D-01
  F =  0.25778535404016217     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40052D+00

At iterate   50    f=  2.62331D-01    |proj g|=  9.71475D-03

At iterate  100    f=  2.55465D-01    |proj g|=  2.95447D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    115      1     0     0   2.954D-03   2.555D-01
  F =  0.25546460782807506     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35191D+00

At iterate   50    f=  2.72144D-01    |proj g|=  4.69795D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     66     78      1     0     0   5.285D-05   2.680D-01
  F =  0.26802859362362214     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.28518D+00

At iterate   50    f=  2.72475D-01    |proj g|=  2.12831D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     72     81      1     0     0   5.389D-05   2.638D-01
  F =  0.26378264213729785     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35070D+00

At iterate   50    f=  2.68552D-01    |proj g|=  9.71261D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     78     94      1     0     0   5.470D-05   2.552D-01
  F =  0.25520640049472371     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39759D+00

At iterate   50    f=  2.69250D-01    |proj g|=  8.32373D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     73     85      1     0     0   8.604D-05   2.646D-01
  F =  0.26455946431345112     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35869D+00

At iterate   50    f=  2.60623D-01    |proj g|=  2.23809D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     75     90      1     0     0   5.180D-05   2.562D-01
  F =  0.25623098691052015     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.38473D+00

At iterate   50    f=  2.66939D-01    |proj g|=  5.42257D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     66     78      1     0     0   5.491D-05   2.627D-01
  F =  0.26268026512514292     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39755D+00

At iterate   50    f=  2.69621D-01    |proj g|=  3.88582D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     95    113      1     0     0   9.023D-05   2.575D-01
  F =  0.25747658906934417     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39203D+00

At iterate   50    f=  2.68651D-01    |proj g|=  1.09575D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     74     87      1     0     0   5.985D-05   2.642D-01
  F =  0.26416990531209972     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35810D+00

At iterate   50    f=  2.64914D-01    |proj g|=  3.17408D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     88    102      1     0     0   7.128D-05   2.601D-01
  F =  0.26008376823377327     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36162D+00

At iterate   50    f=  2.77470D-01    |proj g|=  1.38177D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     84     99      1     0     0   6.756D-05   2.669D-01
  F =  0.26688827165835222     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40809D+00

At iterate   50    f=  2.49474D-01    |proj g|=  4.32754D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     78     94      1     0     0   9.919D-05   2.461D-01
  F =  0.24613958927214491     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35304D+00

At iterate   50    f=  2.70997D-01    |proj g|=  1.80893D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     60     75      1     0     0   5.603D-05   2.666D-01
  F =  0.26656269987818443     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.38858D+00

At iterate   50    f=  2.72917D-01    |proj g|=  1.75108D-02

At iterate  100    f=  2.63409D-01    |proj g|=  7.55790D-05

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    117      1     0     0   7.558D-05   2.634D-01
  F =  0.26340873572875689     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35892D+00

At iterate   50    f=  2.79098D-01    |proj g|=  1.51500D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     85    104      1     0     0   6.636D-05   2.672D-01
  F =  0.26723542381350812     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36229D+00

At iterate   50    f=  2.71961D-01    |proj g|=  1.01032D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     67     82      1     0     0   5.405D-05   2.677D-01
  F =  0.26770508338208182     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.37868D+00

At iterate   50    f=  2.76303D-01    |proj g|=  1.51272D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     87    102      1     0     0   6.953D-05   2.636D-01
  F =  0.26361739563894759     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40893D+00

At iterate   50    f=  2.46763D-01    |proj g|=  4.81313D-03

At iterate  100    f=  2.37246D-01    |proj g|=  1.83450D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    117      1     0     0   1.835D-03   2.372D-01
  F =  0.23724601844840057     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.29959D+00

At iterate   50    f=  2.76072D-01    |proj g|=  7.02773D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
   41     75     86      1     0     0   8.216D-05   2.674D-01
  F =  0.26736473354548385     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39315D+00

At iterate   50    f=  2.68092D-01    |proj g|=  1.61434D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     98    117      1     0     0   8.518D-05   2.622D-01
  F =  0.26215439003125812     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36790D+00

At iterate   50    f=  2.69927D-01    |proj g|=  1.60656D-02

At iterate  100    f=  2.62547D-01    |proj g|=  2.90426D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    119      1     0     0   2.904D-03   2.625D-01
  F =  0.26254741758653422     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36476D+00

At iterate   50    f=  2.76353D-01    |proj g|=  5.03445D-03

At iterate  100    f=  2.65207D-01    |proj g|=  5.23169D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    117      1     0     0   5.232D-04   2.652D-01
  F =  0.26520712522337297     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36415D+00

At iterate   50    f=  2.76465D-01    |proj g|=  1.64304D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     91    104      1     0     0   9.020D-05   2.639D-01
  F =  0.26386163836003129     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40146D+00

At iterate   50    f=  2.68280D-01    |proj g|=  1.54527D-02

           * * *

Tit   = total number of iterations
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     82     99      1     0     0   8.844D-05   2.633D-01
  F =  0.26328945100767454     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.31334D+00

At iterate   50    f=  2.64172D-01    |proj g|=  2.63654D-03

At iterate  100    f=  2.60062D-01    |proj g|=  2.11960D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    118      1     0     0   2.120D-04   2.601D-01
  F =  0.26006217809022530     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.33892D+00

At iterate   50    f=  2.77707D-01    |proj g|=  5.80554D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     88    100      1     0     0   8.109D-05   2.643D-01
  F =  0.26431867618407484     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.38743D+00

At iterate   50    f=  2.78567D-01    |proj g|=  6.37354D-03

At iterate  100    f=  2.65879D-01    |proj g|=  5.44410D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    116      1     0     0   5.444D-04   2.659D-01
  F =  0.26587880673182557     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40652D+00

At iterate   50    f=  2.60924D-01    |proj g|=  1.80629D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     71     82      1     0     0   9.436D-05   2.558D-01
  F =  0.25584178086675946     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.37497D+00

At iterate   50    f=  2.60575D-01    |proj g|=  2.02122D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     68     74      1     0     0   6.821D-05   2.563D-01
  F =  0.25627550889111589     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.33905D+00

At iterate   50    f=  2.76383D-01    |proj g|=  4.55816D-03

At iterate  100    f=  2.62612D-01    |proj g|=  8.42162D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    116      1     0     0   8.422D-03   2.626D-01
  F =  0.26261227660118069     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.37324D+00

At iterate   50    f=  2.70320D-01    |proj g|=  1.99912D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     61     76      1     0     0   7.994D-05   2.666D-01
  F =  0.26660417241152184     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36501D+00

At iterate   50    f=  2.72325D-01    |proj g|=  3.10349D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     85    100      1     0     0   6.977D-05   2.615D-01
  F =  0.26152848630125070     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36065D+00

At iterate   50    f=  2.61482D-01    |proj g|=  1.28389D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     69     82      1     0     0   7.877D-05   2.571D-01
  F =  0.25705721612130322     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40052D+00

At iterate   50    f=  2.62269D-01    |proj g|=  8.23633D-03

At iterate  100    f=  2.51035D-01    |proj g|=  1.59420D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    119      1     0     0   1.594D-04   2.510D-01
  F =  0.25103460696394297     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35191D+00

At iterate   50    f=  2.72349D-01    |proj g|=  1.36699D-03

At iterate  100    f=  2.67296D-01    |proj g|=  5.73769D-05

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    119      1     0     0   5.738D-05   2.673D-01
  F =  0.26729555363692797     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.28518D+00

At iterate   50    f=  2.72275D-01    |proj g|=  1.75739D-02

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     90    101      1     0     0   6.828D-05   2.630D-01
  F =  0.26300610870621355     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35070D+00

At iterate   50    f=  2.68348D-01    |proj g|=  1.53112D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     97    111      1     0     0   6.862D-05   2.540D-01
  F =  0.25404559226248224     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39759D+00

At iterate   50    f=  2.75462D-01    |proj g|=  6.00056D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     99    115      1     0     0   7.865D-05   2.638D-01
  F =  0.26378881334044096     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35869D+00

At iterate   50    f=  2.66677D-01    |proj g|=  6.12899D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     86    103      1     0     0   6.323D-05   2.555D-01
  F =  0.25554561873588477     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.38473D+00

At iterate   50    f=  2.62328D-01    |proj g|=  8.48353D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     53     66      1     0     0   8.708D-05   2.620D-01
  F =  0.26197921989772827     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39755D+00

At iterate   50    f=  2.69695D-01    |proj g|=  7.48273D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     94    105      1     0     0   6.368D-05   2.569D-01
  F =  0.25690878068958917     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.39203D+00

At iterate   50    f=  2.76305D-01    |proj g|=  4.45446D-03

At iterate  100    f=  2.63617D-01    |proj g|=  7.97254D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    118      1     0     0   7.973D-04   2.636D-01
  F =  0.26361654794868417     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35810D+00

At iterate   50    f=  2.66580D-01    |proj g|=  2.21201D-02

At iterate  100    f=  2.59198D-01    |proj g|=  1.35581D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    117      1     0     0   1.356D-04   2.592D-01
  F =  0.25919788917314224     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36162D+00

At iterate   50    f=  2.78267D-01    |proj g|=  1.07517D-02

At iterate  100    f=  2.66279D-01    |proj g|=  1.65380D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    114      1     0     0   1.654D-04   2.663D-01
  F =  0.26627884753942571     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.40809D+00

At iterate   50    f=  2.49247D-01    |proj g|=  5.66189D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     97    117      1     0     0   6.209D-05   2.455D-01
  F =  0.24550340719998209     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35304D+00

At iterate   50    f=  2.71350D-01    |proj g|=  4.35505D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     72     91      1     0     0   9.085D-05   2.659D-01
  F =  0.26585903803727201     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.38858D+00

At iterate   50    f=  2.67451D-01    |proj g|=  7.23664D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/home/michael.cueva/.conda/envs/thesis-writing-1/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/scratch1/michael.cueva/thesis-writing-1/server-side/modelling
subjects features, labels, names and subject to id lookup loaded
        raw_128hz_max  ...  subject_id
0            0.000222  ...           0
1            0.000222  ...           0
2            0.000222  ...           0
3            0.000222  ...           0
4            0.000222  ...           0
...               ...  ...         ...
405122       4.199053  ...          32
405123       4.178632  ...          32
405124       4.188843  ...          32
405125       4.173527  ...          32
405126       4.153106  ...          32

[405127 rows x 139 columns]
          0  subject_id
0       0.0           0
1       0.0           0
2       0.0           0
3       0.0           0
4       0.0           0
...     ...         ...
405122  0.0          32
405123  0.0          32
405124  0.0          32
405125  0.0          32
405126  0.0          32

[405127 rows x 2 columns]
file not found please run needed script first to produce file
Fitting estimator with 138 features.
Fitting estimator with 137 features.
Fitting estimator with 136 features.
Fitting estimator with 135 features.
Fitting estimator with 134 features.
Fitting estimator with 133 features.
Fitting estimator with 132 features.
Fitting estimator with 131 features.
Fitting estimator with 130 features.
Fitting estimator with 129 features.
Fitting estimator with 128 features.
Fitting estimator with 127 features.
Fitting estimator with 126 features.
Fitting estimator with 125 features.
Fitting estimator with 124 features.
Fitting estimator with 123 features.
Fitting estimator with 122 features.
Fitting estimator with 121 features.
Fitting estimator with 120 features.
Fitting estimator with 119 features.
Fitting estimator with 118 features.
Fitting estimator with 117 features.
Fitting estimator with 116 features.
Fitting estimator with 115 features.
Fitting estimator with 114 features.
Fitting estimator with 113 features.
Fitting estimator with 112 features.
Fitting estimator with 111 features.
Fitting estimator with 110 features.
Fitting estimator with 109 features.
Fitting estimator with 108 features.
Fitting estimator with 107 features.
Fitting estimator with 106 features.
Fitting estimator with 105 features.
Fitting estimator with 104 features.
Fitting estimator with 103 features.
Fitting estimator with 102 features.
Fitting estimator with 101 features.
Fitting estimator with 100 features.
Fitting estimator with 99 features.
Fitting estimator with 98 features.
Fitting estimator with 97 features.
Fitting estimator with 96 features.
Fitting estimator with 95 features.
Fitting estimator with 94 features.
Fitting estimator with 93 features.
Fitting estimator with 92 features.
Fitting estimator with 91 features.
Fitting estimator with 90 features.
Fitting estimator with 89 features.
Fitting estimator with 88 features.
Fitting estimator with 87 features.
Fitting estimator with 86 features.
Fitting estimator with 85 features.
Fitting estimator with 84 features.
Fitting estimator with 83 features.
Fitting estimator with 82 features.
Fitting estimator with 81 features.
Fitting estimator with 80 features.
Fitting estimator with 79 features.
Fitting estimator with 78 features.
Fitting estimator with 77 features.
Fitting estimator with 76 features.
Fitting estimator with 75 features.
Fitting estimator with 74 features.
Fitting estimator with 73 features.
Fitting estimator with 72 features.
Fitting estimator with 71 features.
Fitting estimator with 70 features.
Fitting estimator with 69 features.
Fitting estimator with 68 features.
Fitting estimator with 67 features.
Fitting estimator with 66 features.
Fitting estimator with 65 features.
Fitting estimator with 64 features.
Fitting estimator with 63 features.
Fitting estimator with 62 features.
Fitting estimator with 61 features.
Fitting estimator with 60 features.
Fitting estimator with 59 features.
Fitting estimator with 58 features.
Fitting estimator with 57 features.
Fitting estimator with 56 features.
Fitting estimator with 55 features.
Fitting estimator with 54 features.
Fitting estimator with 53 features.
Fitting estimator with 52 features.
Fitting estimator with 51 features.
Fitting estimator with 50 features.
Fitting estimator with 49 features.
Fitting estimator with 48 features.
Fitting estimator with 47 features.
Fitting estimator with 46 features.
Fitting estimator with 45 features.
Fitting estimator with 44 features.
Fitting estimator with 43 features.
Fitting estimator with 42 features.
Fitting estimator with 41 features.
selected features: ['raw_128hz_min', 'raw_128hz_amp', 'raw_128hz_std', 'raw_128hz_1d_max', 'raw_128hz_1d_min', 'raw_128hz_1d_amp', 'raw_128hz_1d_std', 'raw_128hz_1d_avg_abs', 'raw_128hz_2d_avg_abs', 'filt_128hz_min', 'filt_128hz_std', 'filt_128hz_range', 'filt_128hz_1d_min', 'filt_128hz_1d_std', 'filt_128hz_1d_range', 'third_16thofa_sec_max', 'first_16thofa_sec_mean', 'third_16thofa_sec_mean', 'second_16thofa_sec_std', 'third_16thofa_sec_std', 'first_16thofa_sec_median', 'first_32thofa_sec_mean', 'second_32thofa_sec_mean', 'first_32thofa_sec_std', 'second_32thofa_sec_std', 'raw_16hz_min', 'raw_16hz_std', 'raw_16hz_1d_min', 'filt_16hz_min', 'filt_16hz_std', 'filt_16hz_range', 'filt_16hz_1d_std', 'filt_16hz_1d_range', 'first_2thofa_sec_max', 'second_2thofa_sec_max', 'third_2thofa_sec_max', 'second_2thofa_sec_mean', 'second_2thofa_sec_median', 'second_4thofa_sec_max', 'second_4thofa_sec_std']
fold: 0 with hyper params: {'C': 0.01}               
train acc: 0.9057798085778689 cross acc: 0.9475515784240617               
train prec: 0.8950853074306554 cross prec: 0.9503046964983272               
train rec: 0.9057798085778689 cross rec: 0.9475515784240617               
train f1: 0.8727929579728051 cross f1: 0.9228373903648881               
train roc_auc_prob: 0.7857660895386163 cross roc_auc_prob: 0.8007990561484422
fold: 1 with hyper params: {'C': 0.01}               
train acc: 0.9183844166937846 cross acc: 0.506662140371722               
train prec: 0.9071457817090054 cross prec: 0.7500857018706762               
train rec: 0.9183844166937846 cross rec: 0.506662140371722               
train f1: 0.8906234182460927 cross f1: 0.3409497186837662               
train roc_auc_prob: 0.7968781943272683 cross roc_auc_prob: 0.8312086069815744
fold: 2 with hyper params: {'C': 0.01}               
train acc: 0.9044243280750671 cross acc: 0.9898485336770867               
train prec: 0.8934768733709954 cross prec: 0.9869836650097442               
train rec: 0.9044243280750671 cross rec: 0.9898485336770867               
train f1: 0.8709817458587586 cross f1: 0.9865896947087162               
train roc_auc_prob: 0.786206529563212 cross roc_auc_prob: 0.7907819198199584
fold: 3 with hyper params: {'C': 0.01}               
train acc: 0.9062330609513097 cross acc: 0.9338754723180549               
train prec: 0.89560807687696 cross prec: 0.9272404182742735               
train rec: 0.9062330609513097 cross rec: 0.9338754723180549               
train f1: 0.8734818782610955 cross f1: 0.9024281751806223               
train roc_auc_prob: 0.7858206485656937 cross roc_auc_prob: 0.8433433377364309
fold: 4 with hyper params: {'C': 0.01}               
train acc: 0.9060496484358125 cross acc: 0.9384482758620689               
train prec: 0.8952033910246271 cross prec: 0.9299467517211639               
train rec: 0.9060496484358125 cross rec: 0.9384482758620689               
train f1: 0.8731995410915724 cross f1: 0.9090735105031449               
train roc_auc_prob: 0.7871489980287595 cross roc_auc_prob: 0.7927449705636637
fold: 5 with hyper params: {'C': 0.01}               
train acc: 0.9054536473080605 cross acc: 0.9645939188644246               
train prec: 0.8964369910495968 cross prec: 0.9719826705774892               
train rec: 0.9054536473080605 cross rec: 0.9645939188644246               
train f1: 0.8726124327150898 cross f1: 0.9681877237662266               
train roc_auc_prob: 0.7888705822891241 cross roc_auc_prob: 0.6853923422722579
fold: 6 with hyper params: {'C': 0.01}               
train acc: 0.9056571665146947 cross acc: 0.9571578762922727               
train prec: 0.895228289694858 cross prec: 0.9340414646961546               
train rec: 0.9056571665146947 cross rec: 0.9571578762922727               
train f1: 0.8726725900757635 cross f1: 0.9407033095116284               
train roc_auc_prob: 0.788235626661205 cross roc_auc_prob: 0.6681163132974428
fold: 7 with hyper params: {'C': 0.01}               
train acc: 0.9054705741711984 cross acc: 0.9624864232600886               
train prec: 0.8950834208865106 cross prec: 0.9453601601568352               
train rec: 0.9054705741711984 cross rec: 0.9624864232600886               
train f1: 0.8723979462814481 cross f1: 0.9445781608037387               
train roc_auc_prob: 0.7858088132338824 cross roc_auc_prob: 0.9240723985399654
fold: 8 with hyper params: {'C': 0.01}               
train acc: 0.9070319546776662 cross acc: 0.9138178684286125               
train prec: 0.8974682878598174 cross prec: 0.892161387646251               
train rec: 0.9070319546776662 cross rec: 0.9138178684286125               
train f1: 0.8746813521339877 cross f1: 0.9007199489471351               
train roc_auc_prob: 0.7947002399436413 cross roc_auc_prob: 0.689955841739156
fold: 9 with hyper params: {'C': 0.01}               
train acc: 0.9054326518470305 cross acc: 0.9586294416243655               
train prec: 0.8945811672949056 cross prec: 0.960344737795782               
train rec: 0.9054326518470305 cross rec: 0.9586294416243655               
train f1: 0.872260392840891 cross f1: 0.9403702984352456               
train roc_auc_prob: 0.7858399180970764 cross roc_auc_prob: 0.8282608863678328
fold: 10 with hyper params: {'C': 0.01}               
train acc: 0.9046434873703856 cross acc: 0.9902912621359223               
train prec: 0.8940700601737739 cross prec: 0.9887931418593201               
train rec: 0.9046434873703856 cross rec: 0.9902912621359223               
train f1: 0.8712642184700131 cross f1: 0.9859261905203333               
train roc_auc_prob: 0.7854446721471576 cross roc_auc_prob: 0.7728369191783826
fold: 11 with hyper params: {'C': 0.01}               
train acc: 0.9095182826570557 cross acc: 0.8189690384772486               
train prec: 0.898720016235357 cross prec: 0.8518945646373287               
train rec: 0.9095182826570557 cross rec: 0.8189690384772486               
train f1: 0.87781148832733 cross f1: 0.7419370291025187               
train roc_auc_prob: 0.7865569342655913 cross roc_auc_prob: 0.7802100906799692
fold: 12 with hyper params: {'C': 0.01}               
train acc: 0.909547010329202 cross acc: 0.8163059041522783               
train prec: 0.8991614464950725 cross prec: 0.8137815121308523               
train rec: 0.909547010329202 cross rec: 0.8163059041522783               
train f1: 0.8771640423637123 cross f1: 0.7595658532460771               
train roc_auc_prob: 0.7857335787913626 cross roc_auc_prob: 0.8125879693078555
fold: 13 with hyper params: {'C': 0.01}               
train acc: 0.9057301717422186 cross acc: 0.946711006569876               
train prec: 0.8950107339868377 cross prec: 0.949557881564476               
train rec: 0.9057301717422186 cross rec: 0.946711006569876               
train f1: 0.8726223064122511 cross f1: 0.9230953398485484               
train roc_auc_prob: 0.7860281270332529 cross roc_auc_prob: 0.8204677782816548
fold: 14 with hyper params: {'C': 0.01}               
train acc: 0.9043822765125938 cross acc: 0.9809799901011101               
train prec: 0.8935863422256998 cross prec: 0.9801989622175239               
train rec: 0.9043822765125938 cross rec: 0.9809799901011101               
train f1: 0.8708594947240574 cross f1: 0.9734563902033765               
train roc_auc_prob: 0.7849279862418059 cross roc_auc_prob: 0.840319113095368
fold: 15 with hyper params: {'C': 0.01}               
train acc: 0.9068826117321995 cross acc: 0.9073018352206169               
train prec: 0.8961179156099864 cross prec: 0.9066298942348926               
train rec: 0.9068826117321995 cross rec: 0.9073018352206169               
train f1: 0.8741201509570895 cross f1: 0.8687598548237261               
train roc_auc_prob: 0.7856236184865111 cross roc_auc_prob: 0.8106897206832075
fold: 16 with hyper params: {'C': 0.01}               
train acc: 0.9091463321310321 cross acc: 0.8377986413250896               
train prec: 0.8981309609474893 cross prec: 0.8366810428996637               
train rec: 0.9091463321310321 cross rec: 0.8377986413250896               
train f1: 0.8756734022353694 cross f1: 0.8053449310176206               
train roc_auc_prob: 0.7803501655696354 cross roc_auc_prob: 0.8390677990463653
fold: 17 with hyper params: {'C': 0.01}               
train acc: 0.9122312050416959 cross acc: 0.7314949201741655               
train prec: 0.9014706986170391 cross prec: 0.8041818493522833               
train rec: 0.9122312050416959 cross rec: 0.7314949201741655               
train f1: 0.8814979654417858 cross f1: 0.625936874594924               
train roc_auc_prob: 0.7917034346481054 cross roc_auc_prob: 0.7533443443413848
fold: 18 with hyper params: {'C': 0.01}               
train acc: 0.9042875604415772 cross acc: 0.9918007184132438               
train prec: 0.8935227698536425 cross prec: 0.9918680096877952               
train rec: 0.9042875604415772 cross rec: 0.9918007184132438               
train f1: 0.8707236350706574 cross f1: 0.9884806936236799               
train roc_auc_prob: 0.7838141545478994 cross roc_auc_prob: 0.8549372799510715
fold: 19 with hyper params: {'C': 0.01}               
train acc: 0.9058394067742715 cross acc: 0.9490101480618865               
train prec: 0.8953311350146879 cross prec: 0.9233309990488007               
train rec: 0.9058394067742715 cross rec: 0.9490101480618865               
train f1: 0.8729571688985842 cross f1: 0.92684405082419               
train roc_auc_prob: 0.7854255829992189 cross roc_auc_prob: 0.5178066375567278
fold: 20 with hyper params: {'C': 0.01}               
train acc: 0.9067713092870325 cross acc: 0.9249861725663717               
train prec: 0.8970238931753198 cross prec: 0.9142971790699599               
train rec: 0.9067713092870325 cross rec: 0.9249861725663717               
train f1: 0.8737888532393133 cross f1: 0.9167904554232383               
train roc_auc_prob: 0.7883468339941957 cross roc_auc_prob: 0.8160894462312871
fold: 21 with hyper params: {'C': 0.01}               
train acc: 0.905136099127247 cross acc: 0.9739255987769662               
train prec: 0.8946476061350833 cross prec: 0.9485310719530721               
train rec: 0.905136099127247 cross rec: 0.9739255987769662               
train f1: 0.8719319202973884 cross f1: 0.9610606119509032               
train roc_auc_prob: 0.7863931976997607 cross roc_auc_prob: 0.9040052335422792
fold: 22 with hyper params: {'C': 0.01}               
train acc: 0.9096362505353756 cross acc: 0.8204053109713487               
train prec: 0.8993033219669626 cross prec: 0.8397123571399822               
train rec: 0.9096362505353756 cross rec: 0.8204053109713487               
train f1: 0.8773359147307522 cross f1: 0.7563168922558384               
train roc_auc_prob: 0.788236335990291 cross roc_auc_prob: 0.7146911838992923
fold: 23 with hyper params: {'C': 0.01}               
train acc: 0.9061258180833148 cross acc: 0.930877243775333               
train prec: 0.8955026527618763 cross prec: 0.9317039533048882               
train rec: 0.9061258180833148 cross rec: 0.930877243775333               
train f1: 0.8731792094589722 cross f1: 0.899920525337674               
train roc_auc_prob: 0.7853111891134859 cross roc_auc_prob: 0.8639747580705085
fold: 24 with hyper params: {'C': 0.01}               
train acc: 0.908913706515009 cross acc: 0.8423089071383449               
train prec: 0.8982691341941859 cross prec: 0.8671989729338495               
train rec: 0.908913706515009 cross rec: 0.8423089071383449               
train f1: 0.8770713738188332 cross f1: 0.7711553526277968               
train roc_auc_prob: 0.7867776773698915 cross roc_auc_prob: 0.7954336410600913
fold: 25 with hyper params: {'C': 0.01}               
train acc: 0.9052828168256409 cross acc: 0.9705562601920638               
train prec: 0.8946782084394835 cross prec: 0.9714233511160923               
train rec: 0.9052828168256409 cross rec: 0.9705562601920638               
train f1: 0.8720813245034343 cross f1: 0.9562333918984478               
train roc_auc_prob: 0.7852279061533535 cross roc_auc_prob: 0.8234037031295103
fold: 26 with hyper params: {'C': 0.01}               
train acc: 0.9068456396262972 cross acc: 0.9070628529112283               
train prec: 0.895696204332557 cross prec: 0.906898223831313               
train rec: 0.9068456396262972 cross rec: 0.9070628529112283               
train f1: 0.8732908801474741 cross f1: 0.8849416205180097               
train roc_auc_prob: 0.7821975506850122 cross roc_auc_prob: 0.8683781175166676
fold: 27 with hyper params: {'C': 0.01}               
train acc: 0.9045959889458739 cross acc: 0.994939454183987               
train prec: 0.8937154650286956 cross prec: 0.9940225898915153               
train rec: 0.9045959889458739 cross rec: 0.994939454183987               
train f1: 0.8712017086045897 cross f1: 0.9931179992401061               
train roc_auc_prob: 0.7868959985648804 cross roc_auc_prob: 0.8817893200126639
fold: 28 with hyper params: {'C': 0.01}               
train acc: 0.9157375013097334 cross acc: 0.6362650079560249               
train prec: 0.9020822115137074 cross prec: 0.7540614217510034               
train rec: 0.9157375013097334 cross rec: 0.6362650079560249               
train f1: 0.8832284718315226 cross f1: 0.5666619139197034               
train roc_auc_prob: 0.7815891950234264 cross roc_auc_prob: 0.8116132316964406
fold: 29 with hyper params: {'C': 0.01}               
train acc: 0.905009066010218 cross acc: 0.9763025210084033               
train prec: 0.8942486231759633 cross prec: 0.9710242601298201               
train rec: 0.905009066010218 cross rec: 0.9763025210084033               
train f1: 0.8717367189526616 cross f1: 0.9690948670389544               
train roc_auc_prob: 0.7875535117889386 cross roc_auc_prob: 0.8684569890522384
fold: 30 with hyper params: {'C': 0.01}               
train acc: 0.906117920136719 cross acc: 0.9340314936554044               
train prec: 0.8953353960015111 cross prec: 0.9359079365055288               
train rec: 0.906117920136719 cross rec: 0.9340314936554044               
train f1: 0.8731496473547241 cross f1: 0.9073605940653968               
train roc_auc_prob: 0.7856779520385868 cross roc_auc_prob: 0.8251970475923923
fold: 31 with hyper params: {'C': 0.01}               
train acc: 0.9044936253694287 cross acc: 0.9958670570001722               
train prec: 0.8937992071341319 cross prec: 0.9929498966712322               
train rec: 0.9044936253694287 cross rec: 0.9958670570001722               
train f1: 0.871085196615991 cross f1: 0.9944063374169968               
train roc_auc_prob: 0.7841972315504191 cross roc_auc_prob: 0.9428990522515612
fold: 32 with hyper params: {'C': 0.01}               
train acc: 0.904321710955331 cross acc: 0.99426340530635               
train prec: 0.8935439596152633 cross prec: 0.9931795189384683               
train rec: 0.904321710955331 cross rec: 0.99426340530635               
train f1: 0.8708619259360761 cross f1: 0.9917616692991786               
train roc_auc_prob: 0.7858696857046931 cross roc_auc_prob: 0.8487346371700332
fold: 0 with hyper params: {'C': 0.1}               
train acc: 0.9088277048171007 cross acc: 0.9484630043914161               
train prec: 0.8980227800327651 cross prec: 0.9488777823706781               
train rec: 0.9088277048171007 cross rec: 0.9484630043914161               
train f1: 0.88049231916196 cross f1: 0.9251549734710974               
train roc_auc_prob: 0.8108893843921766 cross roc_auc_prob: 0.8094545222908436
fold: 1 with hyper params: {'C': 0.1}               
train acc: 0.921186035632932 cross acc: 0.5076805567342781               
train prec: 0.9102425113356728 cross prec: 0.7503473160498064               
train rec: 0.921186035632932 cross rec: 0.5076805567342781               
train f1: 0.8976440833409011 cross f1: 0.34321140132300404               
train roc_auc_prob: 0.8203215414651447 cross roc_auc_prob: 0.8224606903197104
fold: 2 with hyper params: {'C': 0.1}               
train acc: 0.9074417834816597 cross acc: 0.9904125040283597               
train prec: 0.8962823617526958 cross prec: 0.9888045190937053               
train rec: 0.9074417834816597 cross rec: 0.9904125040283597               
train f1: 0.8787061118875933 cross f1: 0.9871882336504497               
train roc_auc_prob: 0.8089751383757672 cross roc_auc_prob: 0.7786067895703801
fold: 3 with hyper params: {'C': 0.1}               
train acc: 0.9092843164449692 cross acc: 0.9341218991292919               
train prec: 0.8985631735826983 cross prec: 0.9158838020950191               
train rec: 0.9092843164449692 cross rec: 0.9341218991292919               
train f1: 0.8811614401701593 cross f1: 0.9041049546420586               
train roc_auc_prob: 0.8116876487385175 cross roc_auc_prob: 0.8423506558693797
fold: 4 with hyper params: {'C': 0.1}               
train acc: 0.9090202197053824 cross acc: 0.9388793103448276               
train prec: 0.8980083542161316 cross prec: 0.9295857653495513               
train rec: 0.9090202197053824 cross rec: 0.9388793103448276               
train f1: 0.8807711842630572 cross f1: 0.9104456979708473               
train roc_auc_prob: 0.8125763888606898 cross roc_auc_prob: 0.8111609587509392
fold: 5 with hyper params: {'C': 0.1}               
train acc: 0.9083691511682386 cross acc: 0.9603193805952093               
train prec: 0.8980237629342592 cross prec: 0.9728434173665911               
train rec: 0.9083691511682386 cross rec: 0.9603193805952093               
train f1: 0.8803190184783539 cross f1: 0.9663013849066853               
train roc_auc_prob: 0.8119483812829559 cross roc_auc_prob: 0.7005313267276647
fold: 6 with hyper params: {'C': 0.1}               
train acc: 0.9086288743323182 cross acc: 0.9573330997021202               
train prec: 0.8978213054727501 cross prec: 0.9362492272457842               
train rec: 0.9086288743323182 cross rec: 0.9573330997021202               
train f1: 0.880280921111712 cross f1: 0.9416488835879069               
train roc_auc_prob: 0.8128680601414295 cross roc_auc_prob: 0.6851364790613272
fold: 7 with hyper params: {'C': 0.1}               
train acc: 0.9084846295891219 cross acc: 0.9627370707661459               
train prec: 0.8979061744656339 cross prec: 0.9500701528549425               
train rec: 0.9084846295891219 cross rec: 0.9627370707661459               
train f1: 0.8799936177062774 cross f1: 0.9455001566515405               
train roc_auc_prob: 0.8121126905241023 cross roc_auc_prob: 0.8836745622989359
fold: 8 with hyper params: {'C': 0.1}               
train acc: 0.9101635738970923 cross acc: 0.8918893038398927               
train prec: 0.8998347871522125 cross prec: 0.8885245751754145               
train rec: 0.9101635738970923 cross rec: 0.8918893038398927               
train f1: 0.8827125804871643 cross f1: 0.8901799708815445               
train roc_auc_prob: 0.820425432433387 cross roc_auc_prob: 0.7039361135091519
fold: 9 with hyper params: {'C': 0.1}               
train acc: 0.9084430228803454 cross acc: 0.9605752961082911               
train prec: 0.8975200906299534 cross prec: 0.959377996397776               
train rec: 0.9084430228803454 cross rec: 0.9605752961082911               
train f1: 0.8798821562602716 cross f1: 0.9451830055003723               
train roc_auc_prob: 0.810783025792916 cross roc_auc_prob: 0.8150083947733411
fold: 10 with hyper params: {'C': 0.1}               
train acc: 0.9076743914036238 cross acc: 0.9908822287885184               
train prec: 0.8967928238713512 cross prec: 0.9902747776048276               
train rec: 0.9076743914036238 cross rec: 0.9908822287885184               
train f1: 0.879002293710117 cross f1: 0.987241818355593               
train roc_auc_prob: 0.8119278371197818 cross roc_auc_prob: 0.776258175110399
fold: 11 with hyper params: {'C': 0.1}               
train acc: 0.9124707930544739 cross acc: 0.8250959888897966               
train prec: 0.9016359399937612 cross prec: 0.8533771455320328               
train rec: 0.9124707930544739 cross rec: 0.8250959888897966               
train f1: 0.8852927256117222 cross f1: 0.7559848582269917               
train roc_auc_prob: 0.8125531886938662 cross roc_auc_prob: 0.7664760405316371
fold: 12 with hyper params: {'C': 0.1}               
train acc: 0.9125094722494876 cross acc: 0.8257390718436789               
train prec: 0.9019502165009314 cross prec: 0.8189794144162107               
train rec: 0.9125094722494876 cross rec: 0.8257390718436789               
train f1: 0.8847395482797639 cross f1: 0.7820204540300129               
train roc_auc_prob: 0.8109414644239602 cross roc_auc_prob: 0.8266828303813822
fold: 13 with hyper params: {'C': 0.1}               
train acc: 0.9088310021945122 cross acc: 0.9480087598345365               
train prec: 0.8980702304658494 cross prec: 0.9507221929029344               
train rec: 0.9088310021945122 cross rec: 0.9480087598345365               
train f1: 0.8804278177068512 cross f1: 0.9260342840680598               
train roc_auc_prob: 0.8117705068183078 cross roc_auc_prob: 0.7727149229967057
fold: 14 with hyper params: {'C': 0.1}               
train acc: 0.9074591287623023 cross acc: 0.9814749345966203               
train prec: 0.8965462046279863 cross prec: 0.9808991327724745               
train rec: 0.9074591287623023 cross rec: 0.9814749345966203               
train f1: 0.8786490146248227 cross f1: 0.9744902237476722               
train roc_auc_prob: 0.8094853968065244 cross roc_auc_prob: 0.8571725309696762
fold: 15 with hyper params: {'C': 0.1}               
train acc: 0.9097654477699441 cross acc: 0.9143303397110504               
train prec: 0.8987444337319137 cross prec: 0.9125336688663463               
train rec: 0.9097654477699441 cross rec: 0.9143303397110504               
train f1: 0.8815407360270662 cross f1: 0.8849699386487136               
train roc_auc_prob: 0.8114790252958423 cross roc_auc_prob: 0.8313268374395171
fold: 16 with hyper params: {'C': 0.1}               
train acc: 0.9120134889012463 cross acc: 0.8509274101213647               
train prec: 0.9005380399662316 cross prec: 0.8463864211427293               
train rec: 0.9120134889012463 cross rec: 0.8509274101213647               
train f1: 0.8834020916592553 cross f1: 0.8288504764649867               
train roc_auc_prob: 0.8049640915003438 cross roc_auc_prob: 0.8557216087834383
fold: 17 with hyper params: {'C': 0.1}               
train acc: 0.9151518237952766 cross acc: 0.7377842283502661               
train prec: 0.9043569813317428 cross prec: 0.8045801657067415               
train rec: 0.9151518237952766 cross rec: 0.7377842283502661               
train f1: 0.8889186512380814 cross f1: 0.6400502058217904               
train roc_auc_prob: 0.8138133934933612 cross roc_auc_prob: 0.7682628448070032
fold: 18 with hyper params: {'C': 0.1}               
train acc: 0.9073309866155521 cross acc: 0.9922692487896299               
train prec: 0.8963517779616134 cross prec: 0.9923290974266197               
train rec: 0.9073309866155521 cross rec: 0.9922692487896299               
train f1: 0.878481492817777 cross f1: 0.9894496603125653               
train roc_auc_prob: 0.8083760328482997 cross roc_auc_prob: 0.8154515688600328
fold: 19 with hyper params: {'C': 0.1}               
train acc: 0.9089098332506582 cross acc: 0.9501746797537848               
train prec: 0.8980877553654429 cross prec: 0.93279636801073               
train rec: 0.9089098332506582 cross rec: 0.9501746797537848               
train f1: 0.880782726295828 cross f1: 0.9299216453354467               
train roc_auc_prob: 0.8098315958912026 cross roc_auc_prob: 0.556601223425635
fold: 20 with hyper params: {'C': 0.1}               
train acc: 0.9100605893058723 cross acc: 0.9249170353982301               
train prec: 0.8991606305771118 cross prec: 0.9171485580357078               
train rec: 0.9100605893058723 cross rec: 0.9249170353982301               
train f1: 0.8825248667090018 cross f1: 0.9198851706705149               
train roc_auc_prob: 0.8132177899946693 cross roc_auc_prob: 0.8015719501635423
fold: 21 with hyper params: {'C': 0.1}               
train acc: 0.9082325544739712 cross acc: 0.9739255987769662               
train prec: 0.8974479098118467 cross prec: 0.9485310719530721               
train rec: 0.9082325544739712 cross rec: 0.9739255987769662               
train f1: 0.8798086832470934 cross f1: 0.9610606119509032               
train roc_auc_prob: 0.8113258249170066 cross roc_auc_prob: 0.8680198013333262
fold: 22 with hyper params: {'C': 0.1}               
train acc: 0.912721033631784 cross acc: 0.8277816600667753               
train prec: 0.902284642852567 cross prec: 0.845056332702319               
train rec: 0.912721033631784 cross rec: 0.8277816600667753               
train f1: 0.8851565882730847 cross f1: 0.7719028599486347               
train roc_auc_prob: 0.8108428231480296 cross roc_auc_prob: 0.733475066025339
fold: 23 with hyper params: {'C': 0.1}               
train acc: 0.9092358763234358 cross acc: 0.9337000579038796               
train prec: 0.8985830449283125 cross prec: 0.9325812027556153               
train rec: 0.9092358763234358 cross rec: 0.9337000579038796               
train f1: 0.8809808026355151 cross f1: 0.9068806583299907               
train roc_auc_prob: 0.8109232488212748 cross roc_auc_prob: 0.8582150101419879
fold: 24 with hyper params: {'C': 0.1}               
train acc: 0.9118693991535508 cross acc: 0.8442830069488313               
train prec: 0.9010166635374722 cross prec: 0.8686018404244712               
train rec: 0.9118693991535508 cross rec: 0.8442830069488313               
train f1: 0.8846153992288891 cross f1: 0.7758351703565787               
train roc_auc_prob: 0.8134577120639935 cross roc_auc_prob: 0.7770355315673556
fold: 25 with hyper params: {'C': 0.1}               
train acc: 0.9083125892882065 cross acc: 0.9717340097843812               
train prec: 0.8975028096978742 cross prec: 0.9725340632133274               
train rec: 0.9083125892882065 cross rec: 0.9717340097843812               
train f1: 0.8797732995166953 cross f1: 0.9590436357160966               
train roc_auc_prob: 0.8114841807794733 cross roc_auc_prob: 0.8419401929537698
fold: 26 with hyper params: {'C': 0.1}               
train acc: 0.9098862864040738 cross acc: 0.9128945663241692               
train prec: 0.8987552337988511 cross prec: 0.9098675618079278               
train rec: 0.9098862864040738 cross rec: 0.9128945663241692               
train f1: 0.8811139585614636 cross f1: 0.8960928208904859               
train roc_auc_prob: 0.8095378941163857 cross roc_auc_prob: 0.8457743589856394
fold: 27 with hyper params: {'C': 0.1}               
train acc: 0.9077021070341902 cross acc: 0.9951201879631303               
train prec: 0.8966120550864956 cross prec: 0.9942107160749702               
train rec: 0.9077021070341902 cross rec: 0.9951201879631303               
train f1: 0.8791312146038217 cross f1: 0.9935845046034947               
train roc_auc_prob: 0.8110465332331325 cross roc_auc_prob: 0.9001228292351168
fold: 28 with hyper params: {'C': 0.1}               
train acc: 0.9182956343070935 cross acc: 0.668595399971069               
train prec: 0.9055969427310079 cross prec: 0.7549156627176092               
train rec: 0.9182956343070935 cross rec: 0.668595399971069               
train f1: 0.8902208192720761 cross f1: 0.6216609351804708               
train roc_auc_prob: 0.8032731612312738 cross roc_auc_prob: 0.8090490846722662
fold: 29 with hyper params: {'C': 0.1}               
train acc: 0.9079819035824092 cross acc: 0.9772268907563025               
train prec: 0.8968464935944908 cross prec: 0.9725280601920572               
train rec: 0.9079819035824092 cross rec: 0.9772268907563025               
train f1: 0.8794166057477624 cross f1: 0.9713206544762161               
train roc_auc_prob: 0.8107022809881665 cross roc_auc_prob: 0.879696943708693
fold: 30 with hyper params: {'C': 0.1}               
train acc: 0.9090614597814026 cross acc: 0.9375477755694848               
train prec: 0.8980516909212877 cross prec: 0.9390425627511529               
train rec: 0.9090614597814026 cross rec: 0.9375477755694848               
train f1: 0.8806933595780359 cross f1: 0.9149347334582805               
train roc_auc_prob: 0.8113019654662414 cross roc_auc_prob: 0.8121866999300957
fold: 31 with hyper params: {'C': 0.1}               
train acc: 0.9075507035345719 cross acc: 0.9958670570001722               
train prec: 0.8965271966767911 cross prec: 0.9929498966712322               
train rec: 0.9075507035345719 cross rec: 0.9958670570001722               
train f1: 0.8789217158269728 cross f1: 0.9944063374169968               
train roc_auc_prob: 0.8084865228319333 cross roc_auc_prob: 0.9449496620603466
fold: 32 with hyper params: {'C': 0.1}               
train acc: 0.9073758966416694 cross acc: 0.9944227551589515               
train prec: 0.8963715703662358 cross prec: 0.9944538732156128               
train rec: 0.9073758966416694 cross rec: 0.9944227551589515               
train f1: 0.8786462889541816 cross f1: 0.9919905118186458               
train roc_auc_prob: 0.8100545386264737 cross roc_auc_prob: 0.8424452281714225
fold: 0 with hyper params: {'C': 1}               
train acc: 0.9115779350630187 cross acc: 0.9500372856077554               
train prec: 0.9021624103812937 cross prec: 0.9502618505303069               
train rec: 0.9115779350630187 cross rec: 0.9500372856077554               
train f1: 0.8861309401429285 cross f1: 0.9288939586960436               
train roc_auc_prob: 0.8368335146762755 cross roc_auc_prob: 0.7858116832119264
fold: 1 with hyper params: {'C': 1}               
train acc: 0.9239444354051416 cross acc: 0.5096325214291776               
train prec: 0.9146804089172981 cross prec: 0.7508502373479765               
train rec: 0.9239444354051416 cross rec: 0.5096325214291776               
train f1: 0.9030280286025283 cross f1: 0.347523880959501               
train roc_auc_prob: 0.8424178198300588 cross roc_auc_prob: 0.8140418719268601
fold: 2 with hyper params: {'C': 1}               
train acc: 0.9103955794914887 cross acc: 0.9907347728005156               
train prec: 0.9007860953827492 cross prec: 0.9895805144853211               
train rec: 0.9103955794914887 cross rec: 0.9907347728005156               
train f1: 0.8847449298765925 cross f1: 0.9877138321153989               
train roc_auc_prob: 0.8345407548735522 cross roc_auc_prob: 0.755638215392227
fold: 3 with hyper params: {'C': 1}               
train acc: 0.9120429160739325 cross acc: 0.9352718909150649               
train prec: 0.9026381200531944 cross prec: 0.9247361131073263               
train rec: 0.9120429160739325 cross rec: 0.9352718909150649               
train f1: 0.8868445329230868 cross f1: 0.9069451579819104               
train roc_auc_prob: 0.837187599226837 cross roc_auc_prob: 0.8117545845054835
fold: 4 with hyper params: {'C': 1}               
train acc: 0.9118891461068745 cross acc: 0.9396551724137931               
train prec: 0.9024705916278157 cross prec: 0.9288881351743               
train rec: 0.9118891461068745 cross rec: 0.9396551724137931               
train f1: 0.8865758236165149 cross f1: 0.9130572152676445               
train roc_auc_prob: 0.8380603914735558 cross roc_auc_prob: 0.8352278982062411
fold: 5 with hyper params: {'C': 1}               
train acc: 0.9106353506752766 cross acc: 0.9740301637228809               
train prec: 0.9016033152790849 cross prec: 0.97336831082192               
train rec: 0.9106353506752766 cross rec: 0.9740301637228809               
train f1: 0.8847943016331107 cross f1: 0.9736973334491077               
train roc_auc_prob: 0.8421747843275278 cross roc_auc_prob: 0.7221367554790469
fold: 6 with hyper params: {'C': 1}               
train acc: 0.9113008714469677 cross acc: 0.9595233923252147               
train prec: 0.9016416771589932 cross prec: 0.9451438707655794               
train rec: 0.9113008714469677 cross rec: 0.9595233923252147               
train f1: 0.8858860787008576 cross f1: 0.9436341283361137               
train roc_auc_prob: 0.8377639610636435 cross roc_auc_prob: 0.683558712344432
fold: 7 with hyper params: {'C': 1}               
train acc: 0.9113180960326382 cross acc: 0.9628206199348317               
train prec: 0.9020241136487022 cross prec: 0.951178285135267               
train rec: 0.9113180960326382 cross rec: 0.9628206199348317               
train f1: 0.8858617032602105 cross f1: 0.9456996179076786               
train roc_auc_prob: 0.8373406584775507 cross roc_auc_prob: 0.823024738616679
fold: 8 with hyper params: {'C': 1}               
train acc: 0.9130570279008017 cross acc: 0.8822177535191037               
train prec: 0.9040282084024773 cross prec: 0.8912884257212704               
train rec: 0.9130570279008017 cross rec: 0.8822177535191037               
train f1: 0.8885956860984968 cross f1: 0.8865795798434665               
train roc_auc_prob: 0.8448044486637848 cross roc_auc_prob: 0.7247999183696485
fold: 9 with hyper params: {'C': 1}               
train acc: 0.9113237242154347 cross acc: 0.9609137055837563               
train prec: 0.901916669137026 cross prec: 0.959365180382015               
train rec: 0.9113237242154347 cross rec: 0.9609137055837563               
train f1: 0.8857697226282938 cross f1: 0.9460138620248415               
train roc_auc_prob: 0.8368475891068237 cross roc_auc_prob: 0.7815679117840288
fold: 10 with hyper params: {'C': 1}               
train acc: 0.9104535676690009 cross acc: 0.9911355002110596               
train prec: 0.9009591283097343 cross prec: 0.9906684737065587               
train rec: 0.9104535676690009 cross rec: 0.9911355002110596               
train f1: 0.8847106803758047 cross f1: 0.9877732387959525               
train roc_auc_prob: 0.8357213750559749 cross roc_auc_prob: 0.7519732777982419
fold: 11 with hyper params: {'C': 1}               
train acc: 0.9150542396522147 cross acc: 0.8313863246466792               
train prec: 0.9055887606231914 cross prec: 0.8570658904330155               
train rec: 0.9150542396522147 cross rec: 0.8313863246466792               
train f1: 0.8905795631775912 cross f1: 0.7696556550448598               
train roc_auc_prob: 0.838779328233002 cross roc_auc_prob: 0.7441249746659613
fold: 12 with hyper params: {'C': 1}               
train acc: 0.9153219039094326 cross acc: 0.835509138381201               
train prec: 0.9066929301885224 cross prec: 0.8301902909299578               
train rec: 0.9153219039094326 cross rec: 0.835509138381201               
train f1: 0.890275483142566 cross f1: 0.8001627362375154               
train roc_auc_prob: 0.8309169670887582 cross roc_auc_prob: 0.839932653613485
fold: 13 with hyper params: {'C': 1}               
train acc: 0.9116772488658292 cross acc: 0.9480898694135778               
train prec: 0.9023436375580125 cross prec: 0.9487865627907404               
train rec: 0.9116772488658292 cross rec: 0.9480898694135778               
train f1: 0.886265696802996 cross f1: 0.9264727103283246               
train roc_auc_prob: 0.8397862271139981 cross roc_auc_prob: 0.6998956674784194
fold: 14 with hyper params: {'C': 1}               
train acc: 0.9102853313690585 cross acc: 0.9816870536661246               
train prec: 0.9007627961889835 cross prec: 0.9811770775100153               
train rec: 0.9102853313690585 cross rec: 0.9816870536661246               
train f1: 0.884476820142096 cross f1: 0.974923391093535               
train roc_auc_prob: 0.8348732384050254 cross roc_auc_prob: 0.8413678768532724
fold: 15 with hyper params: {'C': 1}               
train acc: 0.9123780975831077 cross acc: 0.918859820382663               
train prec: 0.9025792100293156 cross prec: 0.9143991032713469               
train rec: 0.9123780975831077 cross rec: 0.918859820382663               
train f1: 0.8870312907564435 cross f1: 0.8953433605701652               
train roc_auc_prob: 0.8368294501091748 cross roc_auc_prob: 0.8544900275037024
fold: 16 with hyper params: {'C': 1}               
train acc: 0.9144776111788504 cross acc: 0.8639798488664987               
train prec: 0.9043276770314089 cross prec: 0.8585138381999009               
train rec: 0.9144776111788504 cross rec: 0.8639798488664987               
train f1: 0.8887179472123005 cross f1: 0.8491874428680509               
train roc_auc_prob: 0.8316008474506862 cross roc_auc_prob: 0.8748571474934512
fold: 17 with hyper params: {'C': 1}               
train acc: 0.9176930422051054 cross acc: 0.7424608934042897               
train prec: 0.9082994991514988 cross prec: 0.8078401155895272               
train rec: 0.9176930422051054 cross rec: 0.7424608934042897               
train f1: 0.8940792839154071 cross f1: 0.6500326207053363               
train roc_auc_prob: 0.8367383072153788 cross roc_auc_prob: 0.7857341481916811
fold: 18 with hyper params: {'C': 1}               
train acc: 0.9101704981380043 cross acc: 0.9924254255817585               
train prec: 0.9006634688506608 cross prec: 0.9924828895046875               
train rec: 0.9101704981380043 cross rec: 0.9924254255817585               
train f1: 0.8842969003929605 cross f1: 0.9897583363413966               
train roc_auc_prob: 0.8343627393980625 cross roc_auc_prob: 0.7449281395218822
fold: 19 with hyper params: {'C': 1}               
train acc: 0.9116444715788402 cross acc: 0.9498419564132424               
train prec: 0.9021093389307887 cross prec: 0.9310540886655446               
train rec: 0.9116444715788402 cross rec: 0.9498419564132424               
train f1: 0.8864253896267139 cross f1: 0.9310423139843135               
train roc_auc_prob: 0.837339786620164 cross roc_auc_prob: 0.6131686031198292
fold: 20 with hyper params: {'C': 1}               
train acc: 0.9136493601902407 cross acc: 0.9246404867256637               
train prec: 0.9046836901711464 cross prec: 0.9176215456885356               
train rec: 0.9136493601902407 cross rec: 0.9246404867256637               
train f1: 0.8896403223088698 cross f1: 0.9202531672456314               
train roc_auc_prob: 0.8422795408417475 cross roc_auc_prob: 0.7567390698347842
fold: 21 with hyper params: {'C': 1}               
train acc: 0.9110061446080239 cross acc: 0.9738406658739596               
train prec: 0.90154585769419 cross prec: 0.9572988062959841               
train rec: 0.9110061446080239 cross rec: 0.9738406658739596               
train f1: 0.8855213048642866 cross f1: 0.9611852621342615               
train roc_auc_prob: 0.8372362101216134 cross roc_auc_prob: 0.7678888207457798
fold: 22 with hyper params: {'C': 1}               
train acc: 0.9154259550080561 cross acc: 0.8329062815435981               
train prec: 0.9063852731566302 cross prec: 0.8464379091819               
train rec: 0.9154259550080561 cross rec: 0.8329062815435981               
train f1: 0.8907253293153153 cross f1: 0.7828488784092706               
train roc_auc_prob: 0.8359671600343578 cross roc_auc_prob: 0.752620756413664
fold: 23 with hyper params: {'C': 1}               
train acc: 0.9121082719371548 cross acc: 0.9350752750434279               
train prec: 0.9028823081610272 cross prec: 0.9316032464285184               
train rec: 0.9121082719371548 cross rec: 0.9350752750434279               
train f1: 0.8868611685277394 cross f1: 0.9105631169068283               
train roc_auc_prob: 0.8379105481459824 cross roc_auc_prob: 0.8137667010793352
fold: 24 with hyper params: {'C': 1}               
train acc: 0.9144734662885419 cross acc: 0.8473626026531902               
train prec: 0.9050872149245278 cross prec: 0.8708014403614348               
train rec: 0.9144734662885419 cross rec: 0.8473626026531902               
train f1: 0.8898697035954938 cross f1: 0.7829546963551868               
train roc_auc_prob: 0.8421092214431267 cross roc_auc_prob: 0.7270175809046994
fold: 25 with hyper params: {'C': 1}               
train acc: 0.910982037052544 cross acc: 0.9729117593766987               
train prec: 0.9020952599276749 cross prec: 0.9736473982455786               
train rec: 0.910982037052544 cross rec: 0.9729117593766987               
train f1: 0.8849138896051747 cross f1: 0.9616906176764761               
train roc_auc_prob: 0.8387269366764443 cross roc_auc_prob: 0.8634756860605448
fold: 26 with hyper params: {'C': 1}               
train acc: 0.9128026698856778 cross acc: 0.9118763306488938               
train prec: 0.9034004962389738 cross prec: 0.9090036783822967               
train rec: 0.9128026698856778 cross rec: 0.9118763306488938               
train f1: 0.8871001505142088 cross f1: 0.8943431389653652               
train roc_auc_prob: 0.8384188518507545 cross roc_auc_prob: 0.7896795238508987
fold: 27 with hyper params: {'C': 1}               
train acc: 0.910465638568648 cross acc: 0.9950298210735586               
train prec: 0.9008084027837293 cross prec: 0.9940288119773588               
train rec: 0.910465638568648 cross rec: 0.9950298210735586               
train f1: 0.8847887637793549 cross f1: 0.9934117499593572               
train roc_auc_prob: 0.835552363204862 cross roc_auc_prob: 0.8897746860379216
fold: 28 with hyper params: {'C': 1}               
train acc: 0.92058032052052 cross acc: 0.6888470996672935               
train prec: 0.9098201463482622 cross prec: 0.7563360057810871               
train rec: 0.92058032052052 cross rec: 0.6888470996672935               
train f1: 0.8951961641932771 cross f1: 0.6538078968227904               
train roc_auc_prob: 0.8270189374659179 cross roc_auc_prob: 0.8077865760257559
fold: 29 with hyper params: {'C': 1}               
train acc: 0.9108809924038787 cross acc: 0.9791596638655462               
train prec: 0.9015465583934237 cross prec: 0.9762782706791144               
train rec: 0.9108809924038787 cross rec: 0.9791596638655462               
train f1: 0.8851720373919529 cross f1: 0.9740182375184772               
train roc_auc_prob: 0.8362320938268231 cross roc_auc_prob: 0.8650499009896635
fold: 30 with hyper params: {'C': 1}               
train acc: 0.9117881875805074 cross acc: 0.9400703256382816               
train prec: 0.9021036478035356 cross prec: 0.9418230337943019               
train rec: 0.9117881875805074 cross rec: 0.9400703256382816               
train f1: 0.8863641115356553 cross f1: 0.9199113190497507               
train roc_auc_prob: 0.8375767277578674 cross roc_auc_prob: 0.7782432392651406
fold: 31 with hyper params: {'C': 1}               
train acc: 0.9105213804880652 cross acc: 0.9955226450835198               
train prec: 0.9007793214980427 cross prec: 0.9929486832329204               
train rec: 0.9105213804880652 cross rec: 0.9955226450835198               
train f1: 0.8851407321019922 cross f1: 0.9942339982353794               
train roc_auc_prob: 0.8341128734143158 cross roc_auc_prob: 0.894906774177912
fold: 32 with hyper params: {'C': 1}               
train acc: 0.9101779018584937 cross acc: 0.9944227551589515               
train prec: 0.9006160446345199 cross prec: 0.9944538732156128               
train rec: 0.9101779018584937 cross rec: 0.9944227551589515               
train f1: 0.8843816965862361 cross f1: 0.9919905118186458               
train roc_auc_prob: 0.8349819216665435 cross roc_auc_prob: 0.8299679384418082
fold: 0 with hyper params: {'C': 10}               
train acc: 0.9127533341135405 cross acc: 0.952357278979203               
train prec: 0.9034868793548712 cross prec: 0.9533027555643295               
train rec: 0.9127533341135405 cross rec: 0.952357278979203               
train f1: 0.8886777247769464 cross f1: 0.9339142182912923               
train roc_auc_prob: 0.8399901841054396 cross roc_auc_prob: 0.7744674705774585
fold: 1 with hyper params: {'C': 10}               
train acc: 0.9249613569801497 cross acc: 0.5123482983959943               
train prec: 0.9158919287070583 cross prec: 0.7515532392715805               
train rec: 0.9249613569801497 cross rec: 0.5123482983959943               
train f1: 0.9051855024436043 cross f1: 0.3534754391977853               
train roc_auc_prob: 0.8410688700885124 cross roc_auc_prob: 0.8073443000099124
fold: 2 with hyper params: {'C': 10}               
train acc: 0.9116229326610901 cross acc: 0.9907347728005156               
train prec: 0.9023726213748507 cross prec: 0.9895805144853211               
train rec: 0.9116229326610901 cross rec: 0.9907347728005156               
train f1: 0.8872694381780437 cross f1: 0.9877138321153989               
train roc_auc_prob: 0.8371647467521404 cross roc_auc_prob: 0.7439795582827269
fold: 3 with hyper params: {'C': 10}               
train acc: 0.9132822500400811 cross acc: 0.9352718909150649               
train prec: 0.9040892811606749 cross prec: 0.9239304349360405               
train rec: 0.9132822500400811 cross rec: 0.9352718909150649               
train f1: 0.8894858708073379 cross f1: 0.9070877906217056               
train roc_auc_prob: 0.8403598657780083 cross roc_auc_prob: 0.7867476372550993
fold: 4 with hyper params: {'C': 10}               
train acc: 0.9129919929255171 cross acc: 0.9399137931034482               
train prec: 0.9040959075093205 cross prec: 0.9303656586831108               
train rec: 0.9129919929255171 cross rec: 0.9399137931034482               
train f1: 0.8887154577879032 cross f1: 0.9136547885693567               
train roc_auc_prob: 0.8423055526808666 cross roc_auc_prob: 0.848582315438035
fold: 5 with hyper params: {'C': 10}               
train acc: 0.9119874315047565 cross acc: 0.9737075570610533               
train prec: 0.9027296611750297 cross prec: 0.9742247568008614               
train rec: 0.9119874315047565 cross rec: 0.9737075570610533               
train f1: 0.8879530222736057 cross f1: 0.9739648624654451               
train roc_auc_prob: 0.8394710182889273 cross roc_auc_prob: 0.7231434973213179
fold: 6 with hyper params: {'C': 10}               
train acc: 0.9125225735497685 cross acc: 0.9599614508498335               
train prec: 0.9031831175936247 cross prec: 0.9476998586026483               
train rec: 0.9125225735497685 cross rec: 0.9599614508498335               
train f1: 0.8884322372760765 cross f1: 0.9438979516001574               
train roc_auc_prob: 0.8409333063918343 cross roc_auc_prob: 0.6807530577874837
fold: 7 with hyper params: {'C': 10}               
train acc: 0.912495739626308 cross acc: 0.9629877182722032               
train prec: 0.9033623506396778 cross prec: 0.9530081163302754               
train rec: 0.912495739626308 cross rec: 0.9629877182722032               
train f1: 0.888392328870911 cross f1: 0.9460964959952686               
train roc_auc_prob: 0.8404086007416509 cross roc_auc_prob: 0.7983195681761941
fold: 8 with hyper params: {'C': 10}               
train acc: 0.9143796049497827 cross acc: 0.8798238054198985               
train prec: 0.9055946492399732 cross prec: 0.893681882158498               
train rec: 0.9143796049497827 cross rec: 0.8798238054198985               
train f1: 0.8913456843897061 cross f1: 0.8863479068355788               
train roc_auc_prob: 0.8468207044516373 cross roc_auc_prob: 0.7344541842939445
fold: 9 with hyper params: {'C': 10}               
train acc: 0.9125975383097682 cross acc: 0.96082910321489               
train prec: 0.9033653890517448 cross prec: 0.9587352638520187               
train rec: 0.9125975383097682 cross rec: 0.96082910321489               
train f1: 0.8885182166958491 cross f1: 0.9459564550334467               
train roc_auc_prob: 0.8398556444388801 cross roc_auc_prob: 0.7649688470176012
fold: 10 with hyper params: {'C': 10}               
train acc: 0.9109265107480129 cross acc: 0.9911355002110596               
train prec: 0.9026635402135669 cross prec: 0.9906684737065587               
train rec: 0.9109265107480129 cross rec: 0.9911355002110596               
train f1: 0.8850369393195421 cross f1: 0.9877732387959525               
train roc_auc_prob: 0.8371865606950708 cross roc_auc_prob: 0.7418306227488436
fold: 11 with hyper params: {'C': 10}               
train acc: 0.9161919742622542 cross acc: 0.8335103341230292               
train prec: 0.9070925362011919 cross prec: 0.8543176796391158               
train rec: 0.9161919742622542 cross rec: 0.8335103341230292               
train f1: 0.8929141937557564 cross f1: 0.7748226261776336               
train roc_auc_prob: 0.8418085318027934 cross roc_auc_prob: 0.7296854112658969
fold: 12 with hyper params: {'C': 10}               
train acc: 0.9162195425857079 cross acc: 0.8393834751115977               
train prec: 0.9069823241343247 cross prec: 0.8327070313628623               
train rec: 0.9162195425857079 cross rec: 0.8393834751115977               
train f1: 0.8926755186112917 cross f1: 0.8081669694863094               
train roc_auc_prob: 0.8371001790312284 cross roc_auc_prob: 0.8465362657890831
fold: 13 with hyper params: {'C': 10}               
train acc: 0.9130876430124389 cross acc: 0.9480898694135778               
train prec: 0.9040442519770906 cross prec: 0.9456283454106659               
train rec: 0.9130876430124389 cross rec: 0.9480898694135778               
train f1: 0.8892321982609025 cross f1: 0.9269804323077524               
train roc_auc_prob: 0.8428797088682825 cross roc_auc_prob: 0.6685694408096483
fold: 14 with hyper params: {'C': 10}               
train acc: 0.9113518711763141 cross acc: 0.9818991727356289               
train prec: 0.9020347720225633 cross prec: 0.9814455220816948               
train rec: 0.9113518711763141 cross rec: 0.9818991727356289               
train f1: 0.886746463011939 cross f1: 0.9753508413617005               
train roc_auc_prob: 0.8378506454096741 cross roc_auc_prob: 0.8196480206311003
fold: 15 with hyper params: {'C': 10}               
train acc: 0.9135353102808408 cross acc: 0.9209683717297931               
train prec: 0.9040926463773936 cross prec: 0.9155446194438506               
train rec: 0.9135353102808408 cross rec: 0.9209683717297931               
train f1: 0.8894362088113987 cross f1: 0.900027314521718               
train roc_auc_prob: 0.8398487199290081 cross roc_auc_prob: 0.8613900985841065
fold: 16 with hyper params: {'C': 10}               
train acc: 0.9156356976322999 cross acc: 0.8685596519349668               
train prec: 0.9056975107567836 cross prec: 0.8626788706018707               
train rec: 0.9156356976322999 cross rec: 0.8685596519349668               
train f1: 0.8913154460990258 cross f1: 0.8562866971822067               
train roc_auc_prob: 0.8355116650098244 cross roc_auc_prob: 0.880331443694587
fold: 17 with hyper params: {'C': 10}               
train acc: 0.9185867973772996 cross acc: 0.7456861796484437               
train prec: 0.9086642120483449 cross prec: 0.8089924657885309               
train rec: 0.9185867973772996 cross rec: 0.7456861796484437               
train f1: 0.8965066032849243 cross f1: 0.6568876000887461               
train roc_auc_prob: 0.8264270303478614 cross roc_auc_prob: 0.7720716459958203
fold: 18 with hyper params: {'C': 10}               
train acc: 0.9114882965734692 cross acc: 0.9924254255817585               
train prec: 0.9022653209966204 cross prec: 0.9924828895046875               
train rec: 0.9114882965734692 cross rec: 0.9924254255817585               
train f1: 0.8870734475540192 cross f1: 0.9897583363413966               
train roc_auc_prob: 0.8373942618965509 cross roc_auc_prob: 0.7174071626747173
fold: 19 with hyper params: {'C': 10}               
train acc: 0.9127942916014805 cross acc: 0.9510896689402761               
train prec: 0.9034214958271569 cross prec: 0.9360544846422448               
train rec: 0.9127942916014805 cross rec: 0.9510896689402761               
train f1: 0.8889051595540839 cross f1: 0.933671056455899               
train roc_auc_prob: 0.8411168791274718 cross roc_auc_prob: 0.635520764103419
fold: 20 with hyper params: {'C': 10}               
train acc: 0.9153746323557644 cross acc: 0.9246404867256637               
train prec: 0.9066569010175011 cross prec: 0.9176215456885356               
train rec: 0.9153746323557644 cross rec: 0.9246404867256637               
train f1: 0.893235211148737 cross f1: 0.9202531672456314               
train roc_auc_prob: 0.8460947917450694 cross roc_auc_prob: 0.7353784096389846
fold: 21 with hyper params: {'C': 10}               
train acc: 0.9122442182975598 cross acc: 0.9735858671649397               
train prec: 0.9029795211418271 cross prec: 0.957527841533217               
train rec: 0.9122442182975598 cross rec: 0.9735858671649397               
train f1: 0.8881708509590506 cross f1: 0.9615401608628431               
train roc_auc_prob: 0.8403552836298351 cross roc_auc_prob: 0.7212096231957502
fold: 22 with hyper params: {'C': 10}               
train acc: 0.9164635638677572 cross acc: 0.8370991536609985               
train prec: 0.9075896529183403 cross prec: 0.8502903566625922               
train rec: 0.9164635638677572 cross rec: 0.8370991536609985               
train f1: 0.8929579331914478 cross f1: 0.7907997285108518               
train roc_auc_prob: 0.8398957267986769 cross roc_auc_prob: 0.7560401551144779
fold: 23 with hyper params: {'C': 10}               
train acc: 0.9134499157958759 cross acc: 0.9353647944412276               
train prec: 0.9044278140711532 cross prec: 0.9297092708795535               
train rec: 0.9134499157958759 cross rec: 0.9353647944412276               
train f1: 0.8897236228789122 cross f1: 0.9118684836907175               
train roc_auc_prob: 0.8415659565916687 cross roc_auc_prob: 0.7873221990169466
fold: 24 with hyper params: {'C': 10}               
train acc: 0.9154162303198008 cross acc: 0.8502053063802906               
train prec: 0.9059954915630622 cross prec: 0.8702023914924699               
train rec: 0.9154162303198008 cross rec: 0.8502053063802906               
train f1: 0.8920293141662732 cross f1: 0.7895954335072997               
train roc_auc_prob: 0.8443503694864731 cross roc_auc_prob: 0.70803201641417
fold: 25 with hyper params: {'C': 10}               
train acc: 0.9122837734623397 cross acc: 0.9731835477441565               
train prec: 0.9030029041566965 cross prec: 0.9730904033601608               
train rec: 0.9122837734623397 cross rec: 0.9731835477441565               
train f1: 0.8880730816104567 cross f1: 0.9624150224001653               
train roc_auc_prob: 0.8412696210990649 cross roc_auc_prob: 0.8697983181712932
fold: 26 with hyper params: {'C': 10}               
train acc: 0.9142278938132095 cross acc: 0.9112283624919004               
train prec: 0.9051295605862473 cross prec: 0.9079261564151978               
train rec: 0.9142278938132095 cross rec: 0.9112283624919004               
train f1: 0.8901419084552735 cross f1: 0.8935342178982973               
train roc_auc_prob: 0.8419766906695652 cross roc_auc_prob: 0.7632840662537448
fold: 27 with hyper params: {'C': 10}               
train acc: 0.9117598544387798 cross acc: 0.9950298210735586               
train prec: 0.902391299445852 cross prec: 0.9940288119773588               
train rec: 0.9117598544387798 cross rec: 0.9950298210735586               
train f1: 0.8875158152518526 cross f1: 0.9934117499593572               
train roc_auc_prob: 0.8383209685802651 cross roc_auc_prob: 0.8881594376238552
fold: 28 with hyper params: {'C': 10}               
train acc: 0.921819775569191 cross acc: 0.6928251121076233               
train prec: 0.9117281911726861 cross prec: 0.7497270906421392               
train rec: 0.921819775569191 cross rec: 0.6928251121076233               
train f1: 0.897878452250641 cross f1: 0.6620439901122984               
train roc_auc_prob: 0.830421191932447 cross roc_auc_prob: 0.8061278973087352
fold: 29 with hyper params: {'C': 10}               
train acc: 0.9120482571135756 cross acc: 0.9789075630252101               
train prec: 0.902757692688381 cross prec: 0.9758923398829894               
train rec: 0.9120482571135756 cross rec: 0.9789075630252101               
train f1: 0.8877651726183946 cross f1: 0.9735907438270717               
train roc_auc_prob: 0.8382294998189426 cross roc_auc_prob: 0.8457684488527225
fold: 30 with hyper params: {'C': 10}               
train acc: 0.9129997831881544 cross acc: 0.9400703256382816               
train prec: 0.9036249133363733 cross prec: 0.9414787781686306               
train rec: 0.9129997831881544 cross rec: 0.9400703256382816               
train f1: 0.8889068141737633 cross f1: 0.9200016498882683               
train roc_auc_prob: 0.8407278318732939 cross roc_auc_prob: 0.7634117803682205
fold: 31 with hyper params: {'C': 10}               
train acc: 0.911649678663729 cross acc: 0.9955226450835198               
train prec: 0.9023836607391814 cross prec: 0.9929486832329204               
train rec: 0.911649678663729 cross rec: 0.9955226450835198               
train f1: 0.8873615772364838 cross f1: 0.9942339982353794               
train roc_auc_prob: 0.8374959970863086 cross roc_auc_prob: 0.8598356561635261
fold: 32 with hyper params: {'C': 10}               
train acc: 0.9114031423214868 cross acc: 0.9948211297904549               
train prec: 0.9020678755034476 cross prec: 0.9948479718735121               
train rec: 0.9114031423214868 cross rec: 0.9948211297904549               
train f1: 0.8869908880626427 cross f1: 0.9928477014901264               
train roc_auc_prob: 0.837774732761215 cross roc_auc_prob: 0.8277428663033024
fold: 0 with hyper params: {'C': 100}               
train acc: 0.9128856301105689 cross acc: 0.9525229927914491               
train prec: 0.90351175504575 cross prec: 0.9534930668825945               
train rec: 0.9128856301105689 cross rec: 0.9525229927914491               
train f1: 0.8890452495424769 cross f1: 0.9342612007129418               
train roc_auc_prob: 0.8400068745004349 cross roc_auc_prob: 0.7726559626771389
fold: 1 with hyper params: {'C': 100}               
train acc: 0.9260494630654084 cross acc: 0.5137910549096155               
train prec: 0.9174367263205628 cross prec: 0.7519282735488233               
train rec: 0.9260494630654084 cross rec: 0.5137910549096155               
train f1: 0.9072317343680324 cross f1: 0.3566145849895126               
train roc_auc_prob: 0.8432520596136682 cross roc_auc_prob: 0.8047649084022784
fold: 2 with hyper params: {'C': 100}               
train acc: 0.9118750238722738 cross acc: 0.9907347728005156               
train prec: 0.902572538494293 cross prec: 0.9895805144853211               
train rec: 0.9118750238722738 cross rec: 0.9907347728005156               
train f1: 0.887863437952187 cross f1: 0.9877138321153989               
train roc_auc_prob: 0.8371409869710128 cross roc_auc_prob: 0.7420964827334462
fold: 3 with hyper params: {'C': 100}               
train acc: 0.913462933226111 cross acc: 0.9352718909150649               
train prec: 0.9041237009335391 cross prec: 0.923205964213801               
train rec: 0.913462933226111 cross rec: 0.9352718909150649               
train f1: 0.8899872042965311 cross f1: 0.9072297406623558               
train roc_auc_prob: 0.8403442793732209 cross roc_auc_prob: 0.7843115587563361
fold: 4 with hyper params: {'C': 100}               
train acc: 0.9131952826616725 cross acc: 0.9401724137931035               
train prec: 0.9039168593193634 cross prec: 0.9316511942075444               
train rec: 0.9131952826616725 cross rec: 0.9401724137931035               
train f1: 0.8894185815901112 cross f1: 0.9142486158092971               
train roc_auc_prob: 0.8414659831327251 cross roc_auc_prob: 0.8467904660077243
fold: 5 with hyper params: {'C': 100}               
train acc: 0.9123464586176693 cross acc: 0.9741108153883378               
train prec: 0.9032438135085205 cross prec: 0.973784352095942               
train rec: 0.9123464586176693 cross rec: 0.9741108153883378               
train f1: 0.888638513394427 cross f1: 0.9739470840561559               
train roc_auc_prob: 0.8395627456063625 cross roc_auc_prob: 0.7234896168307268
fold: 6 with hyper params: {'C': 100}               
train acc: 0.9127892652769911 cross acc: 0.9601366742596811               
train prec: 0.9034680975453638 cross prec: 0.9491173554729929               
train rec: 0.9127892652769911 cross rec: 0.9601366742596811               
train f1: 0.8890129481350157 cross f1: 0.943862809521547               
train roc_auc_prob: 0.8408958891093685 cross roc_auc_prob: 0.6798251774476181
fold: 7 with hyper params: {'C': 100}               
train acc: 0.9127729818546233 cross acc: 0.9629877182722032               
train prec: 0.9035753150508662 cross prec: 0.9530081163302754               
train rec: 0.9127729818546233 cross rec: 0.9629877182722032               
train f1: 0.889048606423322 cross f1: 0.9460964959952686               
train roc_auc_prob: 0.8403983247494147 cross roc_auc_prob: 0.7951346356099975
fold: 8 with hyper params: {'C': 100}               
train acc: 0.9145493610077936 cross acc: 0.8808771425835488               
train prec: 0.9058907166252127 cross prec: 0.894076911178308               
train rec: 0.9145493610077936 cross rec: 0.8808771425835488               
train f1: 0.8916281319830703 cross f1: 0.8871011023800512               
train roc_auc_prob: 0.8467781424793752 cross roc_auc_prob: 0.7361058563064147
fold: 9 with hyper params: {'C': 100}               
train acc: 0.9128034843010676 cross acc: 0.9609137055837563               
train prec: 0.9034540421408512 cross prec: 0.9588763062599663               
train rec: 0.9128034843010676 cross rec: 0.9609137055837563               
train f1: 0.8890554881452447 cross f1: 0.9461322899994217               
train roc_auc_prob: 0.8397658091079356 cross roc_auc_prob: 0.7619874357509994
fold: 10 with hyper params: {'C': 100}               
train acc: 0.9120402154179443 cross acc: 0.991304347826087               
train prec: 0.9034155124703586 cross prec: 0.9909041744734578               
train rec: 0.9120402154179443 cross rec: 0.991304347826087               
train f1: 0.8876948702803982 cross f1: 0.9881176605378529               
train roc_auc_prob: 0.8386693039009628 cross roc_auc_prob: 0.725582881967387
fold: 11 with hyper params: {'C': 100}               
train acc: 0.9163116018387013 cross acc: 0.8340821828282003               
train prec: 0.9069900570400945 cross prec: 0.8549238427901951               
train rec: 0.9163116018387013 cross rec: 0.8340821828282003               
train f1: 0.8933335566438533 cross f1: 0.7759833112153198               
train roc_auc_prob: 0.8414353697378122 cross roc_auc_prob: 0.7281936159635161
fold: 12 with hyper params: {'C': 100}               
train acc: 0.9163517726456692 cross acc: 0.83988882338078               
train prec: 0.907123589024561 cross prec: 0.8333593159598809               
train rec: 0.9163517726456692 cross rec: 0.83988882338078               
train f1: 0.8929674706898894 cross f1: 0.8089606519181854               
train roc_auc_prob: 0.8370811202402643 cross roc_auc_prob: 0.8467979695697339
fold: 13 with hyper params: {'C': 100}               
train acc: 0.9133855060361814 cross acc: 0.9478465406764539               
train prec: 0.9047766538701206 cross prec: 0.9438092657171404               
train rec: 0.9133855060361814 cross rec: 0.9478465406764539               
train f1: 0.8896001144083614 cross f1: 0.9267011364944479               
train roc_auc_prob: 0.8422532411048709 cross roc_auc_prob: 0.6572328308324826
fold: 14 with hyper params: {'C': 100}               
train acc: 0.9117252879913245 cross acc: 0.9818991727356289               
train prec: 0.9024864394875787 cross prec: 0.9814455220816948               
train rec: 0.9117252879913245 cross rec: 0.9818991727356289               
train f1: 0.8875239514472976 cross f1: 0.9753508413617005               
train roc_auc_prob: 0.8381822364359999 cross roc_auc_prob: 0.815966561717578
fold: 15 with hyper params: {'C': 100}               
train acc: 0.9137545179724818 cross acc: 0.9216712221788363               
train prec: 0.9043440067462863 cross prec: 0.9163973712015409               
train rec: 0.9137545179724818 cross rec: 0.9216712221788363               
train f1: 0.8899071669810733 cross f1: 0.9013193930925726               
train roc_auc_prob: 0.8401221626132307 cross roc_auc_prob: 0.8626332100322858
fold: 16 with hyper params: {'C': 100}               
train acc: 0.9158856810517669 cross acc: 0.8685596519349668               
train prec: 0.905998487510054 cross prec: 0.8624811465809761               
train rec: 0.9158856810517669 cross rec: 0.8685596519349668               
train f1: 0.8918631527087932 cross f1: 0.8565597099354959               
train roc_auc_prob: 0.8356804374174751 cross roc_auc_prob: 0.8809105825668428
fold: 17 with hyper params: {'C': 100}               
train acc: 0.9190196702527214 cross acc: 0.7468956619900016               
train prec: 0.9098993552719165 cross prec: 0.8106904469915638               
train rec: 0.9190196702527214 cross rec: 0.7468956619900016               
train f1: 0.8968989108479926 cross f1: 0.6592685640098704               
train roc_auc_prob: 0.8386868463907476 cross roc_auc_prob: 0.7888505404944673
fold: 18 with hyper params: {'C': 100}               
train acc: 0.9117584834867366 cross acc: 0.9924254255817585               
train prec: 0.9026204770155841 cross prec: 0.9924828895046875               
train rec: 0.9117584834867366 cross rec: 0.9924254255817585               
train f1: 0.8876142534593833 cross f1: 0.9897583363413966               
train roc_auc_prob: 0.8375081379598944 cross roc_auc_prob: 0.7138170014677225
fold: 19 with hyper params: {'C': 100}               
train acc: 0.9130766589079253 cross acc: 0.9514223922808185               
train prec: 0.9037930404840877 cross prec: 0.9371866268688734               
train rec: 0.9130766589079253 cross rec: 0.9514223922808185               
train f1: 0.8894709465091227 cross f1: 0.9343575421573653               
train roc_auc_prob: 0.8413444635008556 cross roc_auc_prob: 0.6386166356733134
fold: 20 with hyper params: {'C': 100}               
train acc: 0.9157278779920289 cross acc: 0.9240182522123894               
train prec: 0.9069751150237164 cross prec: 0.9169369282251384               
train rec: 0.9157278779920289 cross rec: 0.9240182522123894               
train f1: 0.8940184572404966 cross f1: 0.9196119754222533               
train roc_auc_prob: 0.8462647032695969 cross roc_auc_prob: 0.732784366923364
fold: 21 with hyper params: {'C': 100}               
train acc: 0.9125060696117736 cross acc: 0.9735858671649397               
train prec: 0.9033643892479825 cross prec: 0.957527841533217               
train rec: 0.9125060696117736 cross rec: 0.9735858671649397               
train f1: 0.8886667017887566 cross f1: 0.9615401608628431               
train roc_auc_prob: 0.8407102218551038 cross roc_auc_prob: 0.7131550698236464
fold: 22 with hyper params: {'C': 100}               
train acc: 0.9167261528420795 cross acc: 0.8378756114605171               
train prec: 0.9079326151821441 cross prec: 0.8508698916826598               
train rec: 0.9167261528420795 cross rec: 0.8378756114605171               
train f1: 0.8934898909217214 cross f1: 0.7922821628201199               
train roc_auc_prob: 0.8401485707180174 cross roc_auc_prob: 0.7560325324817532
fold: 23 with hyper params: {'C': 100}               
train acc: 0.9136569122769357 cross acc: 0.9353647944412276               
train prec: 0.904567533092018 cross prec: 0.9297092708795535               
train rec: 0.9136569122769357 cross rec: 0.9353647944412276               
train f1: 0.8902251673527027 cross f1: 0.9118684836907175               
train roc_auc_prob: 0.8415632619490228 cross roc_auc_prob: 0.7829072328262076
fold: 24 with hyper params: {'C': 100}               
train acc: 0.9158162680303621 cross acc: 0.8519425142135186               
train prec: 0.9065025565929725 cross prec: 0.8718929863416938               
train rec: 0.9158162680303621 cross rec: 0.8519425142135186               
train f1: 0.8928521861208623 cross f1: 0.7934053310392829               
train roc_auc_prob: 0.8451886143613901 cross roc_auc_prob: 0.6996669375422051
fold: 25 with hyper params: {'C': 100}               
train acc: 0.9129206854289259 cross acc: 0.9734553361116144               
train prec: 0.9037228563540048 cross prec: 0.9734227205742746               
train rec: 0.9129206854289259 cross rec: 0.9734553361116144               
train f1: 0.8894263097880617 cross f1: 0.9629934100613615               
train roc_auc_prob: 0.8411401081492186 cross roc_auc_prob: 0.8687661973729028
fold: 26 with hyper params: {'C': 100}               
train acc: 0.9144459885779207 cross acc: 0.9115060631306119               
train prec: 0.9053171982334364 cross prec: 0.90825194458144               
train rec: 0.9144459885779207 cross rec: 0.9115060631306119               
train f1: 0.8906488208221066 cross f1: 0.8939629570432608               
train roc_auc_prob: 0.8417673914683876 cross roc_auc_prob: 0.7601443367421571
fold: 27 with hyper params: {'C': 100}               
train acc: 0.9118537485313187 cross acc: 0.9950298210735586               
train prec: 0.9023371110013686 cross prec: 0.9940288119773588               
train rec: 0.9118537485313187 cross rec: 0.9950298210735586               
train f1: 0.88782664363493 cross f1: 0.9934117499593572               
train roc_auc_prob: 0.8382552505836238 cross roc_auc_prob: 0.8891576083770124
fold: 28 with hyper params: {'C': 100}               
train acc: 0.9219628879047076 cross acc: 0.6939823520902647               
train prec: 0.9119371020673063 cross prec: 0.7490935402466357               
train rec: 0.9219628879047076 cross rec: 0.6939823520902647               
train f1: 0.8981863061068356 cross f1: 0.6640523682595078               
train roc_auc_prob: 0.8306051352135027 cross roc_auc_prob: 0.8059101841253191
fold: 29 with hyper params: {'C': 100}               
train acc: 0.9122923909090678 cross acc: 0.9788235294117648               
train prec: 0.9030571155699857 cross prec: 0.975761814556331               
train rec: 0.9122923909090678 cross rec: 0.9788235294117648               
train f1: 0.888270067519503 cross f1: 0.9734472741796029               
train roc_auc_prob: 0.8382333534200906 cross roc_auc_prob: 0.8426791174501183
fold: 30 with hyper params: {'C': 100}               
train acc: 0.9132421023096838 cross acc: 0.939993884727106               
train prec: 0.9038970725314124 cross prec: 0.9410586622652469               
train rec: 0.9132421023096838 cross rec: 0.939993884727106               
train f1: 0.8894279109696627 cross f1: 0.9199447079101103               
train roc_auc_prob: 0.8409044215806725 cross roc_auc_prob: 0.7608247549509594
fold: 31 with hyper params: {'C': 100}               
train acc: 0.9115022883615027 cross acc: 0.9955226450835198               
train prec: 0.9022239391965868 cross prec: 0.9929486832329204               
train rec: 0.9115022883615027 cross rec: 0.9955226450835198               
train f1: 0.8870424749455512 cross f1: 0.9942339982353794               
train roc_auc_prob: 0.8392919720259379 cross roc_auc_prob: 0.846486671036243
fold: 32 with hyper params: {'C': 100}               
train acc: 0.911543242582328 cross acc: 0.9948211297904549               
train prec: 0.9021893000933324 cross prec: 0.9948479718735121               
train rec: 0.911543242582328 cross rec: 0.9948211297904549               
train f1: 0.8873149034570437 cross f1: 0.9928477014901264               
train roc_auc_prob: 0.8381267528167478 cross roc_auc_prob: 0.8270813294859464
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
 This problem is unconstrained.
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41     75     87      1     0     0   5.572D-05   2.627D-01
  F =  0.26271520359975126     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.35892D+00

At iterate   50    f=  2.79163D-01    |proj g|=  6.14048D-03

At iterate  100    f=  2.66787D-01    |proj g|=  1.44096D-03

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    114      1     0     0   1.441D-03   2.668D-01
  F =  0.26678664110476674     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           41     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.36229D+00

At iterate   50    f=  2.74162D-01    |proj g|=  4.07893D-03

At iterate  100    f=  2.67003D-01    |proj g|=  6.92839D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   41    100    117      1     0     0   6.928D-04   2.670D-01
  F =  0.26700259102582519     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
Finished on Fri Oct 18 08:28:25 PST 2024
Total runtime (sec): 58642.76008653641
